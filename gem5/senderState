ext/sst/translator.hh:    switch ((gem5::MemCmd::Command)pkt->cmd.toInt()) {
ext/sst/translator.hh:            panic("Unable to convert gem5 packet: %s\n", pkt->cmd.toString());
ext/sst/translator.hh:    SST::Interfaces::StandardMem::Addr addr = pkt->getAddr();
ext/sst/translator.hh:    auto data_size = pkt->getSize();
ext/sst/translator.hh:        uint8_t* data_ptr = pkt->getPtr<uint8_t>();
ext/sst/translator.hh:    if ((gem5::MemCmd::Command)pkt->cmd.toInt() == gem5::MemCmd::LoadLockedReq
ext/sst/translator.hh:        || (gem5::MemCmd::Command)pkt->cmd.toInt() == gem5::MemCmd::SwapReq
ext/sst/translator.hh:        || pkt->req->isLockedRMW()) {
ext/sst/translator.hh:    } else if ((gem5::MemCmd::Command)pkt->cmd.toInt() ==
ext/sst/translator.hh:    if (pkt->req->isUncacheable()) {
ext/sst/translator.hh:    if (pkt->needsResponse())
ext/sst/translator.hh:    pkt->makeResponse();
ext/sst/translator.hh:    if (pkt->isLLSC() && pkt->isWrite()) {
ext/sst/translator.hh:        pkt->req->setExtraData(1);
ext/sst/translator.hh:    if (!pkt->isWrite()) {
ext/sst/translator.hh:            pkt->setData(dynamic_cast<SST::Interfaces::StandardMem::ReadResp*>(
ext/sst/translator.hh:    pkt->headerDelay = pkt->payloadDelay = 0;
ext/sst/sst_responder_subcomponent.cc:    pkt->setData(
ext/sst/sst_responder_subcomponent.cc:    pkt->makeAtomicResponse();
ext/sst/sst_responder_subcomponent.cc:    pkt->headerDelay = pkt->payloadDelay = 0;
ext/sst/sst_responder_subcomponent.cc:    (*(pkt->getAtomicOp()))(data.data()); // apply the atomic op
ext/sst/sst_responder_subcomponent.cc:        if ((gem5::MemCmd::Command)pkt->cmd.toInt() == gem5::MemCmd::SwapReq) {
ext/sst/sst_responder_subcomponent.cc:        pkt->headerDelay = pkt->payloadDelay = 0;
Binary file build/X86/arch/x86/interrupts.o matches
Binary file build/X86/arch/x86/pagetable_walker.o matches
Binary file build/X86/arch/x86/tlb.o matches
Binary file build/X86/mem/cache/noncoherent_cache.o matches
Binary file build/X86/mem/cache/mshr.o matches
Binary file build/X86/mem/cache/base.o matches
Binary file build/X86/mem/cache/cache.o matches
Binary file build/X86/mem/bridge.o matches
Binary file build/X86/mem/hmc_controller.o matches
Binary file build/X86/mem/xbar.o matches
Binary file build/X86/mem/physical.o matches
Binary file build/X86/mem/comm_monitor.o matches
Binary file build/X86/mem/protocol/atomic.o matches
Binary file build/X86/mem/protocol/timing.o matches
Binary file build/X86/mem/protocol/functional.o matches
Binary file build/X86/mem/packet_queue.o matches
Binary file build/X86/mem/coherent_xbar.o matches
Binary file build/X86/mem/hetero_mem_ctrl.o matches
Binary file build/X86/mem/cfi_mem.o matches
Binary file build/X86/mem/snoop_filter.o matches
Binary file build/X86/mem/ruby/slicc_interface/AbstractController.o matches
Binary file build/X86/mem/ruby/common/DataBlock.o matches
Binary file build/X86/mem/ruby/system/RubySystem.o matches
Binary file build/X86/mem/ruby/system/HTMSequencer.o matches
Binary file build/X86/mem/ruby/system/DMASequencer.o matches
Binary file build/X86/mem/ruby/system/RubyPort.o matches
Binary file build/X86/mem/ruby/system/Sequencer.o matches
Binary file build/X86/mem/tport.o matches
Binary file build/X86/mem/qos/q_policy.o matches
Binary file build/X86/mem/qos/mem_sink.o matches
Binary file build/X86/mem/qos/policy.o matches
Binary file build/X86/mem/qos/mem_ctrl.o matches
Binary file build/X86/mem/packet.o matches
Binary file build/X86/mem/noncoherent_xbar.o matches
Binary file build/X86/mem/mem_ctrl.o matches
Binary file build/X86/mem/mem_checker_monitor.o matches
Binary file build/X86/mem/hbm_ctrl.o matches
Binary file build/X86/mem/simple_mem.o matches
Binary file build/X86/mem/abstract_mem.o matches
Binary file build/X86/gem5.opt matches
Binary file build/X86/cpu/base.o matches
Binary file build/X86/cpu/minor/lsq.o matches
Binary file build/X86/cpu/testers/traffic_gen/base.o matches
Binary file build/X86/cpu/testers/traffic_gen/gups_gen.o matches
Binary file build/X86/cpu/testers/memtest/memtest.o matches
Binary file build/X86/cpu/testers/garnet_synthetic_traffic/GarnetSyntheticTraffic.o matches
Binary file build/X86/cpu/simple/timing.o matches
Binary file build/X86/cpu/o3/lsq_unit.o matches
Binary file build/X86/cpu/o3/fetch.o matches
Binary file build/X86/cpu/o3/lsq.o matches
Binary file build/X86/dev/x86/speaker.o matches
Binary file build/X86/dev/x86/i82094aa.o matches
Binary file build/X86/dev/x86/cmos.o matches
Binary file build/X86/dev/x86/i8254.o matches
Binary file build/X86/dev/x86/i8259.o matches
Binary file build/X86/dev/x86/i8042.o matches
Binary file build/X86/dev/isa_fake.o matches
Binary file build/X86/dev/serial/simple.o matches
Binary file build/X86/dev/io_device.o matches
Binary file build/X86/dev/i2c/bus.o matches
Binary file build/X86/dev/dma_device.o matches
Binary file build/X86/dev/net/ns_gige.o matches
Binary file build/X86/dev/net/i8254xGBe.o matches
Binary file build/X86/learning_gem5/part2/simple_cache.o matches
Binary file build/X86/systemc/tlm_bridge/tlm_to_gem5.o matches
util/tlm/src/sc_master_port.cc:    pkt->dataStatic(trans.get_data_ptr());
util/tlm/src/sc_master_port.cc:    pkt->pushSenderState(tlmSenderState);
util/tlm/src/sc_master_port.cc:    panic_if(pkt->needsResponse() && !pkt->isResponse(),
util/tlm/src/sc_master_port.cc:    sc_assert(pkt->isResponse());
util/tlm/src/sc_master_port.cc:    auto delay = sc_core::sc_time::from_value(pkt->payloadDelay);
util/tlm/src/sc_master_port.cc:    pkt->payloadDelay = 0;
util/tlm/src/sc_master_port.cc:    pkt->headerDelay = 0;
util/tlm/src/sc_master_port.cc:    auto tlmSenderState = dynamic_cast<TlmSenderState*>(pkt->popSenderState());
src/arch/x86/interrupts.cc:    Addr offset = pkt->getAddr() - pioAddr;
src/arch/x86/interrupts.cc:    if ((offset & ~mask(3)) != ((offset + pkt->getSize()) & ~mask(3)))
src/arch/x86/interrupts.cc:    pkt->setData(((uint8_t *)&val) + (offset & mask(3)));
src/arch/x86/interrupts.cc:    pkt->makeAtomicResponse();
src/arch/x86/interrupts.cc:    Addr offset = pkt->getAddr() - pioAddr;
src/arch/x86/interrupts.cc:    if ((offset & ~mask(3)) != ((offset + pkt->getSize()) & ~mask(3)))
src/arch/x86/interrupts.cc:    pkt->writeData(((uint8_t *)&val) + (offset & mask(3)));
src/arch/x86/interrupts.cc:    pkt->makeAtomicResponse();
src/arch/x86/interrupts.cc:        requestInterrupt(pkt->getLE<uint8_t>(), dm, trigger);
src/arch/x86/interrupts.cc:    Addr offset = pkt->getAddr() - x86InterruptAddress(initialApicId, 0);
src/arch/x86/interrupts.cc:    assert(pkt->cmd == MemCmd::WriteReq);
src/arch/x86/interrupts.cc:            TriggerIntMessage message = pkt->getRaw<TriggerIntMessage>();
src/arch/x86/interrupts.cc:    pkt->makeAtomicResponse();
src/arch/x86/intmessage.hh:        pkt->allocate();
src/arch/x86/memhelpers.hh:        mem = pkt->getLE<uint8_t>();
src/arch/x86/memhelpers.hh:        mem = pkt->getLE<uint16_t>();
src/arch/x86/memhelpers.hh:        mem = pkt->getLE<uint32_t>();
src/arch/x86/memhelpers.hh:        mem = pkt->getLE<uint64_t>();
src/arch/x86/memhelpers.hh:    std::array<T, N> real_mem = pkt->getLE<std::array<T, N> >();
src/arch/x86/kvm/x86_cpu.cc:        pkt->dataStatic(guestData);
src/arch/x86/tlb.cc:        assert(pkt->getSize() <= sizeof(RegVal));
src/arch/x86/tlb.cc:        pkt->setData((uint8_t *)&data);
src/arch/x86/tlb.cc:        assert(pkt->getSize() <= sizeof(RegVal));
src/arch/x86/tlb.cc:        pkt->writeData((uint8_t *)&data);
src/arch/x86/tlb.cc:                    pkt->setLE(ret);
src/arch/x86/pagetable_walker.cc:        dynamic_cast<WalkerSenderState *>(pkt->popSenderState());
src/arch/x86/pagetable_walker.cc:    pkt->pushSenderState(walker_state);
src/arch/x86/pagetable_walker.cc:        pkt->popSenderState();
src/arch/x86/pagetable_walker.cc:    assert(pkt->isResponse());
src/arch/x86/pagetable_walker.cc:    if (pkt->isRead()) {
src/arch/x86/pagetable_walker.cc:        pkt->headerDelay = pkt->payloadDelay = 0;
src/arch/arm/isa.cc:    assert(pkt->isInvalidate() || pkt->isWrite());
src/arch/arm/isa.cc:            tc->getCpuPtr()->name(), pkt->getAddr(),
src/arch/arm/isa.cc:    Addr snoop_addr = pkt->getAddr() & cacheBlockMask;
src/arch/arm/fastmodel/iris/thread_context.cc:    auto addr = pkt->getAddr();
src/arch/arm/fastmodel/iris/thread_context.cc:    auto size = pkt->getSize();
src/arch/arm/fastmodel/iris/thread_context.cc:    auto data = pkt->getPtr<uint8_t>();
src/arch/arm/fastmodel/iris/thread_context.cc:    pkt->makeResponse();
src/arch/arm/fastmodel/iris/thread_context.cc:    if (pkt->isRead())
src/arch/arm/fastmodel/CortexR52/thread_context.cc:  auto addr = pkt->getAddr();
src/arch/arm/fastmodel/CortexR52/thread_context.cc:  auto size = pkt->getSize();
src/arch/arm/fastmodel/CortexR52/thread_context.cc:  auto data = pkt->getPtr<uint8_t>();
src/arch/arm/fastmodel/CortexR52/thread_context.cc:  pkt->makeResponse();
src/arch/arm/fastmodel/CortexR52/thread_context.cc:  if (pkt->isRead())
src/arch/arm/fastmodel/reset_controller/example.cc:    pkt->makeResponse();
src/arch/arm/fastmodel/reset_controller/example.cc:    auto data = pkt->getPtr<uint8_t>();
src/arch/arm/fastmodel/reset_controller/example.cc:    auto size = pkt->getSize();
src/arch/arm/fastmodel/reset_controller/example.cc:    pkt->makeResponse();
src/arch/arm/fastmodel/reset_controller/example.cc:    size_t size = pkt->getSize();
src/arch/arm/fastmodel/reset_controller/example.cc:        pkt->setBadAddress();
src/arch/arm/fastmodel/reset_controller/example.cc:        auto addr = pkt->getAddr() - pioAddr;
src/arch/arm/fastmodel/reset_controller/example.cc:        registers.write(addr, pkt->getPtr<void>(), size);
src/arch/arm/table_walker.cc:    pkt->dataStatic(data);
src/arch/arm/table_walker.cc:    pkt->senderState = state;
src/arch/arm/table_walker.cc:    assert(pkt->req->isUncacheable() ||
src/arch/arm/table_walker.cc:           !(pkt->cacheResponding() && !pkt->hasSharers()));
src/arch/arm/table_walker.cc:    assert(pkt->isResponse());
src/arch/arm/table_walker.cc:    auto *state = dynamic_cast<TableWalkerState*>(pkt->senderState);
src/arch/arm/table_walker.cc:    handleResp(state, pkt->getAddr(), pkt->req->getSize(), delay);
src/arch/arm/isa/templates/mem.isa:            MemUnion &memUnion = *(MemUnion *)pkt->getPtr<uint8_t>();
src/arch/arm/isa/templates/mem.isa:            uint64_t writeResult = pkt->req->getExtraData();
src/arch/arm/isa/templates/mem64.isa:        uint64_t writeResult = pkt->req->getExtraData();
src/arch/arm/isa/templates/sme.isa:            memcpy((uint8_t*)data, pkt->getPtr<uint8_t>(), pkt->getSize());
src/arch/arm/isa/templates/sme.isa:                row[i] = pkt->getPtr<uint8_t>()[i];
src/arch/arm/isa/templates/sve_mem.isa:            memcpy(memData.as<uint8_t>(), pkt->getPtr<uint8_t>(),
src/arch/arm/isa/templates/sve_mem.isa:                   pkt->getSize());
src/arch/arm/isa/templates/sve_mem.isa:        memcpy(memData.as<uint8_t>(), pkt->getPtr<uint8_t>(),
src/arch/arm/isa/templates/sve_mem.isa:                pkt->getSize());
src/arch/arm/isa/templates/neon64.isa:        memcpy(&memUnion, pkt->getPtr<uint8_t>(), pkt->getSize());
src/arch/arm/mmu.cc:                    pkt->setLE(ret);
src/arch/sparc/tlb.cc:    Addr va = pkt->getAddr();
src/arch/sparc/tlb.cc:    ASI asi = (ASI)pkt->req->getArchFlags();
src/arch/sparc/tlb.cc:         (uint32_t)pkt->req->getArchFlags(), pkt->getAddr());
src/arch/sparc/tlb.cc:        pkt->setBE(tc->readMiscReg(MISCREG_MMU_LSU_CTRL));
src/arch/sparc/tlb.cc:            pkt->setBE(tc->readMiscReg(MISCREG_MMU_P_CONTEXT));
src/arch/sparc/tlb.cc:            pkt->setBE(tc->readMiscReg(MISCREG_MMU_S_CONTEXT));
src/arch/sparc/tlb.cc:        pkt->setBE(tc->readMiscReg(MISCREG_QUEUE_CPU_MONDO_HEAD +
src/arch/sparc/tlb.cc:        pkt->setBE(c0_tsb_ps0);
src/arch/sparc/tlb.cc:        pkt->setBE(c0_tsb_ps1);
src/arch/sparc/tlb.cc:        pkt->setBE(c0_config);
src/arch/sparc/tlb.cc:        pkt->setBE(itb->c0_tsb_ps0);
src/arch/sparc/tlb.cc:        pkt->setBE(itb->c0_tsb_ps1);
src/arch/sparc/tlb.cc:        pkt->setBE(itb->c0_config);
src/arch/sparc/tlb.cc:        pkt->setBE(cx_tsb_ps0);
src/arch/sparc/tlb.cc:        pkt->setBE(cx_tsb_ps1);
src/arch/sparc/tlb.cc:        pkt->setBE(cx_config);
src/arch/sparc/tlb.cc:        pkt->setBE(itb->cx_tsb_ps0);
src/arch/sparc/tlb.cc:        pkt->setBE(itb->cx_tsb_ps1);
src/arch/sparc/tlb.cc:        pkt->setBE(itb->cx_config);
src/arch/sparc/tlb.cc:        pkt->setBE((uint64_t)0);
src/arch/sparc/tlb.cc:        pkt->setBE(tc->readMiscReg(MISCREG_SCRATCHPAD_R0 + (va >> 3)));
src/arch/sparc/tlb.cc:            pkt->setBE(bits(temp,63,22) | bits(temp,12,0) << 48);
src/arch/sparc/tlb.cc:            pkt->setBE(itb->sfsr);
src/arch/sparc/tlb.cc:            pkt->setBE(itb->tag_access);
src/arch/sparc/tlb.cc:            pkt->setBE(bits(temp,63,22) | bits(temp,12,0) << 48);
src/arch/sparc/tlb.cc:            pkt->setBE(sfsr);
src/arch/sparc/tlb.cc:            pkt->setBE(sfar);
src/arch/sparc/tlb.cc:            pkt->setBE(tag_access);
src/arch/sparc/tlb.cc:            pkt->setBE(tc->readMiscReg(MISCREG_MMU_PART_ID));
src/arch/sparc/tlb.cc:        pkt->setBE(MakeTsbPtr(Ps0,
src/arch/sparc/tlb.cc:        pkt->setBE(MakeTsbPtr(Ps1,
src/arch/sparc/tlb.cc:          pkt->setBE(MakeTsbPtr(Ps0,
src/arch/sparc/tlb.cc:          pkt->setBE(MakeTsbPtr(Ps1,
src/arch/sparc/tlb.cc:            pkt->setBE(interrupts->get_vec(IT_INT_VEC));
src/arch/sparc/tlb.cc:            pkt->setBE(temp);
src/arch/sparc/tlb.cc:    pkt->makeAtomicResponse();
src/arch/sparc/tlb.cc:    uint64_t data = pkt->getBE<uint64_t>();
src/arch/sparc/tlb.cc:    Addr va = pkt->getAddr();
src/arch/sparc/tlb.cc:    ASI asi = (ASI)pkt->req->getArchFlags();
src/arch/sparc/tlb.cc:            (uint32_t)pkt->req->getArchFlags(), pkt->getAddr(), data);
src/arch/sparc/tlb.cc:    pkt->makeAtomicResponse();
src/arch/power/isa/formats/mem.isa:        EA = pkt->req->getVaddr();
src/arch/power/isa/formats/mem.isa:        EA = pkt->req->getVaddr();
src/arch/riscv/isa.cc:    Addr snoop_addr = pkt->getAddr() & cacheBlockMask;
src/arch/riscv/pma_checker.cc:    return isUncacheable(pkt->getAddrRange());
src/arch/riscv/isa/formats/amo.isa:        uint64_t result = !pkt->req->getExtraData();
src/arch/riscv/isa/templates/vector_mem.isa:    memcpy(Mem.as<uint8_t>(), pkt->getPtr<uint8_t>(), pkt->getSize());
src/arch/riscv/isa/templates/vector_mem.isa:    memcpy(Mem.as<uint8_t>(), pkt->getPtr<uint8_t>(), pkt->getSize());
src/arch/riscv/isa/templates/vector_mem.isa:        memcpy(Mem.as<uint8_t>(), pkt->getPtr<uint8_t>(), pkt->getSize());
src/arch/riscv/isa/templates/vector_mem.isa:        memcpy(Mem.as<uint8_t>(), pkt->getPtr<uint8_t>(), pkt->getSize());
src/arch/riscv/pagetable_walker.cc:        dynamic_cast<WalkerSenderState *>(pkt->popSenderState());
src/arch/riscv/pagetable_walker.cc:    pkt->pushSenderState(walker_state);
src/arch/riscv/pagetable_walker.cc:        pkt->popSenderState();
src/arch/riscv/pagetable_walker.cc:    assert(pkt->isResponse());
src/arch/riscv/pagetable_walker.cc:    if (pkt->isRead()) {
src/arch/riscv/pagetable_walker.cc:        pkt->headerDelay = pkt->payloadDelay = 0;
src/arch/mips/isa/formats/mem.isa:        uint64_t write_result = pkt->req->getExtraData();
src/arch/generic/memhelpers.hh:    mem = pkt->get<MemT>(Order);
src/arch/amdgpu/gcn3/gpu_mem_helpers.hh:                pkt->dataStatic(&(reinterpret_cast<T*>(
src/arch/amdgpu/gcn3/gpu_mem_helpers.hh:        pkt->dataStatic(gpuDynInst->scalar_data);
src/arch/amdgpu/common/tlb_coalescer.cc:      safe_cast<GpuTranslationState*>(incoming_pkt->senderState);
src/arch/amdgpu/common/tlb_coalescer.cc:     safe_cast<GpuTranslationState*>(coalesced_pkt->senderState);
src/arch/amdgpu/common/tlb_coalescer.cc:    Addr incoming_virt_page_addr = roundDown(incoming_pkt->req->getVaddr(),
src/arch/amdgpu/common/tlb_coalescer.cc:    Addr coalesced_virt_page_addr = roundDown(coalesced_pkt->req->getVaddr(),
src/arch/amdgpu/common/tlb_coalescer.cc:    Addr virt_page_addr = roundDown(pkt->req->getVaddr(), X86ISA::PageBytes);
src/arch/amdgpu/common/tlb_coalescer.cc:        safe_cast<GpuTranslationState*>(pkt->senderState);
src/arch/amdgpu/common/tlb_coalescer.cc:    Addr phys_page_paddr = pkt->req->getPaddr();
src/arch/amdgpu/common/tlb_coalescer.cc:                    local_pkt->senderState);
src/arch/amdgpu/common/tlb_coalescer.cc:            paddr |= (local_pkt->req->getVaddr() & (page_size - 1));
src/arch/amdgpu/common/tlb_coalescer.cc:            local_pkt->req->setPaddr(paddr);
src/arch/amdgpu/common/tlb_coalescer.cc:                local_pkt->req->setFlags(Request::UNCACHEABLE);
src/arch/amdgpu/common/tlb_coalescer.cc:        if (local_pkt->isRequest()) {
src/arch/amdgpu/common/tlb_coalescer.cc:            local_pkt->makeTimingResponse();
src/arch/amdgpu/common/tlb_coalescer.cc:        safe_cast<GpuTranslationState*>(pkt->senderState);
src/arch/amdgpu/common/tlb_coalescer.cc:        safe_cast<GpuTranslationState*>(pkt->senderState);
src/arch/amdgpu/common/tlb_coalescer.cc:    Addr virt_page_addr = roundDown(pkt->req->getVaddr(), X86ISA::PageBytes);
src/arch/amdgpu/common/tlb.cc:            pkt->setData((uint8_t *)&data);
src/arch/amdgpu/common/tlb.cc:        assert(pkt->senderState);
src/arch/amdgpu/common/tlb.cc:        Addr virt_page_addr = roundDown(pkt->req->getVaddr(),
src/arch/amdgpu/common/tlb.cc:                safe_cast<GpuTranslationState*>(pkt->senderState);
src/arch/amdgpu/common/tlb.cc:        const RequestPtr &tmp_req = pkt->req;
src/arch/amdgpu/common/tlb.cc:        uint32_t flags = pkt->req->getFlags();
src/arch/amdgpu/common/tlb.cc:        Addr vaddr = pkt->req->getVaddr();
src/arch/amdgpu/common/tlb.cc:            safe_cast<GpuTranslationState*>(pkt->senderState);
src/arch/amdgpu/common/tlb.cc:        if (pkt->isRequest()) {
src/arch/amdgpu/common/tlb.cc:            pkt->makeTimingResponse();
src/arch/amdgpu/common/tlb.cc:        pkt->req->setPaddr(paddr);
src/arch/amdgpu/common/tlb.cc:             pkt->req->setFlags(Request::UNCACHEABLE);
src/arch/amdgpu/common/tlb.cc:            safe_cast<GpuTranslationState*>(pkt->senderState);
src/arch/amdgpu/common/tlb.cc:                safe_cast<GpuTranslationState*>(pkt->senderState);
src/arch/amdgpu/common/tlb.cc:            Addr vaddr = pkt->req->getVaddr();
src/arch/amdgpu/common/tlb.cc:            safe_cast<GpuTranslationState*>(pkt->senderState);
src/arch/amdgpu/common/tlb.cc:        Addr vaddr = pkt->req->getVaddr();
src/arch/amdgpu/common/tlb.cc:        pkt->req->setPaddr(paddr);
src/arch/amdgpu/common/tlb.cc:             pkt->req->setFlags(Request::UNCACHEABLE);
src/arch/amdgpu/common/tlb.cc:            safe_cast<GpuTranslationState*>(pkt->senderState);
src/arch/amdgpu/common/tlb.cc:        Addr virt_page_addr = roundDown(pkt->req->getVaddr(),
src/arch/amdgpu/common/tlb.cc:        bool success = tlb->tlbLookup(pkt->req, tc, update_stats);
src/arch/amdgpu/common/tlb.cc:                if (sender_state->isPrefetch && !pkt->req->hasPaddr())
src/arch/amdgpu/common/tlb.cc:                Addr vaddr = pkt->req->getVaddr();
src/arch/amdgpu/common/tlb.cc:                    tlb->lookup(pkt->req->getVaddr()));
src/arch/amdgpu/common/tlb.cc:            TlbEntry *entry = tlb->lookup(pkt->req->getVaddr(),
src/arch/amdgpu/common/tlb.cc:        // This is the function that would populate pkt->req with the paddr of
src/arch/amdgpu/common/tlb.cc:        Addr virt_page_addr = roundDown(pkt->req->getVaddr(),
src/arch/amdgpu/vega/gpu_mem_helpers.hh:                pkt->dataStatic(&(reinterpret_cast<T*>(
src/arch/amdgpu/vega/gpu_mem_helpers.hh:        pkt->dataStatic(gpuDynInst->scalar_data);
src/arch/amdgpu/vega/tlb_coalescer.cc:      safe_cast<GpuTranslationState*>(incoming_pkt->senderState);
src/arch/amdgpu/vega/tlb_coalescer.cc:     safe_cast<GpuTranslationState*>(coalesced_pkt->senderState);
src/arch/amdgpu/vega/tlb_coalescer.cc:    Addr incoming_virt_page_addr = roundDown(incoming_pkt->req->getVaddr(),
src/arch/amdgpu/vega/tlb_coalescer.cc:    Addr coalesced_virt_page_addr = roundDown(coalesced_pkt->req->getVaddr(),
src/arch/amdgpu/vega/tlb_coalescer.cc:    Addr virt_page_addr = roundDown(pkt->req->getVaddr(), VegaISA::PageBytes);
src/arch/amdgpu/vega/tlb_coalescer.cc:        safe_cast<GpuTranslationState*>(pkt->senderState);
src/arch/amdgpu/vega/tlb_coalescer.cc:    bool is_system = pkt->req->systemReq();
src/arch/amdgpu/vega/tlb_coalescer.cc:            safe_cast<GpuTranslationState*>(local_pkt->senderState);
src/arch/amdgpu/vega/tlb_coalescer.cc:                       + (local_pkt->req->getVaddr() & (page_size - 1));
src/arch/amdgpu/vega/tlb_coalescer.cc:            local_pkt->req->setPaddr(paddr);
src/arch/amdgpu/vega/tlb_coalescer.cc:                local_pkt->req->setFlags(Request::UNCACHEABLE);
src/arch/amdgpu/vega/tlb_coalescer.cc:        local_pkt->req->setSystemReq(is_system);
src/arch/amdgpu/vega/tlb_coalescer.cc:        if (local_pkt->isRequest()) {
src/arch/amdgpu/vega/tlb_coalescer.cc:            local_pkt->makeTimingResponse();
src/arch/amdgpu/vega/tlb_coalescer.cc:        safe_cast<GpuTranslationState*>(pkt->senderState);
src/arch/amdgpu/vega/tlb_coalescer.cc:        safe_cast<GpuTranslationState*>(pkt->senderState);
src/arch/amdgpu/vega/tlb_coalescer.cc:    Addr virt_page_addr = roundDown(pkt->req->getVaddr(), VegaISA::PageBytes);
src/arch/amdgpu/vega/tlb.cc:    assert(pkt->senderState);
src/arch/amdgpu/vega/tlb.cc:    Addr virt_page_addr = roundDown(pkt->req->getVaddr(),
src/arch/amdgpu/vega/tlb.cc:            safe_cast<GpuTranslationState*>(pkt->senderState);
src/arch/amdgpu/vega/tlb.cc:    const RequestPtr &tmp_req = pkt->req;
src/arch/amdgpu/vega/tlb.cc:        pkt->req->setSystemReq(entry->pte.s);
src/arch/amdgpu/vega/tlb.cc:        fatal("Page fault on addr %lx PTE=%#lx", pkt->req->getVaddr(),
src/arch/amdgpu/vega/tlb.cc:            pkt->req->getVaddr(), entry.vaddr, entry.paddr, entry.size());
src/arch/amdgpu/vega/tlb.cc:    Addr virt_page_addr = roundDown(pkt->req->getVaddr(),
src/arch/amdgpu/vega/tlb.cc:    pkt->req->setPaddr(paddr);
src/arch/amdgpu/vega/tlb.cc:    pkt->req->setSystemReq(entry.pte.s);
src/arch/amdgpu/vega/tlb.cc:        safe_cast<GpuTranslationState*>(pkt->senderState);
src/arch/amdgpu/vega/tlb.cc:    Addr vaddr = pkt->req->getVaddr();
src/arch/amdgpu/vega/tlb.cc:        safe_cast<GpuTranslationState*>(pkt->senderState);
src/arch/amdgpu/vega/tlb.cc:    if (pkt->isRequest()) {
src/arch/amdgpu/vega/tlb.cc:        pkt->makeTimingResponse();
src/arch/amdgpu/vega/tlb.cc:    pkt->req->setPaddr(paddr);
src/arch/amdgpu/vega/tlb.cc:         pkt->req->setFlags(Request::UNCACHEABLE);
src/arch/amdgpu/vega/tlb.cc:        safe_cast<GpuTranslationState*>(pkt->senderState);
src/arch/amdgpu/vega/tlb.cc:        Addr vaddr = pkt->req->getVaddr();
src/arch/amdgpu/vega/tlb.cc:    [[maybe_unused]] Addr virt_page_addr = roundDown(pkt->req->getVaddr(),
src/arch/amdgpu/vega/tlb.cc:        safe_cast<GpuTranslationState*>(pkt->senderState);
src/arch/amdgpu/vega/tlb.cc:    Addr vaddr = pkt->req->getVaddr();
src/arch/amdgpu/vega/tlb.cc:    pkt->req->setPaddr(paddr);
src/arch/amdgpu/vega/tlb.cc:         pkt->req->setFlags(Request::UNCACHEABLE);
src/arch/amdgpu/vega/tlb.cc:        safe_cast<GpuTranslationState*>(pkt->senderState);
src/arch/amdgpu/vega/tlb.cc:    Addr virt_page_addr = roundDown(pkt->req->getVaddr(),
src/arch/amdgpu/vega/tlb.cc:    bool success = tlb->tlbLookup(pkt->req, update_stats);
src/arch/amdgpu/vega/tlb.cc:            if (sender_state->isPrefetch && !pkt->req->hasPaddr())
src/arch/amdgpu/vega/tlb.cc:            Addr vaddr = pkt->req->getVaddr();
src/arch/amdgpu/vega/tlb.cc:            pkt->req->setPaddr(paddr);
src/arch/amdgpu/vega/tlb.cc:            pkt->req->setSystemReq(pte.s);
src/arch/amdgpu/vega/tlb.cc:    // This is the function that would populate pkt->req with the paddr of
src/arch/amdgpu/vega/tlb.cc:    Addr virt_page_addr = roundDown(pkt->req->getVaddr(),
src/arch/amdgpu/vega/pagetable_walker.cc:    pkt->pushSenderState(walker_state);
src/arch/amdgpu/vega/pagetable_walker.cc:                pkt->getAddr(), sending_walker);
src/arch/amdgpu/vega/pagetable_walker.cc:        (void)pkt->popSenderState();
src/arch/amdgpu/vega/pagetable_walker.cc:        safe_cast<WalkerSenderState *>(pkt->popSenderState());
src/arch/amdgpu/vega/pagetable_walker.cc:            pkt->getAddr(), senderState->senderWalk, pkt->getLE<uint64_t>());
src/mem/cache/mshr.cc:        if (pkt->needsWritable()) {
src/mem/cache/mshr.cc:        if (pkt->isUpgrade() || pkt->cmd == MemCmd::StoreCondReq) {
src/mem/cache/mshr.cc:            hasFromCache = hasFromCache || pkt->fromCache();
src/mem/cache/mshr.cc:        if (!pkt->isWrite()) {
src/mem/cache/mshr.cc:        const auto &req_flags = pkt->req->getFlags();
src/mem/cache/mshr.cc:        bool masked_write = pkt->isMaskedWrite();
src/mem/cache/mshr.cc:            auto offset = pkt->getOffset(blkSize);
src/mem/cache/mshr.cc:            std::fill(begin, begin + pkt->getSize(), true);
src/mem/cache/mshr.cc:        MSHR *mshr = pkt->findNextSenderState<MSHR>();
src/mem/cache/mshr.cc:    DPRINTF(MSHR, "New target allocated: %s\n", pkt->print());
src/mem/cache/mshr.cc:    bool has_data = pkt->hasData() || pkt->hasRespData();
src/mem/cache/mshr.cc:    if (pkt->cmd == MemCmd::UpgradeReq) {
src/mem/cache/mshr.cc:        pkt->cmd = MemCmd::ReadExReq;
src/mem/cache/mshr.cc:    } else if (pkt->cmd == MemCmd::SCUpgradeReq) {
src/mem/cache/mshr.cc:        pkt->cmd = MemCmd::SCUpgradeFailReq;
src/mem/cache/mshr.cc:    } else if (pkt->cmd == MemCmd::StoreCondReq) {
src/mem/cache/mshr.cc:        pkt->cmd = MemCmd::StoreCondFailReq;
src/mem/cache/mshr.cc:        assert(!pkt->hasData());
src/mem/cache/mshr.cc:        if (pkt->hasRespData()) {
src/mem/cache/mshr.cc:            pkt->allocate();
src/mem/cache/mshr.cc:            MSHR *mshr = t->pkt->findNextSenderState<MSHR>();
src/mem/cache/mshr.cc:        if (pkt->trySatisfyFunctional(t.pkt)) {
src/mem/cache/mshr.cc:        t.pkt->print(os, verbosity, "");
src/mem/cache/mshr.cc:    assert(pkt->cmd != MemCmd::HardPFReq);
src/mem/cache/mshr.cc:    if (pkt->req->isCacheMaintenance() ||
src/mem/cache/mshr.cc:        tgt_pkt->req->isCacheMaintenance() ||
src/mem/cache/mshr.cc:          (pkt->needsWritable() &&
src/mem/cache/mshr.cc:    DPRINTF(MSHR, "%s for %s\n", __func__, pkt->print());
src/mem/cache/mshr.cc:    panic_if((pkt->needsWritable() != pkt->isInvalidate()) &&
src/mem/cache/mshr.cc:             !pkt->req->isCacheMaintenance(),
src/mem/cache/mshr.cc:             "does not match isInvalidate", name(), pkt->print());
src/mem/cache/mshr.cc:    if (!inService || (pkt->isExpressSnoop() && downstreamPending)) {
src/mem/cache/mshr.cc:        if (pkt->needsWritable() || pkt->req->isCacheInvalidate()) {
src/mem/cache/mshr.cc:    if (pkt->needsWritable() || pkt->req->isCacheInvalidate()) {
src/mem/cache/mshr.cc:    if (hasPostInvalidate() || tgt_pkt->req->isCacheInvalidate()) {
src/mem/cache/mshr.cc:    const bool will_respond = isPendingModified() && pkt->needsResponse() &&
src/mem/cache/mshr.cc:        !pkt->isClean();
src/mem/cache/mshr.cc:    if (isPendingModified() || pkt->isInvalidate()) {
src/mem/cache/mshr.cc:            new Packet(std::make_shared<Request>(*pkt->req), pkt->cmd,
src/mem/cache/mshr.cc:                       blkSize, pkt->id);
src/mem/cache/mshr.cc:            pkt->setCacheResponding();
src/mem/cache/mshr.cc:            pkt->setResponderHadWritable();
src/mem/cache/mshr.cc:        } else if (isPendingModified() && pkt->isClean()) {
src/mem/cache/mshr.cc:            pkt->setSatisfied();
src/mem/cache/mshr.cc:        if (pkt->needsWritable() || pkt->isInvalidate()) {
src/mem/cache/mshr.cc:    if (!pkt->needsWritable() && !pkt->req->isUncacheable()) {
src/mem/cache/mshr.cc:        pkt->setHasSharers();
src/mem/cache/mshr.cc:    if (pkt->cmd == MemCmd::ReadRespWithInvalidate) {
src/mem/cache/mshr.cc:        if (it->pkt->cmd != MemCmd::LockedRMWReadReq) {
src/mem/cache/mshr.cc:            if (it->pkt->cmd == MemCmd::LockedRMWReadReq) {
src/mem/cache/mshr.cc:                               return t.pkt->req->isCacheMaintenance();
src/mem/cache/mshr.cc:            return !t.pkt->req->isCacheInvalidate() &&
src/mem/cache/mshr.cc:                   !t.pkt->needsWritable();
src/mem/cache/mshr.cc:        !def_tgt_pkt->req->isCacheInvalidate()) {
src/mem/cache/mshr.cc:            return !t.pkt->req->isCacheInvalidate();
src/mem/cache/mshr.cc:    if (pkt->isPrint()) {
src/mem/cache/mshr.cc:        pkt->trySatisfyFunctional(this, blkAddr, isSecure, blkSize, nullptr);
src/mem/cache/mshr.cc:    return pkt->matchBlockAddr(blkAddr, isSecure, blkSize);
src/mem/cache/mshr.cc:    RequestPtr r = std::make_shared<Request>(*(pkt->req));
src/mem/cache/mshr.cc:        targets.front().pkt->cmd == MemCmd::LockedRMWReadReq) {
src/mem/cache/prefetch/queued.cc:    pkt->allocate();
src/mem/cache/prefetch/queued.cc:        pkt->req->setPC(pfInfo.getPC());
src/mem/cache/prefetch/queued.cc:        Addr paddr = it->pkt ? it->pkt->getAddr() : 0;
src/mem/cache/prefetch/queued.cc:            if (cache.hasBeenPrefetched(pkt->getAddr(), pkt->isSecure())) {
src/mem/cache/prefetch/queued.cc:    DPRINTF(HWPrefetch, "Generating prefetch for %#x.\n", pkt->getAddr());
src/mem/cache/prefetch/queued.cc:            addr, blkSize, pkt->req->getFlags(), requestorId, pfi.getPC(),
src/mem/cache/prefetch/queued.cc:            pkt->req->contextId());
src/mem/cache/prefetch/queued.cc:        pkt->req->getVaddr() : pkt->req->getPaddr();
src/mem/cache/prefetch/queued.cc:            target_paddr = positive_stride ? (pkt->req->getPaddr() + stride) :
src/mem/cache/prefetch/queued.cc:                (pkt->req->getPaddr() - stride);
src/mem/cache/prefetch/queued.cc:        if (!pkt->req->hasContextId()) {
src/mem/cache/prefetch/queued.cc:        } else if (pkt->req->hasVaddr()) {
src/mem/cache/prefetch/queued.cc:                (pkt->req->getVaddr() + stride) :
src/mem/cache/prefetch/queued.cc:                (pkt->req->getVaddr() - stride);
src/mem/cache/prefetch/base.cc:  : address(addr), pc(pkt->req->hasPC() ? pkt->req->getPC() : 0),
src/mem/cache/prefetch/base.cc:    requestorId(pkt->req->requestorId()), validPC(pkt->req->hasPC()),
src/mem/cache/prefetch/base.cc:    secure(pkt->isSecure()), size(pkt->req->getSize()), write(pkt->isWrite()),
src/mem/cache/prefetch/base.cc:    paddress(pkt->req->getPaddr()), cacheMiss(miss)
src/mem/cache/prefetch/base.cc:    unsigned int req_size = pkt->req->getSize();
src/mem/cache/prefetch/base.cc:        Addr offset = pkt->req->getPaddr() - pkt->getAddr();
src/mem/cache/prefetch/base.cc:        std::memcpy(data, &(pkt->getConstPtr<uint8_t>()[offset]), req_size);
src/mem/cache/prefetch/base.cc:    bool fetch = pkt->req->isInstFetch();
src/mem/cache/prefetch/base.cc:    bool read = pkt->isRead();
src/mem/cache/prefetch/base.cc:    bool inv = pkt->isInvalidate();
src/mem/cache/prefetch/base.cc:    if (pkt->req->isUncacheable()) return false;
src/mem/cache/prefetch/base.cc:    if (pkt->cmd == MemCmd::CleanEvict) return false;
src/mem/cache/prefetch/base.cc:    if (pkt->cmd.isSWPrefetch()) return;
src/mem/cache/prefetch/base.cc:    if (pkt->req->isCacheMaintenance()) return;
src/mem/cache/prefetch/base.cc:    if (pkt->isWrite() && cache.coalesce()) return;
src/mem/cache/prefetch/base.cc:    if (!pkt->req->hasPaddr()) {
src/mem/cache/prefetch/base.cc:        acc.cache.hasBeenPrefetched(pkt->getAddr(), pkt->isSecure(),
src/mem/cache/prefetch/base.cc:        if (useVirtualAddresses && pkt->req->hasVaddr()) {
src/mem/cache/prefetch/base.cc:            PrefetchInfo pfi(pkt, pkt->req->getVaddr(), miss);
src/mem/cache/prefetch/base.cc:            PrefetchInfo pfi(pkt, pkt->req->getPaddr(), miss);
src/mem/cache/prefetch/bop.cc:    if (!pkt->cmd.isHWPrefetch()) return;
src/mem/cache/prefetch/bop.cc:    Addr tag_y = tag(pkt->getAddr());
src/mem/cache/prefetch/sbooe.cc:    auto it = demandAddresses.find(pkt->getAddr());
src/mem/cache/cache.cc:    if (pkt->isRead()) {
src/mem/cache/cache.cc:        if (pkt->fromCache()) {
src/mem/cache/cache.cc:            assert(pkt->getSize() == blkSize);
src/mem/cache/cache.cc:            if (pkt->needsWritable()) {
src/mem/cache/cache.cc:                assert(pkt->cmd == MemCmd::ReadExReq ||
src/mem/cache/cache.cc:                       pkt->cmd == MemCmd::SCUpgradeFailReq);
src/mem/cache/cache.cc:                assert(!pkt->hasSharers());
src/mem/cache/cache.cc:                    pkt->setCacheResponding();
src/mem/cache/cache.cc:                !pending_downgrade && !pkt->hasSharers() &&
src/mem/cache/cache.cc:                pkt->cmd != MemCmd::ReadCleanReq) {
src/mem/cache/cache.cc:                        pkt->setCacheResponding();
src/mem/cache/cache.cc:                        pkt->setHasSharers();
src/mem/cache/cache.cc:                pkt->setHasSharers();
src/mem/cache/cache.cc:    if (pkt->req->isUncacheable()) {
src/mem/cache/cache.cc:        assert(pkt->isRequest());
src/mem/cache/cache.cc:        gem5_assert(!(isReadOnly && pkt->isWrite()),
src/mem/cache/cache.cc:        DPRINTF(Cache, "%s for %s\n", __func__, pkt->print());
src/mem/cache/cache.cc:        CacheBlk *old_blk(tags->findBlock(pkt->getAddr(), pkt->isSecure()));
src/mem/cache/cache.cc:    DPRINTF(Cache, "%s for %s\n", __func__, pkt->print());
src/mem/cache/cache.cc:    const bool forwardAsSnoop = outstandingSnoop.find(pkt->req) ==
src/mem/cache/cache.cc:        assert(pkt->cmd == MemCmd::HardPFResp);
src/mem/cache/cache.cc:        outstandingSnoop.erase(pkt->req);
src/mem/cache/cache.cc:                "%#llx (%s)\n", pkt->getAddr(), pkt->isSecure() ? "s" : "ns");
src/mem/cache/cache.cc:    Tick snoop_resp_time = clockEdge(forwardLatency) + pkt->headerDelay;
src/mem/cache/cache.cc:    pkt->headerDelay = pkt->payloadDelay = 0;
src/mem/cache/cache.cc:    if (doFastWrites && (pkt->cmd == MemCmd::WriteReq) &&
src/mem/cache/cache.cc:        (pkt->getSize() == blkSize) && (pkt->getOffset(blkSize) == 0) &&
src/mem/cache/cache.cc:        !pkt->isMaskedWrite()) {
src/mem/cache/cache.cc:        pkt->cmd = MemCmd::WriteLineReq;
src/mem/cache/cache.cc:    assert(!pkt->req->isUncacheable());
src/mem/cache/cache.cc:    assert(pkt->cmd != MemCmd::LockedRMWWriteReq);
src/mem/cache/cache.cc:    if (pkt->req->isUncacheable()) {
src/mem/cache/cache.cc:        stats.cmdStats(pkt).mshrUncacheable[pkt->req->requestorId()]++;
src/mem/cache/cache.cc:        if (pkt->isWrite()) {
src/mem/cache/cache.cc:            assert(pkt->isRead() || pkt->isCleanInvalidateRequest());
src/mem/cache/cache.cc:    Addr blk_addr = pkt->getBlockAddr(blkSize);
src/mem/cache/cache.cc:    MSHR *mshr = mshrQueue.findMatch(blk_addr, pkt->isSecure());
src/mem/cache/cache.cc:    if (pkt->cmd.isSWPrefetch()) {
src/mem/cache/cache.cc:        assert(pkt->needsResponse());
src/mem/cache/cache.cc:        assert(pkt->req->hasPaddr());
src/mem/cache/cache.cc:        assert(!pkt->req->isUncacheable());
src/mem/cache/cache.cc:            RequestPtr req = std::make_shared<Request>(pkt->req->getPaddr(),
src/mem/cache/cache.cc:                                                    pkt->req->getSize(),
src/mem/cache/cache.cc:                                                    pkt->req->getFlags(),
src/mem/cache/cache.cc:                                                    pkt->req->requestorId());
src/mem/cache/cache.cc:            pf = new Packet(req, pkt->cmd);
src/mem/cache/cache.cc:            assert(pf->getSize() == pkt->getSize());
src/mem/cache/cache.cc:        pkt->makeTimingResponse();
src/mem/cache/cache.cc:    if (pkt->cacheResponding()) {
src/mem/cache/cache.cc:                pkt->print());
src/mem/cache/cache.cc:        assert(pkt->needsWritable() && !pkt->responderHadWritable());
src/mem/cache/cache.cc:        snoop_pkt->headerDelay = snoop_pkt->payloadDelay = 0;
src/mem/cache/cache.cc:        snoop_pkt->setExpressSnoop();
src/mem/cache/cache.cc:        snoop_pkt->setCacheResponding();
src/mem/cache/cache.cc:    assert(!cpu_pkt->isEviction());
src/mem/cache/cache.cc:    if (cpu_pkt->req->isUncacheable() ||
src/mem/cache/cache.cc:        (!blkValid && cpu_pkt->isUpgrade()) ||
src/mem/cache/cache.cc:        cpu_pkt->cmd == MemCmd::InvalidateReq || cpu_pkt->isClean()) {
src/mem/cache/cache.cc:    assert(cpu_pkt->needsResponse());
src/mem/cache/cache.cc:    assert(cpu_pkt->cmd != MemCmd::WriteLineReq || is_whole_line_write);
src/mem/cache/cache.cc:        cmd = cpu_pkt->isLLSC() ? MemCmd::SCUpgradeReq : MemCmd::UpgradeReq;
src/mem/cache/cache.cc:    } else if (cpu_pkt->cmd == MemCmd::SCUpgradeFailReq ||
src/mem/cache/cache.cc:               cpu_pkt->cmd == MemCmd::StoreCondFailReq) {
src/mem/cache/cache.cc:    PacketPtr pkt = new Packet(cpu_pkt->req, cmd, blkSize);
src/mem/cache/cache.cc:    if (cpu_pkt->hasSharers() && !needsWritable) {
src/mem/cache/cache.cc:        pkt->setHasSharers();
src/mem/cache/cache.cc:                __func__, cpu_pkt->print(), pkt->print());
src/mem/cache/cache.cc:    assert(pkt->getAddr() == pkt->getBlockAddr(blkSize));
src/mem/cache/cache.cc:    pkt->allocate();
src/mem/cache/cache.cc:    DPRINTF(Cache, "%s: created %s from %s\n", __func__, pkt->print(),
src/mem/cache/cache.cc:            cpu_pkt->print());
src/mem/cache/cache.cc:    if (pkt->isEviction() || pkt->cmd == MemCmd::WriteClean ||
src/mem/cache/cache.cc:        (pkt->req->isUncacheable() && pkt->isWrite())) {
src/mem/cache/cache.cc:        assert(!(pkt->req->isUncacheable() && pkt->isWrite()) ||
src/mem/cache/cache.cc:               pkt->isResponse());
src/mem/cache/cache.cc:    PacketPtr bus_pkt = createMissPacket(pkt, blk, pkt->needsWritable(),
src/mem/cache/cache.cc:                                         pkt->isWholeLineWrite(blkSize));
src/mem/cache/cache.cc:            bus_pkt->print());
src/mem/cache/cache.cc:    bool is_invalidate = bus_pkt->isInvalidate();
src/mem/cache/cache.cc:            bus_pkt->print(), old_state);
src/mem/cache/cache.cc:        if (pkt->needsResponse()) {
src/mem/cache/cache.cc:            assert(bus_pkt->isResponse());
src/mem/cache/cache.cc:            if (bus_pkt->isError()) {
src/mem/cache/cache.cc:                pkt->makeAtomicResponse();
src/mem/cache/cache.cc:                pkt->copyError(bus_pkt);
src/mem/cache/cache.cc:            } else if (pkt->isWholeLineWrite(blkSize)) {
src/mem/cache/cache.cc:                const bool allocate = allocOnFill(pkt->cmd) &&
src/mem/cache/cache.cc:            } else if (bus_pkt->isRead() ||
src/mem/cache/cache.cc:                       bus_pkt->cmd == MemCmd::UpgradeResp) {
src/mem/cache/cache.cc:                                 allocOnFill(pkt->cmd));
src/mem/cache/cache.cc:                maintainClusivity(pkt->fromCache(), blk);
src/mem/cache/cache.cc:                pkt->makeAtomicResponse();
src/mem/cache/cache.cc:    if (pkt->cacheResponding()) {
src/mem/cache/cache.cc:        assert(!pkt->req->isCacheInvalidate());
src/mem/cache/cache.cc:                pkt->print());
src/mem/cache/cache.cc:        assert(pkt->needsWritable() && !pkt->responderHadWritable());
src/mem/cache/cache.cc:    const int initial_offset = initial_tgt->pkt->getOffset(blkSize);
src/mem/cache/cache.cc:    const bool is_error = pkt->isError();
src/mem/cache/cache.cc:    bool is_invalidate = pkt->isInvalidate() &&
src/mem/cache/cache.cc:    if (pkt->cmd == MemCmd::LockedRMWWriteResp) {
src/mem/cache/cache.cc:        assert(initial_tgt->pkt->cmd == MemCmd::LockedRMWReadReq);
src/mem/cache/cache.cc:            completion_time = pkt->headerDelay;
src/mem/cache/cache.cc:            if (tgt_pkt->cmd.isSWPrefetch()) {
src/mem/cache/cache.cc:                if (tgt_pkt->needsWritable()) {
src/mem/cache/cache.cc:            if (tgt_pkt->cmd == MemCmd::WriteLineReq) {
src/mem/cache/cache.cc:                (!mshr->isForward || !pkt->hasData())) {
src/mem/cache/cache.cc:                    tgt_pkt->getOffset(blkSize) - initial_offset;
src/mem/cache/cache.cc:                    (transfer_offset ? pkt->payloadDelay : 0);
src/mem/cache/cache.cc:                assert(!tgt_pkt->req->isUncacheable());
src/mem/cache/cache.cc:                assert(tgt_pkt->req->requestorId() < system->maxRequestors());
src/mem/cache/cache.cc:                    .missLatency[tgt_pkt->req->requestorId()] +=
src/mem/cache/cache.cc:                if (tgt_pkt->cmd == MemCmd::LockedRMWReadReq) {
src/mem/cache/cache.cc:            } else if (pkt->cmd == MemCmd::UpgradeFailResp) {
src/mem/cache/cache.cc:                assert(tgt_pkt->cmd == MemCmd::StoreCondReq ||
src/mem/cache/cache.cc:                       tgt_pkt->cmd == MemCmd::StoreCondFailReq ||
src/mem/cache/cache.cc:                       tgt_pkt->cmd == MemCmd::SCUpgradeFailReq);
src/mem/cache/cache.cc:                    pkt->payloadDelay;
src/mem/cache/cache.cc:                tgt_pkt->req->setExtraData(0);
src/mem/cache/cache.cc:            } else if (pkt->cmd == MemCmd::LockedRMWWriteResp) {
src/mem/cache/cache.cc:                    pkt->payloadDelay;
src/mem/cache/cache.cc:                    if (pkt->isRead()) {
src/mem/cache/cache.cc:                        assert(pkt->matchAddr(tgt_pkt));
src/mem/cache/cache.cc:                        assert(pkt->getSize() >= tgt_pkt->getSize());
src/mem/cache/cache.cc:                        tgt_pkt->setData(pkt->getConstPtr<uint8_t>());
src/mem/cache/cache.cc:                        assert(!tgt_pkt->hasRespData());
src/mem/cache/cache.cc:                tgt_pkt->copyResponderFlags(pkt);
src/mem/cache/cache.cc:            tgt_pkt->makeTimingResponse();
src/mem/cache/cache.cc:                tgt_pkt->copyError(pkt);
src/mem/cache/cache.cc:            if (tgt_pkt->cmd == MemCmd::ReadResp &&
src/mem/cache/cache.cc:                tgt_pkt->cmd = MemCmd::ReadRespWithInvalidate;
src/mem/cache/cache.cc:                        tgt_pkt->print());
src/mem/cache/cache.cc:            tgt_pkt->headerDelay = tgt_pkt->payloadDelay = 0;
src/mem/cache/cache.cc:            assert(tgt_pkt->cmd == MemCmd::HardPFReq);
src/mem/cache/cache.cc:            assert(!is_invalidate || pkt->cmd == MemCmd::InvalidateResp ||
src/mem/cache/cache.cc:                   pkt->req->isCacheMaintenance() ||
src/mem/cache/cache.cc:    pkt->allocate();
src/mem/cache/cache.cc:    DPRINTF(Cache, "Create CleanEvict %s\n", pkt->print());
src/mem/cache/cache.cc:    assert(req_pkt->isRequest());
src/mem/cache/cache.cc:    assert(req_pkt->needsResponse());
src/mem/cache/cache.cc:    DPRINTF(Cache, "%s: for %s\n", __func__, req_pkt->print());
src/mem/cache/cache.cc:        pkt = new Packet(req_pkt, false, req_pkt->isRead());
src/mem/cache/cache.cc:    assert(req_pkt->req->isUncacheable() || req_pkt->isInvalidate() ||
src/mem/cache/cache.cc:           pkt->hasSharers());
src/mem/cache/cache.cc:    pkt->makeTimingResponse();
src/mem/cache/cache.cc:    if (pkt->isRead()) {
src/mem/cache/cache.cc:        pkt->setDataFromBlock(blk_data, blkSize);
src/mem/cache/cache.cc:    if (pkt->cmd == MemCmd::ReadResp && pending_inval) {
src/mem/cache/cache.cc:        pkt->cmd = MemCmd::ReadRespWithInvalidate;
src/mem/cache/cache.cc:    Tick forward_time = clockEdge(forwardLatency) + pkt->headerDelay;
src/mem/cache/cache.cc:    pkt->headerDelay = pkt->payloadDelay = 0;
src/mem/cache/cache.cc:            pkt->print(), forward_time);
src/mem/cache/cache.cc:    DPRINTF(CacheVerbose, "%s: for %s\n", __func__, pkt->print());
src/mem/cache/cache.cc:    assert(pkt->isRequest());
src/mem/cache/cache.cc:    bool invalidate = pkt->isInvalidate();
src/mem/cache/cache.cc:    [[maybe_unused]] bool needs_writable = pkt->needsWritable();
src/mem/cache/cache.cc:    panic_if(invalidate && pkt->req->isUncacheable(),
src/mem/cache/cache.cc:             name(), pkt->print());
src/mem/cache/cache.cc:                pkt->setBlockCached();
src/mem/cache/cache.cc:                pkt->setSatisfied();
src/mem/cache/cache.cc:            pkt->copyResponderFlags(&snoopPkt);
src/mem/cache/cache.cc:            bool already_responded = pkt->cacheResponding();
src/mem/cache/cache.cc:            if (!already_responded && pkt->cacheResponding()) {
src/mem/cache/cache.cc:                assert(pkt->isResponse());
src/mem/cache/cache.cc:    if (pkt->isClean()) {
src/mem/cache/cache.cc:                    __func__, pkt->print(), blk->print());
src/mem/cache/cache.cc:                writecleanBlk(blk, pkt->req->getDest(), pkt->id);
src/mem/cache/cache.cc:                    pkt->headerDelay;
src/mem/cache/cache.cc:            pkt->setSatisfied();
src/mem/cache/cache.cc:                pkt->print());
src/mem/cache/cache.cc:            assert(pkt->needsResponse());
src/mem/cache/cache.cc:            assert(pkt->cacheResponding());
src/mem/cache/cache.cc:                pkt->print(), blk->print());
src/mem/cache/cache.cc:        respond = blk->isSet(CacheBlk::DirtyBit) && pkt->needsResponse();
src/mem/cache/cache.cc:    if (pkt->mustCheckAbove()) {
src/mem/cache/cache.cc:                "from lower cache\n", pkt->getAddr(), pkt->print());
src/mem/cache/cache.cc:        pkt->setBlockCached();
src/mem/cache/cache.cc:    if (pkt->isRead() && !invalidate) {
src/mem/cache/cache.cc:        pkt->setHasSharers();
src/mem/cache/cache.cc:        if (!pkt->req->isUncacheable()) {
src/mem/cache/cache.cc:        pkt->setCacheResponding();
src/mem/cache/cache.cc:        if (!pkt->isClean() && blk->isSet(CacheBlk::WritableBit)) {
src/mem/cache/cache.cc:            pkt->setResponderHadWritable();
src/mem/cache/cache.cc:        panic_if(!invalidate && !pkt->hasSharers(),
src/mem/cache/cache.cc:                 "but keeping the block", name(), pkt->print());
src/mem/cache/cache.cc:            pkt->makeAtomicResponse();
src/mem/cache/cache.cc:            if (pkt->hasData())
src/mem/cache/cache.cc:                pkt->setDataFromBlock(blk->data, blkSize);
src/mem/cache/cache.cc:        if (compressor && pkt->isRead()) {
src/mem/cache/cache.cc:        assert(pkt->needsResponse());
src/mem/cache/cache.cc:    DPRINTF(CacheVerbose, "%s: for %s\n", __func__, pkt->print());
src/mem/cache/cache.cc:    if (!inRange(pkt->getAddr())) {
src/mem/cache/cache.cc:    bool is_secure = pkt->isSecure();
src/mem/cache/cache.cc:    CacheBlk *blk = tags->findBlock(pkt->getAddr(), is_secure);
src/mem/cache/cache.cc:    Addr blk_addr = pkt->getBlockAddr(blkSize);
src/mem/cache/cache.cc:    pkt->snoopDelay = std::max<uint32_t>(pkt->snoopDelay,
src/mem/cache/cache.cc:    if (mshr && pkt->mustCheckAbove()) {
src/mem/cache/cache.cc:                "mshr hit\n", pkt->print());
src/mem/cache/cache.cc:        pkt->setBlockCached();
src/mem/cache/cache.cc:                pkt->getAddr(), is_secure ? "s" : "ns");
src/mem/cache/cache.cc:        assert(wb_pkt->isEviction() || wb_pkt->cmd == MemCmd::WriteClean);
src/mem/cache/cache.cc:        if (pkt->isEviction()) {
src/mem/cache/cache.cc:            pkt->setBlockCached();
src/mem/cache/cache.cc:                    "hit\n", __func__, pkt->print());
src/mem/cache/cache.cc:        bool respond = wb_pkt->cmd == MemCmd::WritebackDirty &&
src/mem/cache/cache.cc:            pkt->needsResponse();
src/mem/cache/cache.cc:        bool have_writable = !wb_pkt->hasSharers();
src/mem/cache/cache.cc:        bool invalidate = pkt->isInvalidate();
src/mem/cache/cache.cc:        if (!pkt->req->isUncacheable() && pkt->isRead() && !invalidate) {
src/mem/cache/cache.cc:            assert(!pkt->needsWritable());
src/mem/cache/cache.cc:            pkt->setHasSharers();
src/mem/cache/cache.cc:            wb_pkt->setHasSharers();
src/mem/cache/cache.cc:            pkt->setCacheResponding();
src/mem/cache/cache.cc:                pkt->setResponderHadWritable();
src/mem/cache/cache.cc:            doTimingSupplyResponse(pkt, wb_pkt->getConstPtr<uint8_t>(),
src/mem/cache/cache.cc:        if (invalidate && wb_pkt->cmd != MemCmd::WriteClean) {
src/mem/cache/cache.cc:    pkt->snoopDelay = std::max<uint32_t>(pkt->snoopDelay, snoop_delay +
src/mem/cache/cache.cc:    if (!inRange(pkt->getAddr())) {
src/mem/cache/cache.cc:    CacheBlk *blk = tags->findBlock(pkt->getAddr(), pkt->isSecure());
src/mem/cache/cache.cc:        assert(pkt->isEviction() || pkt->cmd == MemCmd::WriteClean);
src/mem/cache/cache.cc:        return pkt->isBlockCached();
src/mem/cache/cache.cc:    if (tgt_pkt->cmd == MemCmd::HardPFReq && forwardSnoops) {
src/mem/cache/cache.cc:        DPRINTF(Cache, "%s: MSHR %s\n", __func__, tgt_pkt->print());
src/mem/cache/cache.cc:                    tgt_pkt->getAddr(), tgt_pkt->isSecure()? "s": "ns");
src/mem/cache/noncoherent_cache.hh:        panic("Unexpected timing snoop request %s", pkt->print());
src/mem/cache/noncoherent_cache.hh:        panic("Unexpected timing snoop response %s", pkt->print());
src/mem/cache/noncoherent_cache.hh:        panic("Unexpected atomic snoop request %s", pkt->print());
src/mem/cache/mshr.hh:        return pkt->isClean();
src/mem/cache/mshr.hh:                targets.front().pkt->print());
src/mem/cache/base.cc:    if (pkt->isLockedRMW()) {
src/mem/cache/base.cc:        Addr blk_addr = pkt->getBlockAddr(blkSize);
src/mem/cache/base.cc:        if (pkt->isRead()) {
src/mem/cache/base.cc:            assert(!mshrQueue.findMatch(blk_addr, pkt->isSecure()));
src/mem/cache/base.cc:            // Request *req2 = new Request(*(pkt->req));
src/mem/cache/base.cc:            RequestPtr req2 = std::make_shared<Request>(*(pkt->req));
src/mem/cache/base.cc:            PacketPtr pkt2 = new Packet(req2, pkt->cmd);
src/mem/cache/base.cc:            assert(pkt->isWrite());
src/mem/cache/base.cc:            MSHR *mshr = mshrQueue.findMatch(blk_addr, pkt->isSecure());
src/mem/cache/base.cc:                new Packet(pkt->req, MemCmd::LockedRMWWriteResp);
src/mem/cache/base.cc:            resp_pkt->senderState = mshr;
src/mem/cache/base.cc:    if (pkt->needsResponse()) {
src/mem/cache/base.cc:        assert(pkt->headerDelay == 0);
src/mem/cache/base.cc:        assert(pkt->payloadDelay == 0);
src/mem/cache/base.cc:        pkt->makeTimingResponse();
src/mem/cache/base.cc:                pkt->print());
src/mem/cache/base.cc:        pkt && pkt->isWrite() && !pkt->req->isUncacheable()) {
src/mem/cache/base.cc:        writeAllocator->updateMode(pkt->getAddr(), pkt->getSize(),
src/mem/cache/base.cc:                                   pkt->getBlockAddr(blkSize));
src/mem/cache/base.cc:            assert(!pkt->isWriteback());
src/mem/cache/base.cc:            if (pkt->cmd == MemCmd::CleanEvict) {
src/mem/cache/base.cc:            } else if (pkt->cmd == MemCmd::WriteClean) {
src/mem/cache/base.cc:                        pkt->print());
src/mem/cache/base.cc:                assert(pkt->req->requestorId() < system->maxRequestors());
src/mem/cache/base.cc:                stats.cmdStats(pkt).mshrHits[pkt->req->requestorId()]++;
src/mem/cache/base.cc:                                     allocOnFill(pkt->cmd));
src/mem/cache/base.cc:        assert(pkt->req->requestorId() < system->maxRequestors());
src/mem/cache/base.cc:        stats.cmdStats(pkt).mshrMisses[pkt->req->requestorId()]++;
src/mem/cache/base.cc:        if (prefetcher && pkt->isDemand())
src/mem/cache/base.cc:        if (pkt->isEviction() || pkt->cmd == MemCmd::WriteClean) {
src/mem/cache/base.cc:                assert((pkt->needsWritable() &&
src/mem/cache/base.cc:                    pkt->req->isCacheMaintenance());
src/mem/cache/base.cc:    Tick forward_time = clockEdge(forwardLatency) + pkt->headerDelay;
src/mem/cache/base.cc:    if (pkt->cmd == MemCmd::LockedRMWWriteReq) {
src/mem/cache/base.cc:        CacheBlk *blk = tags->findBlock(pkt->getAddr(), pkt->isSecure());
src/mem/cache/base.cc:    pkt->headerDelay = pkt->payloadDelay = 0;
src/mem/cache/base.cc:                    pkt->getAddr(), pkt->isSecure() ? "s" : "ns");
src/mem/cache/base.cc:        pkt->headerDelay + pkt->payloadDelay;
src/mem/cache/base.cc:    pkt->headerDelay = pkt->payloadDelay = 0;
src/mem/cache/base.cc:    assert(pkt->isResponse());
src/mem/cache/base.cc:    panic_if(pkt->headerDelay != 0 && pkt->cmd != MemCmd::HardPFResp,
src/mem/cache/base.cc:    const bool is_error = pkt->isError();
src/mem/cache/base.cc:                pkt->print());
src/mem/cache/base.cc:            pkt->print());
src/mem/cache/base.cc:    if (pkt->isWrite() && pkt->cmd != MemCmd::LockedRMWWriteResp) {
src/mem/cache/base.cc:        assert(pkt->req->isUncacheable());
src/mem/cache/base.cc:    MSHR *mshr = dynamic_cast<MSHR*>(pkt->popSenderState());
src/mem/cache/base.cc:    if (pkt->req->isUncacheable()) {
src/mem/cache/base.cc:        assert(pkt->req->requestorId() < system->maxRequestors());
src/mem/cache/base.cc:            .mshrUncacheableLatency[pkt->req->requestorId()] += miss_latency;
src/mem/cache/base.cc:        assert(pkt->req->requestorId() < system->maxRequestors());
src/mem/cache/base.cc:            .mshrMissLatency[pkt->req->requestorId()] += miss_latency;
src/mem/cache/base.cc:        (pkt->isRead() || pkt->cmd == MemCmd::UpgradeResp ||
src/mem/cache/base.cc:    assert(!mshr->wasWholeLineWrite || pkt->isInvalidate());
src/mem/cache/base.cc:    CacheBlk *blk = tags->findBlock(pkt->getAddr(), pkt->isSecure());
src/mem/cache/base.cc:                pkt->getAddr());
src/mem/cache/base.cc:        if (blk && blk->isValid() && pkt->isClean() && !pkt->isInvalidate()) {
src/mem/cache/base.cc:            !pkt->req->isCacheInvalidate()) {
src/mem/cache/base.cc:            schedMemSideSendEvent(clockEdge() + pkt->payloadDelay);
src/mem/cache/base.cc:    const Tick forward_time = clockEdge(forwardLatency) + pkt->headerDelay;
src/mem/cache/base.cc:    DPRINTF(CacheVerbose, "%s: Leaving with %s\n", __func__, pkt->print());
src/mem/cache/base.cc:    if (pkt->isClean() && blk && blk->isSet(CacheBlk::DirtyBit)) {
src/mem/cache/base.cc:                __func__, pkt->print(), blk->print());
src/mem/cache/base.cc:        PacketPtr wb_pkt = writecleanBlk(blk, pkt->req->getDest(), pkt->id);
src/mem/cache/base.cc:        pkt->setSatisfied();
src/mem/cache/base.cc:    if (pkt->needsResponse()) {
src/mem/cache/base.cc:        pkt->makeAtomicResponse();
src/mem/cache/base.cc:    Addr blk_addr = pkt->getBlockAddr(blkSize);
src/mem/cache/base.cc:    bool is_secure = pkt->isSecure();
src/mem/cache/base.cc:    CacheBlk *blk = tags->findBlock(pkt->getAddr(), is_secure);
src/mem/cache/base.cc:    pkt->pushLabel(name());
src/mem/cache/base.cc:        && pkt->trySatisfyFunctional(&cbpw, blk_addr, is_secure, blkSize,
src/mem/cache/base.cc:    DPRINTF(CacheVerbose, "%s: %s %s%s%s\n", __func__,  pkt->print(),
src/mem/cache/base.cc:    pkt->popLabel();
src/mem/cache/base.cc:        pkt->makeResponse();
src/mem/cache/base.cc:        cpkt->writeDataToBlock(blk->data, blkSize);
src/mem/cache/base.cc:    assert(pkt->isRequest());
src/mem/cache/base.cc:    int offset = pkt->getOffset(blkSize);
src/mem/cache/base.cc:    assert(sizeof(uint64_t) >= pkt->getSize());
src/mem/cache/base.cc:    pkt->writeData((uint8_t *)&overwrite_val);
src/mem/cache/base.cc:    pkt->setData(blk_data);
src/mem/cache/base.cc:    if (pkt->req->isCondSwap()) {
src/mem/cache/base.cc:        if (pkt->getSize() == sizeof(uint64_t)) {
src/mem/cache/base.cc:            condition_val64 = pkt->req->getExtraData();
src/mem/cache/base.cc:        } else if (pkt->getSize() == sizeof(uint32_t)) {
src/mem/cache/base.cc:            condition_val32 = (uint32_t)pkt->req->getExtraData();
src/mem/cache/base.cc:        std::memcpy(blk_data, &overwrite_val, pkt->getSize());
src/mem/cache/base.cc:            Addr pf_addr = pkt->getBlockAddr(blkSize);
src/mem/cache/base.cc:            if (tags->findBlock(pf_addr, pkt->isSecure())) {
src/mem/cache/base.cc:            } else if (mshrQueue.findMatch(pf_addr, pkt->isSecure())) {
src/mem/cache/base.cc:            } else if (writeBuffer.findMatch(pf_addr, pkt->isSecure())) {
src/mem/cache/base.cc:                assert(pkt->req->requestorId() < system->maxRequestors());
src/mem/cache/base.cc:                stats.cmdStats(pkt).mshrMisses[pkt->req->requestorId()]++;
src/mem/cache/base.cc:    assert(pkt->isRequest());
src/mem/cache/base.cc:    // assert(!pkt->needsWritable() || blk->isSet(CacheBlk::WritableBit));
src/mem/cache/base.cc:    assert(pkt->getOffset(blkSize) + pkt->getSize() <= blkSize);
src/mem/cache/base.cc:    if (pkt->cmd == MemCmd::SwapReq) {
src/mem/cache/base.cc:        if (pkt->isAtomicOp()) {
src/mem/cache/base.cc:            int offset = tags->extractBlkOffset(pkt->getAddr());
src/mem/cache/base.cc:            pkt->setData(blk_data);
src/mem/cache/base.cc:            (*(pkt->getAtomicOp()))(blk_data);
src/mem/cache/base.cc:    } else if (pkt->isWrite()) {
src/mem/cache/base.cc:        DPRINTF(CacheVerbose, "%s for %s (write)\n", __func__, pkt->print());
src/mem/cache/base.cc:    } else if (pkt->isRead()) {
src/mem/cache/base.cc:        if (pkt->isLLSC()) {
src/mem/cache/base.cc:        assert(pkt->hasRespData());
src/mem/cache/base.cc:        pkt->setDataFromBlock(blk->data, blkSize);
src/mem/cache/base.cc:    } else if (pkt->isUpgrade()) {
src/mem/cache/base.cc:        assert(!pkt->hasSharers());
src/mem/cache/base.cc:            pkt->setCacheResponding();
src/mem/cache/base.cc:    } else if (pkt->isClean()) {
src/mem/cache/base.cc:        assert(pkt->isInvalidate());
src/mem/cache/base.cc:                pkt->print());
src/mem/cache/base.cc:    assert(pkt->isRequest());
src/mem/cache/base.cc:    gem5_assert(!(isReadOnly && pkt->isWrite()),
src/mem/cache/base.cc:    DPRINTF(Cache, "%s for %s %s\n", __func__, pkt->print(),
src/mem/cache/base.cc:    if (pkt->req->isCacheMaintenance()) {
src/mem/cache/base.cc:        lat = calculateTagOnlyLatency(pkt->headerDelay, tag_latency);
src/mem/cache/base.cc:    if (pkt->isEviction()) {
src/mem/cache/base.cc:        WriteQueueEntry *wb_entry = writeBuffer.findMatch(pkt->getAddr(),
src/mem/cache/base.cc:                                                          pkt->isSecure());
src/mem/cache/base.cc:            if (pkt->isCleanEviction()) {
src/mem/cache/base.cc:                lat = calculateTagOnlyLatency(pkt->headerDelay, tag_latency);
src/mem/cache/base.cc:                assert(pkt->cmd == MemCmd::WritebackDirty);
src/mem/cache/base.cc:    if (pkt->isWrite()) {
src/mem/cache/base.cc:        lat = calculateTagOnlyLatency(pkt->headerDelay, tag_latency);
src/mem/cache/base.cc:    if (pkt->isWriteback()) {
src/mem/cache/base.cc:        assert(blkSize == pkt->getSize());
src/mem/cache/base.cc:        if (pkt->cmd == MemCmd::WritebackClean &&
src/mem/cache/base.cc:            mshrQueue.findMatch(pkt->getAddr(), pkt->isSecure())) {
src/mem/cache/base.cc:                    "dropping\n", pkt->getAddr());
src/mem/cache/base.cc:            if (!updateCompressionData(blk, pkt->getConstPtr<uint64_t>(),
src/mem/cache/base.cc:        if (pkt->cmd == MemCmd::WritebackDirty) {
src/mem/cache/base.cc:        if (!pkt->hasSharers()) {
src/mem/cache/base.cc:        assert(!pkt->needsResponse());
src/mem/cache/base.cc:        blk->setWhenReady(clockEdge(fillLatency) + pkt->headerDelay +
src/mem/cache/base.cc:            std::max(cyclesToTicks(tag_latency), (uint64_t)pkt->payloadDelay));
src/mem/cache/base.cc:    } else if (pkt->cmd == MemCmd::CleanEvict) {
src/mem/cache/base.cc:        lat = calculateTagOnlyLatency(pkt->headerDelay, tag_latency);
src/mem/cache/base.cc:    } else if (pkt->cmd == MemCmd::WriteClean) {
src/mem/cache/base.cc:        assert(blkSize == pkt->getSize());
src/mem/cache/base.cc:            if (pkt->writeThrough()) {
src/mem/cache/base.cc:            if (!updateCompressionData(blk, pkt->getConstPtr<uint64_t>(),
src/mem/cache/base.cc:        if (!pkt->writeThrough()) {
src/mem/cache/base.cc:        assert(!pkt->needsResponse());
src/mem/cache/base.cc:        blk->setWhenReady(clockEdge(fillLatency) + pkt->headerDelay +
src/mem/cache/base.cc:            std::max(cyclesToTicks(tag_latency), (uint64_t)pkt->payloadDelay));
src/mem/cache/base.cc:        return !pkt->writeThrough();
src/mem/cache/base.cc:    } else if (blk && (pkt->needsWritable() ?
src/mem/cache/base.cc:        if (pkt->isRead()) {
src/mem/cache/base.cc:            lat = calculateAccessLatency(blk, pkt->headerDelay, tag_latency);
src/mem/cache/base.cc:            lat = calculateTagOnlyLatency(pkt->headerDelay, tag_latency);
src/mem/cache/base.cc:        maintainClusivity(pkt->fromCache(), blk);
src/mem/cache/base.cc:    lat = calculateAccessLatency(blk, pkt->headerDelay, tag_latency);
src/mem/cache/base.cc:    if (!blk && pkt->isLLSC() && pkt->isWrite()) {
src/mem/cache/base.cc:        pkt->req->setExtraData(0);
src/mem/cache/base.cc:    assert(pkt->isResponse());
src/mem/cache/base.cc:    Addr addr = pkt->getAddr();
src/mem/cache/base.cc:    bool is_secure = pkt->isSecure();
src/mem/cache/base.cc:    assert(addr == pkt->getBlockAddr(blkSize));
src/mem/cache/base.cc:        assert(pkt->hasData() || pkt->cmd == MemCmd::InvalidateResp);
src/mem/cache/base.cc:    if (pkt->cmd == MemCmd::InvalidateResp) {
src/mem/cache/base.cc:        assert(!pkt->hasSharers());
src/mem/cache/base.cc:    if (!pkt->hasSharers()) {
src/mem/cache/base.cc:        if (pkt->cacheResponding()) {
src/mem/cache/base.cc:    if (pkt->isRead()) {
src/mem/cache/base.cc:        assert(pkt->hasData());
src/mem/cache/base.cc:        assert(pkt->getSize() == blkSize);
src/mem/cache/base.cc:    blk->setWhenReady(clockEdge(fillLatency) + pkt->headerDelay +
src/mem/cache/base.cc:                      pkt->payloadDelay);
src/mem/cache/base.cc:    const Addr addr = pkt->getAddr();
src/mem/cache/base.cc:    const bool is_secure = pkt->isSecure();
src/mem/cache/base.cc:    if (compressor && pkt->hasData()) {
src/mem/cache/base.cc:            pkt->getConstPtr<uint64_t>(), compression_lat, decompression_lat);
src/mem/cache/base.cc:        pkt->print(), blk->isSet(CacheBlk::WritableBit),
src/mem/cache/base.cc:        pkt->setHasSharers();
src/mem/cache/base.cc:    pkt->allocate();
src/mem/cache/base.cc:    pkt->setDataFromBlock(blk->data, blkSize);
src/mem/cache/base.cc:        pkt->payloadDelay = compressor->getDecompressionLatency(blk);
src/mem/cache/base.cc:        pkt->setWriteThrough();
src/mem/cache/base.cc:    DPRINTF(Cache, "Create %s writable: %d, dirty: %d\n", pkt->print(),
src/mem/cache/base.cc:        pkt->setHasSharers();
src/mem/cache/base.cc:    pkt->allocate();
src/mem/cache/base.cc:    pkt->setDataFromBlock(blk->data, blkSize);
src/mem/cache/base.cc:        pkt->payloadDelay = compressor->getDecompressionLatency(blk);
src/mem/cache/base.cc:    DPRINTF(Cache, "%s: MSHR %s\n", __func__, tgt_pkt->print());
src/mem/cache/base.cc:    if (writeAllocator && writeAllocator->coalesce() && tgt_pkt->isWrite()) {
src/mem/cache/base.cc:                Tick delay = blkSize / tgt_pkt->getSize() * clockPeriod();
src/mem/cache/base.cc:                        "for write coalescing\n", tgt_pkt->print(), delay);
src/mem/cache/base.cc:        assert(!pkt->isWrite());
src/mem/cache/base.cc:    pkt->pushSenderState(mshr);
src/mem/cache/base.cc:    if (pkt->isClean() && blk && blk->isSet(CacheBlk::DirtyBit)) {
src/mem/cache/base.cc:        pkt->setSatisfied();
src/mem/cache/base.cc:        bool pending_modified_resp = !pkt->hasSharers() &&
src/mem/cache/base.cc:            pkt->cacheResponding();
src/mem/cache/base.cc:        if (pkt->isClean() && blk && blk->isSet(CacheBlk::DirtyBit)) {
src/mem/cache/base.cc:                    __func__, pkt->print(), blk->print());
src/mem/cache/base.cc:            PacketPtr wb_pkt = writecleanBlk(blk, pkt->req->getDest(),
src/mem/cache/base.cc:                                             pkt->id);
src/mem/cache/base.cc:    DPRINTF(Cache, "%s: write %s\n", __func__, tgt_pkt->print());
src/mem/cache/base.cc:    assert(pkt->isResponse());
src/mem/cache/base.cc:    if (cache.system->bypassCaches() || pkt->isExpressSnoop()) {
src/mem/cache/base.cc:    assert(pkt->isRequest());
src/mem/cache/write_queue_entry.cc:        if (pkt->trySatisfyFunctional(t.pkt)) {
src/mem/cache/write_queue_entry.cc:        t.pkt->print(os, verbosity, "");
src/mem/cache/write_queue_entry.cc:    if (pkt->isPrint()) {
src/mem/cache/write_queue_entry.cc:        pkt->trySatisfyFunctional(this, blkAddr, isSecure, blkSize, nullptr);
src/mem/cache/write_queue_entry.cc:    return pkt->matchBlockAddr(blkAddr, isSecure, blkSize);
src/mem/cache/cache_blk.hh:        assert(pkt->isLLSC());
src/mem/cache/cache_blk.hh:            if (l->intersects(pkt->req))
src/mem/cache/cache_blk.hh:        lockList.emplace_front(pkt->req);
src/mem/cache/cache_blk.hh:        assert(pkt->isWrite());
src/mem/cache/cache_blk.hh:        if (!pkt->isLLSC() && lockList.empty())
src/mem/cache/cache_blk.hh:        const RequestPtr &req = pkt->req;
src/mem/cache/cache_blk.hh:        if (pkt->isLLSC()) {
src/mem/cache/cache_blk.hh:                if (l->matches(pkt->req)) {
src/mem/cache/tags/base_set_assoc.hh:        CacheBlk *blk = findBlock(pkt->getAddr(), pkt->isSecure());
src/mem/cache/tags/base.cc:    RequestorID requestor_id = pkt->req->requestorId();
src/mem/cache/tags/base.cc:    blk->insert(extractTag(pkt->getAddr()), pkt->isSecure(), requestor_id,
src/mem/cache/tags/base.cc:                pkt->req->taskId());
src/mem/cache/tags/fa_lru.cc:        static_cast<FALRUBlk*>(findBlock(pkt->getAddr(), pkt->isSecure()));
src/mem/cache/tags/sector_tags.cc:    CacheBlk *blk = findBlock(pkt->getAddr(), pkt->isSecure());
src/mem/cache/base.hh:        MSHR *mshr = mshrQueue.allocate(pkt->getBlockAddr(blkSize), blkSize,
src/mem/cache/base.hh:                                        allocOnFill(pkt->cmd));
src/mem/cache/base.hh:        assert(pkt->isWrite() || pkt->cmd == MemCmd::CleanEvict);
src/mem/cache/base.hh:        Addr blk_addr = pkt->getBlockAddr(blkSize);
src/mem/cache/base.hh:            time += pkt->payloadDelay;
src/mem/cache/base.hh:            pkt->payloadDelay = 0;
src/mem/cache/base.hh:            writeBuffer.findMatch(blk_addr, pkt->isSecure());
src/mem/cache/base.hh:            DPRINTF(Cache, "Potential to merge writeback %s", pkt->print());
src/mem/cache/base.hh:        assert(pkt->req->requestorId() < system->maxRequestors());
src/mem/cache/base.hh:        stats.cmdStats(pkt).misses[pkt->req->requestorId()]++;
src/mem/cache/base.hh:        pkt->req->incAccessDepth();
src/mem/cache/base.hh:        assert(pkt->req->requestorId() < system->maxRequestors());
src/mem/cache/base.hh:        stats.cmdStats(pkt).hits[pkt->req->requestorId()]++;
src/mem/cache/queue.hh:        pkt->pushLabel(label);
src/mem/cache/queue.hh:                pkt->popLabel();
src/mem/cache/queue.hh:        pkt->popLabel();
src/mem/cache/noncoherent_cache.cc:    assert(pkt->isRead() || pkt->isWrite());
src/mem/cache/noncoherent_cache.cc:    if (pkt->isWriteback() || pkt->cmd == MemCmd::WriteClean) {
src/mem/cache/noncoherent_cache.cc:    Addr blk_addr = pkt->getBlockAddr(blkSize);
src/mem/cache/noncoherent_cache.cc:    MSHR *mshr = mshrQueue.findMatch(blk_addr, pkt->isSecure(), false);
src/mem/cache/noncoherent_cache.cc:    panic_if(pkt->cacheResponding(), "Should not see packets where cache "
src/mem/cache/noncoherent_cache.cc:    panic_if(!(pkt->isRead() || pkt->isWrite()),
src/mem/cache/noncoherent_cache.cc:    assert(cpu_pkt->needsResponse());
src/mem/cache/noncoherent_cache.cc:    PacketPtr pkt = new Packet(cpu_pkt->req, MemCmd::ReadReq, blkSize);
src/mem/cache/noncoherent_cache.cc:    assert(pkt->getAddr() == pkt->getBlockAddr(blkSize));
src/mem/cache/noncoherent_cache.cc:    pkt->allocate();
src/mem/cache/noncoherent_cache.cc:    DPRINTF(Cache, "%s created %s from %s\n", __func__, pkt->print(),
src/mem/cache/noncoherent_cache.cc:            cpu_pkt->print());
src/mem/cache/noncoherent_cache.cc:                                         pkt->isWholeLineWrite(blkSize));
src/mem/cache/noncoherent_cache.cc:    DPRINTF(Cache, "Sending an atomic %s\n", bus_pkt->print());
src/mem/cache/noncoherent_cache.cc:    assert(bus_pkt->isResponse());
src/mem/cache/noncoherent_cache.cc:    assert(bus_pkt->isRead());
src/mem/cache/noncoherent_cache.cc:    assert(pkt->cmd != MemCmd::UpgradeResp);
src/mem/cache/noncoherent_cache.cc:    assert(!bus_pkt->isInvalidate());
src/mem/cache/noncoherent_cache.cc:    assert(!bus_pkt->hasSharers());
src/mem/cache/noncoherent_cache.cc:    DPRINTF(Cache, "Receive response: %s\n", bus_pkt->print());
src/mem/cache/noncoherent_cache.cc:    if (!bus_pkt->isError()) {
src/mem/cache/noncoherent_cache.cc:                bus_pkt->getAddr());
src/mem/cache/noncoherent_cache.cc:        blk = handleFill(bus_pkt, blk, writebacks, allocOnFill(bus_pkt->cmd));
src/mem/cache/noncoherent_cache.cc:    if (!pkt->isWriteback() && pkt->cmd != MemCmd::WriteClean) {
src/mem/cache/noncoherent_cache.cc:        assert(pkt->needsResponse());
src/mem/cache/noncoherent_cache.cc:        pkt->makeAtomicResponse();
src/mem/cache/noncoherent_cache.cc:        if (bus_pkt->isError()) {
src/mem/cache/noncoherent_cache.cc:            pkt->copyError(bus_pkt);
src/mem/cache/noncoherent_cache.cc:    panic_if(pkt->cacheResponding(), "Should not see packets where cache "
src/mem/cache/noncoherent_cache.cc:    panic_if(!(pkt->isRead() || pkt->isWrite()),
src/mem/cache/noncoherent_cache.cc:    const int initial_offset = mshr->getTarget()->pkt->getOffset(blkSize);
src/mem/cache/noncoherent_cache.cc:            completion_time = pkt->headerDelay;
src/mem/cache/noncoherent_cache.cc:            transfer_offset = tgt_pkt->getOffset(blkSize) - initial_offset;
src/mem/cache/noncoherent_cache.cc:                (transfer_offset ? pkt->payloadDelay : 0);
src/mem/cache/noncoherent_cache.cc:            assert(tgt_pkt->req->requestorId() < system->maxRequestors());
src/mem/cache/noncoherent_cache.cc:            stats.cmdStats(tgt_pkt).missLatency[tgt_pkt->req->requestorId()] +=
src/mem/cache/noncoherent_cache.cc:            tgt_pkt->makeTimingResponse();
src/mem/cache/noncoherent_cache.cc:            if (pkt->isError())
src/mem/cache/noncoherent_cache.cc:                tgt_pkt->copyError(pkt);
src/mem/cache/noncoherent_cache.cc:            tgt_pkt->headerDelay = tgt_pkt->payloadDelay = 0;
src/mem/cache/noncoherent_cache.cc:            assert(tgt_pkt->cmd == MemCmd::HardPFReq);
src/mem/cache/noncoherent_cache.cc:    assert(pkt->isResponse());
src/mem/cache/noncoherent_cache.cc:    assert(pkt->isRead());
src/mem/cache/noncoherent_cache.cc:    assert(pkt->cmd != MemCmd::UpgradeResp);
src/mem/cache/noncoherent_cache.cc:    assert(!pkt->isInvalidate());
src/mem/cache/noncoherent_cache.cc:    assert(!pkt->hasSharers());
src/mem/cache/replacement_policies/ship_rp.cc:    return static_cast<SignatureType>(pkt->getAddr() % SHCT.size());
src/mem/cache/replacement_policies/ship_rp.cc:    if (pkt->req->hasPC()) {
src/mem/cache/replacement_policies/ship_rp.cc:        signature = static_cast<SignatureType>(pkt->req->getPC());
src/mem/addr_mapper.cc:    Addr orig_addr = pkt->getAddr();
src/mem/addr_mapper.cc:    pkt->setAddr(remapAddr(orig_addr));
src/mem/addr_mapper.cc:    pkt->setAddr(orig_addr);
src/mem/addr_mapper.cc:    Addr orig_addr = pkt->getAddr();
src/mem/addr_mapper.cc:    pkt->setAddr(remapAddr(orig_addr));
src/mem/addr_mapper.cc:    pkt->setAddr(orig_addr);
src/mem/addr_mapper.cc:    Addr orig_addr = pkt->getAddr();
src/mem/addr_mapper.cc:    pkt->setAddr(remapAddr(orig_addr));
src/mem/addr_mapper.cc:    pkt->setAddr(orig_addr);
src/mem/addr_mapper.cc:    Addr orig_addr = pkt->getAddr();
src/mem/addr_mapper.cc:    pkt->setAddr(remapAddr(orig_addr));
src/mem/addr_mapper.cc:    pkt->setAddr(orig_addr);
src/mem/addr_mapper.cc:    Addr orig_addr = pkt->getAddr();
src/mem/addr_mapper.cc:    pkt->setAddr(remapAddr(orig_addr));
src/mem/addr_mapper.cc:    pkt->setAddr(orig_addr);
src/mem/addr_mapper.cc:        backdoor = getRevertedBackdoor(backdoor, pkt->getAddrRange());
src/mem/addr_mapper.cc:    Addr orig_addr = pkt->getAddr();
src/mem/addr_mapper.cc:    bool needsResponse = pkt->needsResponse();
src/mem/addr_mapper.cc:    bool cacheResponding = pkt->cacheResponding();
src/mem/addr_mapper.cc:        pkt->pushSenderState(new AddrMapperSenderState(orig_addr));
src/mem/addr_mapper.cc:    pkt->setAddr(remapAddr(orig_addr));
src/mem/addr_mapper.cc:        pkt->setAddr(orig_addr);
src/mem/addr_mapper.cc:            delete pkt->popSenderState();
src/mem/addr_mapper.cc:        dynamic_cast<AddrMapperSenderState*>(pkt->senderState);
src/mem/addr_mapper.cc:    Addr remapped_addr = pkt->getAddr();
src/mem/addr_mapper.cc:    pkt->senderState = receivedState->predecessor;
src/mem/addr_mapper.cc:    pkt->setAddr(receivedState->origAddr);
src/mem/addr_mapper.cc:        pkt->senderState = receivedState;
src/mem/addr_mapper.cc:        pkt->setAddr(remapped_addr);
src/mem/hmc_controller.cc:    assert(!pkt->isExpressSnoop());
src/mem/hmc_controller.cc:                src_port->name(), pkt->cmdString(), pkt->getAddr());
src/mem/hmc_controller.cc:            src_port->name(), pkt->cmdString(), pkt->getAddr());
src/mem/hmc_controller.cc:    unsigned int pkt_size = pkt->hasData() ? pkt->getSize() : 0;
src/mem/hmc_controller.cc:    unsigned int pkt_cmd = pkt->cmdToIndex();
src/mem/hmc_controller.cc:    Tick old_header_delay = pkt->headerDelay;
src/mem/hmc_controller.cc:    Tick packetFinishTime = clockEdge(Cycles(1)) + pkt->payloadDelay;
src/mem/hmc_controller.cc:    const bool expect_response = pkt->needsResponse() &&
src/mem/hmc_controller.cc:        !pkt->cacheResponding();
src/mem/hmc_controller.cc:                src_port->name(), pkt->cmdString(), pkt->getAddr());
src/mem/hmc_controller.cc:        pkt->headerDelay = old_header_delay;
src/mem/hmc_controller.cc:        assert(routeTo.find(pkt->req) == routeTo.end());
src/mem/hmc_controller.cc:        routeTo[pkt->req] = cpu_side_port_id;
src/mem/sys_bridge.hh:            pkt->req = data.req;
src/mem/sys_bridge.hh:            backup.req = pkt->req;
src/mem/sys_bridge.hh:                    pkt->requestorId());
src/mem/sys_bridge.hh:                    pkt->requestorId());
src/mem/sys_bridge.hh:                    pkt->requestorId());
src/mem/sys_bridge.hh:                    pkt->popSenderState());
src/mem/sys_bridge.hh:                    pkt->requestorId());
src/mem/sys_bridge.hh:                    pkt->requestorId());
src/mem/sys_bridge.hh:                        pkt->requestorId());
src/mem/sys_bridge.hh:                pkt->pushSenderState(state);
src/mem/sys_bridge.hh:                    pkt->requestorId());
src/mem/sys_bridge.hh:            pkt->pushSenderState(state);
src/mem/sys_bridge.hh:                    pkt->requestorId());
src/mem/sys_bridge.hh:                    pkt->requestorId());
src/mem/sys_bridge.hh:                    pkt->requestorId());
src/mem/sys_bridge.hh:                    pkt->requestorId());
src/mem/sys_bridge.hh:                    pkt->requestorId());
src/mem/sys_bridge.hh:            pkt->pushSenderState(state);
src/mem/sys_bridge.hh:                    pkt->requestorId());
src/mem/sys_bridge.hh:                        pkt->requestorId());
src/mem/sys_bridge.hh:                pkt->popSenderState();
src/mem/sys_bridge.hh:                    pkt->requestorId());
src/mem/sys_bridge.hh:                    pkt->requestorId());
src/mem/sys_bridge.hh:                    pkt->requestorId());
src/mem/sys_bridge.hh:                    pkt->popSenderState());
src/mem/sys_bridge.hh:                    pkt->requestorId());
src/mem/sys_bridge.hh:                    pkt->requestorId());
src/mem/sys_bridge.hh:                    pkt->requestorId());
src/mem/sys_bridge.hh:                    pkt->requestorId());
src/mem/sys_bridge.hh:                    pkt->requestorId());
src/mem/sys_bridge.hh:                    pkt->requestorId());
src/mem/sys_bridge.hh:                    pkt->requestorId());
src/mem/sys_bridge.hh:                    pkt->requestorId());
src/mem/sys_bridge.hh:                    pkt->requestorId());
src/mem/sys_bridge.hh:                    pkt->requestorId());
src/mem/sys_bridge.hh:                    pkt->requestorId());
src/mem/tport.cc:    if (pkt->cacheResponding())
src/mem/tport.cc:    bool needsResponse = pkt->needsResponse();
src/mem/tport.cc:        assert(pkt->isResponse());
src/mem/mem_ctrl.hh:          _requestorId(pkt->requestorId()),
src/mem/mem_ctrl.hh:          burstHelper(NULL), _qosValue(_pkt->qosValue())
src/mem/abstract_mem.hh:        const RequestPtr &req = pkt->req;
src/mem/abstract_mem.hh:            bool isLLSC = pkt->isLLSC();
src/mem/cfi_mem.cc:    panic_if(pkt->cacheResponding(), "Should not see packets where cache "
src/mem/cfi_mem.cc:    pkt->pushLabel(name());
src/mem/cfi_mem.cc:        done = pkt->trySatisfyFunctional(p->pkt);
src/mem/cfi_mem.cc:    pkt->popLabel();
src/mem/cfi_mem.cc:    panic_if(pkt->cacheResponding(), "Should not see packets where cache "
src/mem/cfi_mem.cc:    panic_if(!(pkt->isRead() || pkt->isWrite()),
src/mem/cfi_mem.cc:             "saw %s to %#llx\n", pkt->cmdString(), pkt->getAddr());
src/mem/cfi_mem.cc:    Tick receive_delay = pkt->headerDelay + pkt->payloadDelay;
src/mem/cfi_mem.cc:    pkt->headerDelay = pkt->payloadDelay = 0;
src/mem/cfi_mem.cc:    Tick duration = pkt->getSize() * bandwidth;
src/mem/cfi_mem.cc:    bool needs_response = pkt->needsResponse();
src/mem/cfi_mem.cc:        assert(pkt->isResponse());
src/mem/cfi_mem.cc:               !i->pkt->matchAddr(pkt)) {
src/mem/cfi_mem.cc:    if (pkt->isWrite()) {
src/mem/cfi_mem.cc:    DPRINTF(CFI, "write, address: %#x, val: %#x\n", pkt->getAddr(),
src/mem/cfi_mem.cc:        pkt->getUintX(ByteOrder::little));
src/mem/cfi_mem.cc:    const Addr flash_address = pkt->getAddr() - start();
src/mem/cfi_mem.cc:    const uint16_t value = pkt->getUintX(ByteOrder::little) & 0xffff;
src/mem/cfi_mem.cc:              flash_address, pkt->getPtr<void>(), pkt->getSize());
src/mem/cfi_mem.cc:    pkt->makeResponse();
src/mem/cfi_mem.cc:    const Addr flash_address = pkt->getAddr() - start();
src/mem/cfi_mem.cc:    pkt->setUintX(value, ByteOrder::little);
src/mem/cfi_mem.cc:    pkt->makeResponse();
src/mem/cfi_mem.cc:    DPRINTF(CFI, "read, address: %#x, val: %#x\n", pkt->getAddr(),
src/mem/cfi_mem.cc:        pkt->getUintX(ByteOrder::little));
src/mem/cfi_mem.cc:    auto host_address = parent.toHostAddr(pkt->getAddr());
src/mem/protocol/atomic.cc:    assert(pkt->isRequest());
src/mem/protocol/atomic.cc:    assert(pkt->isRequest());
src/mem/protocol/atomic.cc:    assert(pkt->isRequest());
src/mem/protocol/functional.cc:    assert(pkt->isRequest());
src/mem/protocol/functional.cc:    assert(pkt->isRequest());
src/mem/protocol/timing.cc:    assert(pkt->isRequest());
src/mem/protocol/timing.cc:  assert(pkt->isRequest());
src/mem/protocol/timing.cc:    assert(pkt->isResponse());
src/mem/protocol/timing.cc:    assert(pkt->isResponse());
src/mem/protocol/timing.cc:    assert(pkt->isRequest());
src/mem/packet_queue.cc:        if (p.pkt->matchBlockAddr(pkt, blk_size))
src/mem/packet_queue.cc:    pkt->pushLabel(label);
src/mem/packet_queue.cc:        found = pkt->trySatisfyFunctional(i->pkt);
src/mem/packet_queue.cc:    pkt->popLabel();
src/mem/packet_queue.cc:            __func__, pkt->cmdString(), pkt->getAddr(), pkt->getSize(), when,
src/mem/packet_queue.cc:    assert(!pkt->isExpressSnoop());
src/mem/packet_queue.cc:        if ((forceOrder && it->pkt->matchAddr(pkt)) || it->tick <= when) {
src/mem/coherent_xbar.hh:        return (pkt->req->isToPOC() && pointOfCoherency) ||
src/mem/coherent_xbar.hh:            (pkt->req->isToPOU() && pointOfUnification);
src/mem/packet.hh:    void copyError(Packet *pkt) { assert(pkt->isError()); cmd = pkt->cmd; }
src/mem/packet.hh:           cmd(pkt->cmd), id(pkt->id), req(pkt->req),
src/mem/packet.hh:           addr(pkt->addr), _isSecure(pkt->_isSecure), size(pkt->size),
src/mem/packet.hh:           bytesValid(pkt->bytesValid),
src/mem/packet.hh:           _qosValue(pkt->qosValue()),
src/mem/packet.hh:           headerDelay(pkt->headerDelay),
src/mem/packet.hh:           payloadDelay(pkt->payloadDelay),
src/mem/packet.hh:           senderState(pkt->senderState)
src/mem/packet.hh:            flags.set(pkt->flags & COPY_FLAGS);
src/mem/packet.hh:        flags.set(pkt->flags & (VALID_ADDR|VALID_SIZE));
src/mem/packet.hh:        if (pkt->isHtmTransactional())
src/mem/packet.hh:            setHtmTransactional(pkt->getHtmTransactionUid());
src/mem/packet.hh:        if (pkt->htmTransactionFailedInCache()) {
src/mem/packet.hh:                pkt->getHtmTransactionFailedInCacheRC()
src/mem/packet.hh:            if (pkt->flags.isSet(STATIC_DATA)) {
src/mem/packet.hh:                data = pkt->data;
src/mem/coherent_xbar.cc:    bool is_express_snoop = pkt->isExpressSnoop();
src/mem/coherent_xbar.cc:    bool cache_responding = pkt->cacheResponding();
src/mem/coherent_xbar.cc:                src_port->name(), pkt->print());
src/mem/coherent_xbar.cc:            src_port->name(), pkt->print());
src/mem/coherent_xbar.cc:    unsigned int pkt_size = pkt->hasData() ? pkt->getSize() : 0;
src/mem/coherent_xbar.cc:    unsigned int pkt_cmd = pkt->cmdToIndex();
src/mem/coherent_xbar.cc:    Tick old_header_delay = pkt->headerDelay;
src/mem/coherent_xbar.cc:    Tick packetFinishTime = clockEdge(headerLatency) + pkt->payloadDelay;
src/mem/coherent_xbar.cc:        pkt->cmd != MemCmd::WriteClean;
src/mem/coherent_xbar.cc:        assert(pkt->snoopDelay == 0);
src/mem/coherent_xbar.cc:        if (pkt->isClean() && !is_destination) {
src/mem/coherent_xbar.cc:                        src_port->name(), pkt->print());
src/mem/coherent_xbar.cc:            pkt->headerDelay += sf_res.second * clockPeriod();
src/mem/coherent_xbar.cc:                    __func__, src_port->name(), pkt->print(),
src/mem/coherent_xbar.cc:            if (pkt->isEviction()) {
src/mem/coherent_xbar.cc:                    pkt->setBlockCached();
src/mem/coherent_xbar.cc:        pkt->headerDelay += pkt->snoopDelay;
src/mem/coherent_xbar.cc:        pkt->snoopDelay = 0;
src/mem/coherent_xbar.cc:    const bool expect_snoop_resp = !cache_responding && pkt->cacheResponding();
src/mem/coherent_xbar.cc:    bool expect_response = pkt->needsResponse() && !pkt->cacheResponding();
src/mem/coherent_xbar.cc:    const Addr addr(pkt->getAddr());
src/mem/coherent_xbar.cc:                pkt->print());
src/mem/coherent_xbar.cc:            if (pkt->cacheResponding()) {
src/mem/coherent_xbar.cc:                pkt->setExpressSnoop();
src/mem/coherent_xbar.cc:            if (pkt->isWrite() && is_destination) {
src/mem/coherent_xbar.cc:                pkt->clearWriteThrough();
src/mem/coherent_xbar.cc:            assert(pkt->needsResponse());
src/mem/coherent_xbar.cc:        snoopFilter->finishRequest(!success, addr, pkt->isSecure());
src/mem/coherent_xbar.cc:        pkt->headerDelay = old_header_delay;
src/mem/coherent_xbar.cc:                src_port->name(), pkt->print());
src/mem/coherent_xbar.cc:                assert(outstandingSnoop.find(pkt->req) ==
src/mem/coherent_xbar.cc:                outstandingSnoop.insert(pkt->req);
src/mem/coherent_xbar.cc:                assert(routeTo.find(pkt->req) == routeTo.end());
src/mem/coherent_xbar.cc:                routeTo[pkt->req] = cpu_side_port_id;
src/mem/coherent_xbar.cc:        ((pkt->isClean() && pkt->satisfied()) ||
src/mem/coherent_xbar.cc:         pkt->cmd == MemCmd::WriteClean) &&
src/mem/coherent_xbar.cc:        PacketPtr deferred_rsp = pkt->isWrite() ? nullptr : pkt;
src/mem/coherent_xbar.cc:        auto cmo_lookup = outstandingCMO.find(pkt->id);
src/mem/coherent_xbar.cc:            if (pkt->isWrite()) {
src/mem/coherent_xbar.cc:                const auto route_lookup = routeTo.find(rsp_pkt->req);
src/mem/coherent_xbar.cc:            outstandingCMO.emplace(pkt->id, deferred_rsp);
src/mem/coherent_xbar.cc:            if (!pkt->isWrite()) {
src/mem/coherent_xbar.cc:                assert(routeTo.find(pkt->req) == routeTo.end());
src/mem/coherent_xbar.cc:                routeTo[pkt->req] = cpu_side_port_id;
src/mem/coherent_xbar.cc:        assert(rsp_pkt->needsResponse());
src/mem/coherent_xbar.cc:        rsp_pkt->makeResponse();
src/mem/coherent_xbar.cc:        Tick response_time = clockEdge() + pkt->headerDelay;
src/mem/coherent_xbar.cc:        rsp_pkt->headerDelay = 0;
src/mem/coherent_xbar.cc:    const auto route_lookup = routeTo.find(pkt->req);
src/mem/coherent_xbar.cc:                src_port->name(), pkt->print());
src/mem/coherent_xbar.cc:            src_port->name(), pkt->print());
src/mem/coherent_xbar.cc:    unsigned int pkt_size = pkt->hasData() ? pkt->getSize() : 0;
src/mem/coherent_xbar.cc:    unsigned int pkt_cmd = pkt->cmdToIndex();
src/mem/coherent_xbar.cc:    Tick packetFinishTime = clockEdge(headerLatency) + pkt->payloadDelay;
src/mem/coherent_xbar.cc:    Tick latency = pkt->headerDelay;
src/mem/coherent_xbar.cc:    pkt->headerDelay = 0;
src/mem/coherent_xbar.cc:            memSidePorts[mem_side_port_id]->name(), pkt->print());
src/mem/coherent_xbar.cc:    unsigned int pkt_size = pkt->hasData() ? pkt->getSize() : 0;
src/mem/coherent_xbar.cc:    transDist[pkt->cmdToIndex()]++;
src/mem/coherent_xbar.cc:    assert(pkt->isExpressSnoop());
src/mem/coherent_xbar.cc:    const bool cache_responding = pkt->cacheResponding();
src/mem/coherent_xbar.cc:    assert(pkt->snoopDelay == 0);
src/mem/coherent_xbar.cc:        pkt->headerDelay += sf_res.second * clockPeriod();
src/mem/coherent_xbar.cc:                pkt->print(), sf_res.first.size(), sf_res.second);
src/mem/coherent_xbar.cc:    pkt->headerDelay += pkt->snoopDelay;
src/mem/coherent_xbar.cc:    pkt->snoopDelay = 0;
src/mem/coherent_xbar.cc:    if (!cache_responding && pkt->cacheResponding()) {
src/mem/coherent_xbar.cc:        assert(routeTo.find(pkt->req) == routeTo.end());
src/mem/coherent_xbar.cc:        routeTo[pkt->req] = mem_side_port_id;
src/mem/coherent_xbar.cc:    const auto route_lookup = routeTo.find(pkt->req);
src/mem/coherent_xbar.cc:    const bool forwardAsSnoop = outstandingSnoop.find(pkt->req) ==
src/mem/coherent_xbar.cc:                    src_port->name(), pkt->print());
src/mem/coherent_xbar.cc:                    snoop_port->name(), pkt->print());
src/mem/coherent_xbar.cc:            src_port->name(), pkt->print());
src/mem/coherent_xbar.cc:    unsigned int pkt_size = pkt->hasData() ? pkt->getSize() : 0;
src/mem/coherent_xbar.cc:    unsigned int pkt_cmd = pkt->cmdToIndex();
src/mem/coherent_xbar.cc:    assert(!pkt->isExpressSnoop());
src/mem/coherent_xbar.cc:    Tick packetFinishTime = clockEdge(headerLatency) + pkt->payloadDelay;
src/mem/coherent_xbar.cc:        outstandingSnoop.erase(pkt->req);
src/mem/coherent_xbar.cc:                src_port->name(), pkt->print());
src/mem/coherent_xbar.cc:        Tick latency = pkt->headerDelay;
src/mem/coherent_xbar.cc:        pkt->headerDelay = 0;
src/mem/coherent_xbar.cc:    DPRINTF(CoherentXBar, "%s for %s\n", __func__, pkt->print());
src/mem/coherent_xbar.cc:            cpuSidePorts[cpu_side_port_id]->name(), pkt->print());
src/mem/coherent_xbar.cc:    unsigned int pkt_size = pkt->hasData() ? pkt->getSize() : 0;
src/mem/coherent_xbar.cc:    unsigned int pkt_cmd = pkt->cmdToIndex();
src/mem/coherent_xbar.cc:        pkt->cmd != MemCmd::WriteClean;
src/mem/coherent_xbar.cc:                    pkt->print(), sf_res.first.size(), sf_res.second);
src/mem/coherent_xbar.cc:            snoopFilter->finishRequest(false, pkt->getAddr(), pkt->isSecure());
src/mem/coherent_xbar.cc:            if (pkt->isEviction()) {
src/mem/coherent_xbar.cc:                    pkt->setBlockCached();
src/mem/coherent_xbar.cc:                pkt->print());
src/mem/coherent_xbar.cc:            if (pkt->isWrite() && is_destination) {
src/mem/coherent_xbar.cc:                pkt->clearWriteThrough();
src/mem/coherent_xbar.cc:            assert(pkt->needsResponse());
src/mem/coherent_xbar.cc:            pkt->makeResponse();
src/mem/coherent_xbar.cc:    if (!system->bypassCaches() && snoopFilter && pkt->isResponse()) {
src/mem/coherent_xbar.cc:        assert(!pkt->isResponse());
src/mem/coherent_xbar.cc:        pkt->cmd = snoop_response_cmd;
src/mem/coherent_xbar.cc:    if (pkt->isClean() && isDestination(pkt) && pkt->satisfied()) {
src/mem/coherent_xbar.cc:        auto it = outstandingCMO.find(pkt->id);
src/mem/coherent_xbar.cc:    } else if (pkt->cmd == MemCmd::WriteClean && isDestination(pkt)) {
src/mem/coherent_xbar.cc:        [[maybe_unused]] auto ret = outstandingCMO.emplace(pkt->id, nullptr);
src/mem/coherent_xbar.cc:    if (pkt->isResponse()) {
src/mem/coherent_xbar.cc:        pkt_size = pkt->hasData() ? pkt->getSize() : 0;
src/mem/coherent_xbar.cc:        pkt_cmd = pkt->cmdToIndex();
src/mem/coherent_xbar.cc:    pkt->payloadDelay = response_latency;
src/mem/coherent_xbar.cc:            memSidePorts[mem_side_port_id]->name(), pkt->print());
src/mem/coherent_xbar.cc:    unsigned int pkt_size = pkt->hasData() ? pkt->getSize() : 0;
src/mem/coherent_xbar.cc:                pkt->print(), sf_res.first.size(), sf_res.second);
src/mem/coherent_xbar.cc:        pkt->cmd = snoop_response_cmd;
src/mem/coherent_xbar.cc:    if (pkt->isResponse()) {
src/mem/coherent_xbar.cc:    pkt->payloadDelay = snoop_response_latency;
src/mem/coherent_xbar.cc:    MemCmd orig_cmd = pkt->cmd;
src/mem/coherent_xbar.cc:        if (!pkt->isResponse())
src/mem/coherent_xbar.cc:        assert(pkt->cmd != orig_cmd);
src/mem/coherent_xbar.cc:        assert(pkt->cacheResponding());
src/mem/coherent_xbar.cc:        snoop_response_cmd = pkt->cmd;
src/mem/coherent_xbar.cc:        pkt->cmd = orig_cmd;
src/mem/coherent_xbar.cc:    if (!pkt->isPrint()) {
src/mem/coherent_xbar.cc:                cpuSidePorts[cpu_side_port_id]->name(), pkt->print());
src/mem/coherent_xbar.cc:    if (!pkt->isResponse()) {
src/mem/coherent_xbar.cc:                if (pkt->needsResponse())
src/mem/coherent_xbar.cc:                    pkt->makeResponse();
src/mem/coherent_xbar.cc:    if (!pkt->isPrint()) {
src/mem/coherent_xbar.cc:                memSidePorts[mem_side_port_id]->name(), pkt->print());
src/mem/coherent_xbar.cc:            if (pkt->needsResponse())
src/mem/coherent_xbar.cc:                pkt->makeResponse();
src/mem/coherent_xbar.cc:        if (pkt->isResponse()) {
src/mem/coherent_xbar.cc:    return (pointOfCoherency && pkt->cacheResponding()) ||
src/mem/coherent_xbar.cc:        (pointOfCoherency && !(pkt->isRead() || pkt->isWrite()) &&
src/mem/coherent_xbar.cc:         !pkt->needsResponse()) ||
src/mem/coherent_xbar.cc:        (pkt->isCleanEviction() && pkt->isBlockCached()) ||
src/mem/coherent_xbar.cc:        (pkt->cacheResponding() &&
src/mem/coherent_xbar.cc:         (!pkt->needsWritable() || pkt->responderHadWritable()));
src/mem/coherent_xbar.cc:    if (pkt->isClean()) {
src/mem/coherent_xbar.cc:    return pkt->isRead() || pkt->isWrite() || !pointOfCoherency;
src/mem/mem_checker_monitor.cc:    Addr addr = pkt->getAddr();
src/mem/mem_checker_monitor.cc:    unsigned size = pkt->getSize();
src/mem/mem_checker_monitor.cc:    Addr addr = pkt->getAddr();
src/mem/mem_checker_monitor.cc:    unsigned size = pkt->getSize();
src/mem/mem_checker_monitor.cc:    assert(pkt->isRequest());
src/mem/mem_checker_monitor.cc:    bool is_read = pkt->isRead() && !pkt->req->isPrefetch();
src/mem/mem_checker_monitor.cc:    bool is_write = pkt->isWrite();
src/mem/mem_checker_monitor.cc:    unsigned size = pkt->getSize();
src/mem/mem_checker_monitor.cc:    Addr addr = pkt->getAddr();
src/mem/mem_checker_monitor.cc:    bool expects_response = pkt->needsResponse() && !pkt->cacheResponding();
src/mem/mem_checker_monitor.cc:        pkt->writeData(pkt_data.get());
src/mem/mem_checker_monitor.cc:        pkt->pushSenderState(state);
src/mem/mem_checker_monitor.cc:        delete pkt->popSenderState();
src/mem/mem_checker_monitor.cc:    assert(pkt->isResponse());
src/mem/mem_checker_monitor.cc:    bool is_read = pkt->isRead() && !pkt->req->isPrefetch();
src/mem/mem_checker_monitor.cc:    bool is_write = pkt->isWrite();
src/mem/mem_checker_monitor.cc:    bool is_failed_LLSC = pkt->isLLSC() && pkt->req->getExtraData() == 0;
src/mem/mem_checker_monitor.cc:    unsigned size = pkt->getSize();
src/mem/mem_checker_monitor.cc:    Addr addr = pkt->getAddr();
src/mem/mem_checker_monitor.cc:        pkt->writeData(pkt_data.get());
src/mem/mem_checker_monitor.cc:            dynamic_cast<MemCheckerMonitorSenderState*>(pkt->senderState);
src/mem/mem_checker_monitor.cc:        pkt->senderState = received_state->predecessor;
src/mem/mem_checker_monitor.cc:        pkt->senderState = received_state;
src/mem/comm_monitor.cc:    const bool expects_response(pkt->needsResponse() &&
src/mem/comm_monitor.cc:                                !pkt->cacheResponding());
src/mem/comm_monitor.cc:    assert(pkt->isResponse() || !expects_response);
src/mem/comm_monitor.cc:    assert(pkt->isRequest());
src/mem/comm_monitor.cc:    const bool expects_response(pkt->needsResponse() &&
src/mem/comm_monitor.cc:                                !pkt->cacheResponding());
src/mem/comm_monitor.cc:        pkt->pushSenderState(new CommMonitorSenderState(curTick()));
src/mem/comm_monitor.cc:        delete pkt->popSenderState();
src/mem/comm_monitor.cc:        DPRINTF(CommMonitor, "Forwarded %s request\n", pkt->isRead() ? "read" :
src/mem/comm_monitor.cc:                pkt->isWrite() ? "write" : "non read/write");
src/mem/comm_monitor.cc:    assert(pkt->isResponse());
src/mem/comm_monitor.cc:        dynamic_cast<CommMonitorSenderState*>(pkt->senderState);
src/mem/comm_monitor.cc:        pkt->senderState = received_state->predecessor;
src/mem/comm_monitor.cc:            pkt->senderState = received_state;
src/mem/comm_monitor.cc:        DPRINTF(CommMonitor, "Received %s response\n", pkt->isRead() ? "read" :
src/mem/comm_monitor.cc:                pkt->isWrite() ?  "write" : "non read/write");
src/mem/xbar.hh:        return findPort(pkt->getAddrRange(), pkt);
src/mem/simple_mem.cc:    panic_if(pkt->cacheResponding(), "Should not see packets where cache "
src/mem/simple_mem.cc:    pkt->pushLabel(name());
src/mem/simple_mem.cc:        done = pkt->trySatisfyFunctional(p->pkt);
src/mem/simple_mem.cc:    pkt->popLabel();
src/mem/simple_mem.cc:    panic_if(pkt->cacheResponding(), "Should not see packets where cache "
src/mem/simple_mem.cc:    panic_if(!(pkt->isRead() || pkt->isWrite()),
src/mem/simple_mem.cc:             "saw %s to %#llx\n", pkt->cmdString(), pkt->getAddr());
src/mem/simple_mem.cc:    Tick receive_delay = pkt->headerDelay + pkt->payloadDelay;
src/mem/simple_mem.cc:    pkt->headerDelay = pkt->payloadDelay = 0;
src/mem/simple_mem.cc:    Tick duration = pkt->getSize() * bandwidth;
src/mem/simple_mem.cc:    bool needsResponse = pkt->needsResponse();
src/mem/simple_mem.cc:        assert(pkt->isResponse());
src/mem/simple_mem.cc:               !i->pkt->matchAddr(pkt))
src/mem/dramsim3.cc:    return pkt->cacheResponding() ? 0 : 50000;
src/mem/dramsim3.cc:    pkt->pushLabel(name());
src/mem/dramsim3.cc:        pkt->trySatisfyFunctional(*i);
src/mem/dramsim3.cc:    pkt->popLabel();
src/mem/dramsim3.cc:    if (pkt->cacheResponding()) {
src/mem/dramsim3.cc:    if (pkt->isRead()) {
src/mem/dramsim3.cc:            outstandingReads[pkt->getAddr()].push(pkt);
src/mem/dramsim3.cc:    } else if (pkt->isWrite()) {
src/mem/dramsim3.cc:            outstandingWrites[pkt->getAddr()].push(pkt);
src/mem/dramsim3.cc:        assert(wrapper.canAccept(pkt->getAddr(), pkt->isWrite()));
src/mem/dramsim3.cc:        DPRINTF(DRAMsim3, "Enqueueing address %lld\n", pkt->getAddr());
src/mem/dramsim3.cc:        wrapper.enqueue(pkt->getAddr(), pkt->isWrite());
src/mem/dramsim3.cc:    DPRINTF(DRAMsim3, "Access for address %lld\n", pkt->getAddr());
src/mem/dramsim3.cc:    bool needsResponse = pkt->needsResponse();
src/mem/dramsim3.cc:        assert(pkt->isResponse());
src/mem/dramsim3.cc:        Tick time = curTick() + pkt->headerDelay + pkt->payloadDelay;
src/mem/dramsim3.cc:        pkt->headerDelay = pkt->payloadDelay = 0;
src/mem/dramsim3.cc:                pkt->getAddr());
src/mem/sys_bridge.cc:    RequestPtr old_req = pkt->req;
src/mem/sys_bridge.cc:    pkt->req = new_req;
src/mem/dram_interface.cc:        if (pkt->isDram() && (pkt->pseudoChannel == pseudoChannel)) {
src/mem/dram_interface.cc:            const Bank& bank = ranks[pkt->rank]->banks[pkt->bank];
src/mem/dram_interface.cc:            const Tick col_allowed_at = pkt->isRead() ? bank.rdAllowedAt :
src/mem/dram_interface.cc:                    __func__, pkt->bank, pkt->row);
src/mem/dram_interface.cc:                        pkt->bank, pkt->rank);
src/mem/dram_interface.cc:                if (bank.openRow == pkt->row) {
src/mem/dram_interface.cc:                    if (bits(earliest_banks[pkt->rank],
src/mem/dram_interface.cc:                             pkt->bank, pkt->bank)) {
src/mem/dram_interface.cc:                        pkt->bank, pkt->rank);
src/mem/dram_interface.cc:            mem_pkt->addr, mem_pkt->rank, mem_pkt->bank, mem_pkt->row);
src/mem/dram_interface.cc:    Rank& rank_ref = *ranks[mem_pkt->rank];
src/mem/dram_interface.cc:    Bank& bank_ref = rank_ref.banks[mem_pkt->bank];
src/mem/dram_interface.cc:    if (bank_ref.openRow == mem_pkt->row) {
src/mem/dram_interface.cc:        activateBank(rank_ref, bank_ref, act_tick, mem_pkt->row);
src/mem/dram_interface.cc:    const Tick col_allowed_at = mem_pkt->isRead() ?
src/mem/dram_interface.cc:    Tick max_sync = clkResyncDelay + (mem_pkt->isRead() ? tRL : tWL);
src/mem/dram_interface.cc:    if (mem_pkt->isRead()) {
src/mem/dram_interface.cc:        mem_pkt->readyTime = cmd_at + tRL + tBURST;
src/mem/dram_interface.cc:        mem_pkt->readyTime = cmd_at + tWL + tBURST;
src/mem/dram_interface.cc:            if (mem_pkt->rank == j) {
src/mem/dram_interface.cc:                    dly_to_rd_cmd = mem_pkt->isRead() ?
src/mem/dram_interface.cc:                    dly_to_wr_cmd = mem_pkt->isRead() ?
src/mem/dram_interface.cc:                    dly_to_rd_cmd = mem_pkt->isRead() ? burst_gap :
src/mem/dram_interface.cc:                    dly_to_wr_cmd = mem_pkt->isRead() ? readToWriteDelay() :
src/mem/dram_interface.cc:    activeRank = mem_pkt->rank;
src/mem/dram_interface.cc:                                 mem_pkt->isRead() ? cmd_at + tRTP :
src/mem/dram_interface.cc:                                 mem_pkt->readyTime + tWR);
src/mem/dram_interface.cc:                    bool same_rank_bank = (mem_pkt->rank == (*p)->rank) &&
src/mem/dram_interface.cc:                                          (mem_pkt->bank == (*p)->bank);
src/mem/dram_interface.cc:                    bool same_row = mem_pkt->row == (*p)->row;
src/mem/dram_interface.cc:    std::string mem_cmd = mem_pkt->isRead() ? "RD" : "WR";
src/mem/dram_interface.cc:    rank_ref.cmdList.push_back(Command(command, mem_pkt->bank, cmd_at));
src/mem/dram_interface.cc:            timeStampOffset, mem_cmd, mem_pkt->bank, mem_pkt->rank);
src/mem/dram_interface.cc:        DPRINTF(DRAM, "Auto-precharged bank: %d\n", mem_pkt->bankId);
src/mem/dram_interface.cc:    if (mem_pkt->isRead()) {
src/mem/dram_interface.cc:        stats.perBankRdBursts[mem_pkt->bankId]++;
src/mem/dram_interface.cc:        stats.totMemAccLat += mem_pkt->readyTime - mem_pkt->entryTime;
src/mem/dram_interface.cc:        stats.totQLat += cmd_at - mem_pkt->entryTime;
src/mem/dram_interface.cc:            schedule(rank_ref.writeDoneEvent, mem_pkt->readyTime);
src/mem/dram_interface.cc:        } else if (rank_ref.writeDoneEvent.when() < mem_pkt->readyTime) {
src/mem/dram_interface.cc:            reschedule(rank_ref.writeDoneEvent, mem_pkt->readyTime);
src/mem/dram_interface.cc:        stats.perBankWrBursts[mem_pkt->bankId]++;
src/mem/dramsim2.cc:    return pkt->cacheResponding() ? 0 : 50000;
src/mem/dramsim2.cc:    pkt->pushLabel(name());
src/mem/dramsim2.cc:        pkt->trySatisfyFunctional(*i);
src/mem/dramsim2.cc:    pkt->popLabel();
src/mem/dramsim2.cc:    if (pkt->cacheResponding()) {
src/mem/dramsim2.cc:    if (pkt->isRead()) {
src/mem/dramsim2.cc:            outstandingReads[pkt->getAddr()].push(pkt);
src/mem/dramsim2.cc:    } else if (pkt->isWrite()) {
src/mem/dramsim2.cc:            outstandingWrites[pkt->getAddr()].push(pkt);
src/mem/dramsim2.cc:        DPRINTF(DRAMSim2, "Enqueueing address %lld\n", pkt->getAddr());
src/mem/dramsim2.cc:        wrapper.enqueue(pkt->isWrite(), pkt->getAddr());
src/mem/dramsim2.cc:    DPRINTF(DRAMSim2, "Access for address %lld\n", pkt->getAddr());
src/mem/dramsim2.cc:    bool needsResponse = pkt->needsResponse();
src/mem/dramsim2.cc:        assert(pkt->isResponse());
src/mem/dramsim2.cc:        Tick time = curTick() + pkt->headerDelay + pkt->payloadDelay;
src/mem/dramsim2.cc:        pkt->headerDelay = pkt->payloadDelay = 0;
src/mem/dramsim2.cc:                pkt->getAddr());
src/mem/hetero_mem_ctrl.cc:    if (dram->getAddrRange().contains(pkt->getAddr())) {
src/mem/hetero_mem_ctrl.cc:    } else if (nvm->getAddrRange().contains(pkt->getAddr())) {
src/mem/hetero_mem_ctrl.cc:        panic("Can't handle address range for packet %s\n", pkt->print());
src/mem/hetero_mem_ctrl.cc:            pkt->cmdString(), pkt->getAddr(), pkt->getSize());
src/mem/hetero_mem_ctrl.cc:    panic_if(pkt->cacheResponding(), "Should not see packets where cache "
src/mem/hetero_mem_ctrl.cc:    panic_if(!(pkt->isRead() || pkt->isWrite()),
src/mem/hetero_mem_ctrl.cc:    if (dram->getAddrRange().contains(pkt->getAddr())) {
src/mem/hetero_mem_ctrl.cc:    } else if (nvm->getAddrRange().contains(pkt->getAddr())) {
src/mem/hetero_mem_ctrl.cc:              pkt->print());
src/mem/hetero_mem_ctrl.cc:    unsigned size = pkt->getSize();
src/mem/hetero_mem_ctrl.cc:    unsigned offset = pkt->getAddr() & (burst_size - 1);
src/mem/hetero_mem_ctrl.cc:    if (pkt->isWrite()) {
src/mem/hetero_mem_ctrl.cc:        assert(pkt->isRead());
src/mem/hetero_mem_ctrl.cc:            if (packetReady(mem_pkt, mem_pkt->isDram()? dram : nvm)) {
src/mem/hetero_mem_ctrl.cc:                if (packetReady(mem_pkt, mem_pkt->isDram()? dram : nvm)) {
src/mem/hetero_mem_ctrl.cc:    if (mem_pkt->isDram()) {
src/mem/hetero_mem_ctrl.cc:    if (mem_pkt->isDram()) {
src/mem/hetero_mem_ctrl.cc:        return (mem_pkt->size <= mem_intr->bytesPerBurst());
src/mem/hetero_mem_ctrl.cc:        return (mem_pkt->size <= nvm->bytesPerBurst());
src/mem/hetero_mem_ctrl.cc:        panic("Can't handle address range for packet %s\n", pkt->print());
src/mem/dram_interface.hh:        return ranks[pkt->rank]->inRefIdleState();
src/mem/hbm_ctrl.cc:    if (pc0Int->getAddrRange().contains(pkt->getAddr())) {
src/mem/hbm_ctrl.cc:    } else if (pc1Int->getAddrRange().contains(pkt->getAddr())) {
src/mem/hbm_ctrl.cc:        panic("Can't handle address range for packet %s\n", pkt->print());
src/mem/hbm_ctrl.cc:        panic("Can't handle address range for packet %s\n", pkt->print());
src/mem/hbm_ctrl.cc:    if (pc0Int && pc0Int->getAddrRange().contains(pkt->getAddr())) {
src/mem/hbm_ctrl.cc:    } else if (pc1Int && pc1Int->getAddrRange().contains(pkt->getAddr())) {
src/mem/hbm_ctrl.cc:              pkt->print());
src/mem/hbm_ctrl.cc:            pkt->cmdString(), pkt->getAddr(), pkt->getSize());
src/mem/hbm_ctrl.cc:    panic_if(pkt->cacheResponding(), "Should not see packets where cache "
src/mem/hbm_ctrl.cc:    panic_if(!(pkt->isRead() || pkt->isWrite()),
src/mem/hbm_ctrl.cc:    if (bits(pkt->getAddr(), 6) == 0) {
src/mem/hbm_ctrl.cc:    unsigned size = pkt->getSize();
src/mem/hbm_ctrl.cc:    unsigned offset = pkt->getAddr() & (burst_size - 1);
src/mem/hbm_ctrl.cc:    if (pkt->isWrite()) {
src/mem/hbm_ctrl.cc:        assert(pkt->isRead());
src/mem/ruby/slicc_interface/RubySlicc_Util.hh:    if (pkt->req->isHTMStart()) {
src/mem/ruby/slicc_interface/RubySlicc_Util.hh:    } else if (pkt->req->isHTMCommit()) {
src/mem/ruby/slicc_interface/RubySlicc_Util.hh:    } else if (pkt->req->isHTMCancel()) {
src/mem/ruby/slicc_interface/RubySlicc_Util.hh:    } else if (pkt->req->isHTMAbort()) {
src/mem/ruby/slicc_interface/RubySlicc_Util.hh:    if (pkt->req->isTlbi()) {
src/mem/ruby/slicc_interface/RubySlicc_Util.hh:    } else if (pkt->req->isTlbiSync()) {
src/mem/ruby/slicc_interface/RubySlicc_Util.hh:    } else if (pkt->req->isTlbiExtSync()) {
src/mem/ruby/slicc_interface/RubySlicc_Util.hh:    } else if (pkt->req->isTlbiExtSyncComp()) {
src/mem/ruby/slicc_interface/RubySlicc_Util.hh:    Addr pktLineAddr = makeLineAddress(pkt->getAddr());
src/mem/ruby/slicc_interface/RubySlicc_Util.hh:        uint8_t *data = pkt->getPtr<uint8_t>();
src/mem/ruby/slicc_interface/RubySlicc_Util.hh:        unsigned int size_in_bytes = pkt->getSize();
src/mem/ruby/slicc_interface/RubySlicc_Util.hh:        unsigned startByte = pkt->getAddr() - lineAddr;
src/mem/ruby/slicc_interface/RubySlicc_Util.hh:    Addr pktLineAddr = makeLineAddress(pkt->getAddr());
src/mem/ruby/slicc_interface/RubySlicc_Util.hh:        uint8_t *data = pkt->getPtr<uint8_t>();
src/mem/ruby/slicc_interface/RubySlicc_Util.hh:        unsigned int size_in_bytes = pkt->getSize();
src/mem/ruby/slicc_interface/RubySlicc_Util.hh:        unsigned startByte = pkt->getAddr() - lineAddr;
src/mem/ruby/slicc_interface/RubySlicc_Util.hh:    Addr pktLineAddr = makeLineAddress(pkt->getAddr());
src/mem/ruby/slicc_interface/RubySlicc_Util.hh:        const uint8_t *data = pkt->getConstPtr<uint8_t>();
src/mem/ruby/slicc_interface/RubySlicc_Util.hh:        unsigned int size_in_bytes = pkt->getSize();
src/mem/ruby/slicc_interface/RubySlicc_Util.hh:        unsigned startByte = pkt->getAddr() - lineAddr;
src/mem/ruby/slicc_interface/RubyRequest.cc:    if (!pkt->hasData() || !m_pkt->hasData())
src/mem/ruby/slicc_interface/RubyRequest.cc:    uint8_t *data =  m_pkt->getPtr<uint8_t>();
src/mem/ruby/slicc_interface/RubyRequest.cc:    if (pkt->isMaskedWrite() || m_pkt->isMaskedWrite()) {
src/mem/ruby/slicc_interface/RubyRequest.cc:              pkt->getAddr());
src/mem/ruby/slicc_interface/RubyRequest.cc:    Addr wBase = pkt->getAddr();
src/mem/ruby/slicc_interface/RubyRequest.cc:    Addr wTail = wBase + pkt->getSize();
src/mem/ruby/slicc_interface/RubyRequest.cc:    const uint8_t * pktData = pkt->getConstPtr<uint8_t>();
src/mem/ruby/slicc_interface/RubyRequest.hh:            m_isGLCSet = m_pkt->req->isGLCSet();
src/mem/ruby/slicc_interface/RubyRequest.hh:            m_isSLCSet = m_pkt->req->isSLCSet();
src/mem/ruby/slicc_interface/RubyRequest.hh:        assert(m_pkt->req->isMemMgmt());
src/mem/ruby/slicc_interface/RubyRequest.hh:            m_isGLCSet = m_pkt->req->isGLCSet();
src/mem/ruby/slicc_interface/RubyRequest.hh:            m_isSLCSet = m_pkt->req->isSLCSet();
src/mem/ruby/slicc_interface/RubyRequest.hh:            m_isGLCSet = m_pkt->req->isGLCSet();
src/mem/ruby/slicc_interface/RubyRequest.hh:            m_isSLCSet = m_pkt->req->isSLCSet();
src/mem/ruby/slicc_interface/RubyRequest.hh:            m_isGLCSet = m_pkt->req->isGLCSet();
src/mem/ruby/slicc_interface/RubyRequest.hh:            m_isSLCSet = m_pkt->req->isSLCSet();
src/mem/ruby/slicc_interface/RubyRequest.hh:    RequestPtr getRequestPtr() const { return m_pkt->req; }
src/mem/ruby/slicc_interface/AbstractController.cc:        pkt->allocate();
src/mem/ruby/slicc_interface/AbstractController.cc:        pkt->setData(mem_msg->m_DataBlk.getData(getOffset(mem_msg->m_addr),
src/mem/ruby/slicc_interface/AbstractController.cc:        pkt->dataDynamic(newData);
src/mem/ruby/slicc_interface/AbstractController.cc:    pkt->pushSenderState(s);
src/mem/ruby/slicc_interface/AbstractController.cc:    gem5_assert(pkt->isResponse());
src/mem/ruby/slicc_interface/AbstractController.cc:    (*msg).m_addr = pkt->getAddr();
src/mem/ruby/slicc_interface/AbstractController.cc:    SenderState *s = dynamic_cast<SenderState *>(pkt->senderState);
src/mem/ruby/slicc_interface/AbstractController.cc:    if (pkt->isRead()) {
src/mem/ruby/slicc_interface/AbstractController.cc:        (*msg).m_DataBlk.setData(pkt->getPtr<uint8_t>(), 0,
src/mem/ruby/slicc_interface/AbstractController.cc:    } else if (pkt->isWrite()) {
src/mem/ruby/network/MessageBuffer.cc:            is_read ? "read" : "write", pkt->getAddr());
src/mem/ruby/structures/RubyPrefetcherProxy.cc:            DPRINTF(HWPrefetch, "Next prefetch ready %s\n", pkt->print());
src/mem/ruby/structures/RubyPrefetcherProxy.cc:            Addr line_addr = pkt->getBlockAddr(blk_size);
src/mem/ruby/structures/RubyPrefetcherProxy.cc:                                    pkt->getAddr(), line_addr,
src/mem/ruby/structures/RubyPrefetcherProxy.cc:                                    pkt->needsWritable());
src/mem/ruby/structures/RubyPrefetcherProxy.cc:                RubyRequestType req_type = pkt->needsWritable() ?
src/mem/ruby/structures/RubyPrefetcherProxy.cc:                                                  pkt->getAddr(),
src/mem/ruby/common/DataBlock.cc:    int offset = getOffset(pkt->getAddr());
src/mem/ruby/common/DataBlock.cc:    assert(offset + pkt->getSize() <= RubySystem::getBlockSizeBytes());
src/mem/ruby/common/DataBlock.cc:    pkt->writeData(&m_data[offset]);
src/mem/ruby/system/GPUCoalescer.cc:    uint64_t seqNum = pkt->req->getReqInstSeqNum();
src/mem/ruby/system/GPUCoalescer.cc:            pkt->getAddr(), seqNum, instMap.size(), instMap[seqNum].size());
src/mem/ruby/system/GPUCoalescer.cc:    uint64_t seqNum = pkt->req->getReqInstSeqNum();
src/mem/ruby/system/GPUCoalescer.cc:            if (current_time - pkt->req->time() > threshold) {
src/mem/ruby/system/GPUCoalescer.cc:                      pkt->getAddr(), instMap.size(), current_time,
src/mem/ruby/system/GPUCoalescer.cc:                      pkt->req->time(), current_time - pkt->req->time(),
src/mem/ruby/system/GPUCoalescer.cc:    Addr request_address = pkt->getAddr();
src/mem/ruby/system/GPUCoalescer.cc:        offset = getOffset(pkt->getAddr());
src/mem/ruby/system/GPUCoalescer.cc:        pkt_size = pkt->getSize();
src/mem/ruby/system/GPUCoalescer.cc:        request_address = pkt->getAddr();
src/mem/ruby/system/GPUCoalescer.cc:        if (pkt->getPtr<uint8_t>()) {
src/mem/ruby/system/GPUCoalescer.cc:                    assert(pkt->isAtomicOp());
src/mem/ruby/system/GPUCoalescer.cc:                    pkt->setData(data.getData(offset, pkt_size));
src/mem/ruby/system/GPUCoalescer.cc:                    assert(pkt->isAtomicOp());
src/mem/ruby/system/GPUCoalescer.cc:                    pkt->setData(&log[offset]);
src/mem/ruby/system/GPUCoalescer.cc:    assert(!pkt->req->isLLSC());
src/mem/ruby/system/GPUCoalescer.cc:    assert(!pkt->req->isLockedRMW());
src/mem/ruby/system/GPUCoalescer.cc:    assert(!pkt->req->isInstFetch());
src/mem/ruby/system/GPUCoalescer.cc:    if (pkt->req->isAtomicReturn()) {
src/mem/ruby/system/GPUCoalescer.cc:    } else if (pkt->req->isAtomicNoReturn()) {
src/mem/ruby/system/GPUCoalescer.cc:    } else if (pkt->isRead()) {
src/mem/ruby/system/GPUCoalescer.cc:    } else if (pkt->isWrite()) {
src/mem/ruby/system/GPUCoalescer.cc:    } else if (pkt->isFlush()) {
src/mem/ruby/system/GPUCoalescer.cc:    assert(pkt->req->hasInstSeqNum());
src/mem/ruby/system/GPUCoalescer.cc:    if (pkt->cmd == MemCmd::MemSyncReq) {
src/mem/ruby/system/GPUCoalescer.cc:        assert(pkt->isRead() || pkt->isWrite() || pkt->isFlush());
src/mem/ruby/system/GPUCoalescer.cc:        InstSeqNum seq_num = pkt->req->getReqInstSeqNum();
src/mem/ruby/system/GPUCoalescer.cc:                pkt->getAddr());
src/mem/ruby/system/GPUCoalescer.cc:            safe_cast<RubyPort::SenderState*>(pkt->senderState);
src/mem/ruby/system/GPUCoalescer.cc:    uint64_t seqNum = pkt->req->getReqInstSeqNum();
src/mem/ruby/system/GPUCoalescer.cc:    Addr line_addr = makeLineAddress(pkt->getAddr());
src/mem/ruby/system/GPUCoalescer.cc:        if (pkt->cmd == MemCmd::WriteReq) {
src/mem/ruby/system/GPUCoalescer.cc:                    safe_cast<RubyPort::SenderState*>(pkt->senderState);
src/mem/ruby/system/GPUCoalescer.cc:                safe_cast<RubyPort::SenderState *>(pkt->senderState);
src/mem/ruby/system/GPUCoalescer.cc:            pkt->senderState = ss->predecessor;
src/mem/ruby/system/GPUCoalescer.cc:            if (pkt->cmd != MemCmd::WriteReq) {
src/mem/ruby/system/CacheRecorder.cc:        pkt->req->setReqInstSeqNum(m_records_flushed);
src/mem/ruby/system/CacheRecorder.cc:            pkt->dataStatic(traceRecord->m_data + rec_bytes_read);
src/mem/ruby/system/CacheRecorder.cc:            pkt->req->setReqInstSeqNum(m_records_read);
src/mem/ruby/system/HTMSequencer.cc:        uint8_t* dataptr = pkt->getPtr<uint8_t>();
src/mem/ruby/system/HTMSequencer.cc:        memset(dataptr, 0, pkt->getSize());
src/mem/ruby/system/HTMSequencer.cc:            if (pkt->req->isHTMStart()) {
src/mem/ruby/system/HTMSequencer.cc:                m_htmstart_tick = pkt->req->time();
src/mem/ruby/system/HTMSequencer.cc:                m_htmstart_instruction = pkt->req->getInstCount();
src/mem/ruby/system/HTMSequencer.cc:                        pkt->getHtmTransactionUid());
src/mem/ruby/system/HTMSequencer.cc:            } else if (pkt->req->isHTMCommit()) {
src/mem/ruby/system/HTMSequencer.cc:                Tick transaction_ticks = pkt->req->time() - m_htmstart_tick;
src/mem/ruby/system/HTMSequencer.cc:                    pkt->req->getInstCount() - m_htmstart_instruction;
src/mem/ruby/system/HTMSequencer.cc:                        pkt->getHtmTransactionUid());
src/mem/ruby/system/HTMSequencer.cc:            } else if (pkt->req->isHTMAbort()) {
src/mem/ruby/system/HTMSequencer.cc:                HtmFailureFaultCause cause = pkt->req->getHtmAbortCause();
src/mem/ruby/system/HTMSequencer.cc:                        pkt->getHtmTransactionUid());
src/mem/ruby/system/HTMSequencer.cc:                pkt->getHtmTransactionUid());
src/mem/ruby/system/HTMSequencer.cc:                pkt->req->setExtraData(0);
src/mem/ruby/system/HTMSequencer.cc:                            pkt->getSize(),
src/mem/ruby/system/HTMSequencer.cc:                            address, pkt->getHtmTransactionUid());
src/mem/ruby/system/HTMSequencer.cc:    assert(system->isMemAddr(pkt->getAddr()) || system->isDeviceMemAddr(pkt));
src/mem/ruby/system/HTMSequencer.cc:    assert(pkt->isRequest());
src/mem/ruby/system/HTMSequencer.cc:        safe_cast<RubyPort::SenderState *>(pkt->popSenderState());
src/mem/ruby/system/HTMSequencer.cc:            pkt->req->isHTMStart(), pkt->req->isHTMCommit(),
src/mem/ruby/system/HTMSequencer.cc:            pkt->req->isHTMCancel(), htm_return_code);
src/mem/ruby/system/HTMSequencer.cc:    if (pkt->needsResponse()) {
src/mem/ruby/system/HTMSequencer.cc:        pkt->makeHtmTransactionalReqResponse(
src/mem/ruby/system/HTMSequencer.cc:        if ((m_htmCmdRequestTable.size() > 0) && !pkt->req->isHTMAbort())
src/mem/ruby/system/GPUCoalescer.hh:            pkt->senderState = ss;
src/mem/ruby/system/RubyPort.cc:    DPRINTF(RubyPort, "Response for address: 0x%#x\n", pkt->getAddr());
src/mem/ruby/system/RubyPort.cc:    assert(pkt->isResponse());
src/mem/ruby/system/RubyPort.cc:    assert(!pkt->htmTransactionFailedInCache());
src/mem/ruby/system/RubyPort.cc:        safe_cast<RubyPort::SenderState *>(pkt->popSenderState());
src/mem/ruby/system/RubyPort.cc:            pkt->getAddr(), port->name());
src/mem/ruby/system/RubyPort.cc:            if (it->contains(pkt->getAddr())) {
src/mem/ruby/system/RubyPort.cc:            if (it->contains(pkt->getAddr())) {
src/mem/ruby/system/RubyPort.cc:            pkt->getAddr(), id);
src/mem/ruby/system/RubyPort.cc:    if (pkt->cacheResponding())
src/mem/ruby/system/RubyPort.cc:    if (pkt->req->isCacheMaintenance()) {
src/mem/ruby/system/RubyPort.cc:        pkt->makeResponse();
src/mem/ruby/system/RubyPort.cc:    if (pkt->cmd != MemCmd::MemSyncReq) {
src/mem/ruby/system/RubyPort.cc:        if (!pkt->req->isMemMgmt() && !isPhysMemAddress(pkt)) {
src/mem/ruby/system/RubyPort.cc:                    "pio address\n", pkt->getAddr());
src/mem/ruby/system/RubyPort.cc:            pkt->pushSenderState(new SenderState(this));
src/mem/ruby/system/RubyPort.cc:    pkt->pushSenderState(new SenderState(this));
src/mem/ruby/system/RubyPort.cc:        DPRINTF(RubyPort, "Request %s 0x%x issued\n", pkt->cmdString(),
src/mem/ruby/system/RubyPort.cc:                pkt->getAddr());
src/mem/ruby/system/RubyPort.cc:    SenderState *ss = safe_cast<SenderState *>(pkt->popSenderState());
src/mem/ruby/system/RubyPort.cc:    if (pkt->cmd != MemCmd::MemSyncReq) {
src/mem/ruby/system/RubyPort.cc:                pkt->cmdString(), pkt->getAddr(),
src/mem/ruby/system/RubyPort.cc:    if (pkt->cmd != MemCmd::MemSyncReq) {
src/mem/ruby/system/RubyPort.cc:                    "pio address\n", pkt->getAddr());
src/mem/ruby/system/RubyPort.cc:            pkt->pushSenderState(new SenderState(this));
src/mem/ruby/system/RubyPort.cc:        assert(getOffset(pkt->getAddr()) + pkt->getSize() <=
src/mem/ruby/system/RubyPort.cc:                    pkt->getAddr(), (MachineType)mem_interface_type);
src/mem/ruby/system/RubyPort.cc:    DPRINTF(RubyPort, "Functional access for address: %#x\n", pkt->getAddr());
src/mem/ruby/system/RubyPort.cc:        DPRINTF(RubyPort, "Pio Request for address: 0x%#x\n", pkt->getAddr());
src/mem/ruby/system/RubyPort.cc:    assert(pkt->getAddr() + pkt->getSize() <=
src/mem/ruby/system/RubyPort.cc:           makeLineAddress(pkt->getAddr()) + RubySystem::getBlockSizeBytes());
src/mem/ruby/system/RubyPort.cc:        bool needsResponse = pkt->needsResponse();
src/mem/ruby/system/RubyPort.cc:        if (pkt->isRead()) {
src/mem/ruby/system/RubyPort.cc:        } else if (pkt->isWrite()) {
src/mem/ruby/system/RubyPort.cc:            panic("Unsupported functional command %s\n", pkt->cmdString());
src/mem/ruby/system/RubyPort.cc:        if (!accessSucceeded && !pkt->suppressFuncError()) {
src/mem/ruby/system/RubyPort.cc:                  pkt->isWrite() ? "write" : "read", pkt->getAddr());
src/mem/ruby/system/RubyPort.cc:            if (!pkt->isResponse())
src/mem/ruby/system/RubyPort.cc:                pkt->makeResponse();
src/mem/ruby/system/RubyPort.cc:            pkt->setFunctionalResponseStatus(accessSucceeded);
src/mem/ruby/system/RubyPort.cc:    DPRINTF(RubyPort, "Hit callback for %s 0x%x\n", pkt->cmdString(),
src/mem/ruby/system/RubyPort.cc:            pkt->getAddr());
src/mem/ruby/system/RubyPort.cc:    assert(system->isMemAddr(pkt->getAddr()) || system->isDeviceMemAddr(pkt));
src/mem/ruby/system/RubyPort.cc:    assert(pkt->isRequest());
src/mem/ruby/system/RubyPort.cc:        safe_cast<RubyPort::SenderState *>(pkt->popSenderState());
src/mem/ruby/system/RubyPort.cc:    DPRINTF(RubyPort, "Unaddressed callback for %s\n", pkt->cmdString());
src/mem/ruby/system/RubyPort.cc:    assert(pkt->isRequest());
src/mem/ruby/system/RubyPort.cc:        safe_cast<RubyPort::SenderState *>(pkt->popSenderState());
src/mem/ruby/system/RubyPort.cc:    bool needsResponse = pkt->needsResponse();
src/mem/ruby/system/RubyPort.cc:    if (pkt->isLLSC()) {
src/mem/ruby/system/RubyPort.cc:        if (pkt->isWrite()) {
src/mem/ruby/system/RubyPort.cc:            if (pkt->req->getExtraData() != 0) {
src/mem/ruby/system/RubyPort.cc:                pkt->convertScToWrite();
src/mem/ruby/system/RubyPort.cc:            pkt->convertLlToRead();
src/mem/ruby/system/RubyPort.cc:    if (pkt->isFlush() || pkt->cmd == MemCmd::MemSyncReq
src/mem/ruby/system/RubyPort.cc:        || pkt->cmd == MemCmd::WriteCompleteResp) {
src/mem/ruby/system/RubyPort.cc:    if (pkt->req->isKernel()) {
src/mem/ruby/system/RubyPort.cc:        } else if (owner.system->isMemAddr(pkt->getAddr())) {
src/mem/ruby/system/RubyPort.cc:        pkt->makeResponse();
src/mem/ruby/system/RubyPort.cc:    if (needsResponse || pkt->isResponse()) {
src/mem/ruby/system/RubyPort.cc:    Addr addr = pkt->getAddr();
src/mem/ruby/system/Sequencer.hh:        assert(func_pkt->isWrite());
src/mem/ruby/system/Sequencer.hh:        return func_pkt->trySatisfyFunctional(pkt);
src/mem/ruby/system/DMASequencer.cc:    Addr paddr = pkt->getAddr();
src/mem/ruby/system/DMASequencer.cc:    uint8_t* data =  pkt->getPtr<uint8_t>();
src/mem/ruby/system/DMASequencer.cc:    int len = pkt->getSize();
src/mem/ruby/system/DMASequencer.cc:    bool write = pkt->isWrite();
src/mem/ruby/system/DMASequencer.cc:    assert(!pkt->isMaskedWrite());
src/mem/ruby/system/DMASequencer.cc:    if (pkt->req->isAtomic()) {
src/mem/ruby/system/DMASequencer.cc:        int atomic_offset = pkt->getAddr() - line_addr;
src/mem/ruby/system/DMASequencer.cc:        assert(atomic_offset + pkt->getSize() <= block_size);
src/mem/ruby/system/DMASequencer.cc:        for (int idx = 0; idx < pkt->getSize(); ++idx) {
src/mem/ruby/system/DMASequencer.cc:            atomic_op(atomic_offset, pkt->getAtomicOp());
src/mem/ruby/system/DMASequencer.cc:        assert(pkt->isRead());
src/mem/ruby/system/DMASequencer.cc:    memcpy(pkt->getPtr<uint8_t>(), dblk.getData(offset, pkt->getSize()),
src/mem/ruby/system/DMASequencer.cc:           pkt->getSize());
src/mem/ruby/system/Sequencer.cc:                  seq_req.pkt->getAddr(), table_entry.second.size(),
src/mem/ruby/system/Sequencer.cc:    Addr line_addr = makeLineAddress(pkt->getAddr());
src/mem/ruby/system/Sequencer.cc:             "", "", printAddress(srequest->pkt->getAddr()), total_lat);
src/mem/ruby/system/Sequencer.cc:                seq_req.pkt->req->setExtraData(success ? 1 : 0);
src/mem/ruby/system/Sequencer.cc:    Addr request_address(pkt->getAddr());
src/mem/ruby/system/Sequencer.cc:        m_controller->notifyCoalesced(request_address, type, pkt->req,
src/mem/ruby/system/Sequencer.cc:    } else if (!pkt->isFlush()) {
src/mem/ruby/system/Sequencer.cc:            pkt->setData(
src/mem/ruby/system/Sequencer.cc:                data.getData(getOffset(request_address), pkt->getSize()));
src/mem/ruby/system/Sequencer.cc:        } else if (pkt->req->isSwap()) {
src/mem/ruby/system/Sequencer.cc:            assert(!pkt->isMaskedWrite());
src/mem/ruby/system/Sequencer.cc:            std::vector<uint8_t> overwrite_val(pkt->getSize());
src/mem/ruby/system/Sequencer.cc:            pkt->writeData(&overwrite_val[0]);
src/mem/ruby/system/Sequencer.cc:            pkt->setData(
src/mem/ruby/system/Sequencer.cc:                data.getData(getOffset(request_address), pkt->getSize()));
src/mem/ruby/system/Sequencer.cc:                         getOffset(request_address), pkt->getSize());
src/mem/ruby/system/Sequencer.cc:        } else if (pkt->isAtomicOp()) {
src/mem/ruby/system/Sequencer.cc:            pkt->setData(
src/mem/ruby/system/Sequencer.cc:                data.getData(getOffset(request_address), pkt->getSize()));
src/mem/ruby/system/Sequencer.cc:            (*(pkt->getAtomicOp()))(
src/mem/ruby/system/Sequencer.cc:                pkt->cmdString(), pkt->getAddr());
src/mem/ruby/system/Sequencer.cc:            pkt->findNextSenderState<RubyTester::SenderState>();
src/mem/ruby/system/Sequencer.cc:        assert(pkt->req);
src/mem/ruby/system/Sequencer.cc:        !pkt->req->isHTMAbort()) {
src/mem/ruby/system/Sequencer.cc:    if (pkt->isLLSC()) {
src/mem/ruby/system/Sequencer.cc:        if (pkt->isWrite()) {
src/mem/ruby/system/Sequencer.cc:            assert(pkt->isRead());
src/mem/ruby/system/Sequencer.cc:    } else if (pkt->req->isLockedRMW()) {
src/mem/ruby/system/Sequencer.cc:        if (pkt->isWrite()) {
src/mem/ruby/system/Sequencer.cc:            assert(pkt->isRead());
src/mem/ruby/system/Sequencer.cc:    } else if (pkt->req->isTlbiCmd()) {
src/mem/ruby/system/Sequencer.cc:    } else if (pkt->isAtomicOp()) {
src/mem/ruby/system/Sequencer.cc:        if (pkt->req->isAtomicReturn()){
src/mem/ruby/system/Sequencer.cc:        if (pkt->isWrite()) {
src/mem/ruby/system/Sequencer.cc:        } else if (pkt->isRead()) {
src/mem/ruby/system/Sequencer.cc:            if (pkt->req->isHTMCmd()) {
src/mem/ruby/system/Sequencer.cc:            } else if (pkt->req->isInstFetch()) {
src/mem/ruby/system/Sequencer.cc:                if (pkt->req->isReadModifyWrite()) {
src/mem/ruby/system/Sequencer.cc:        } else if (pkt->isFlush()) {
src/mem/ruby/system/Sequencer.cc:    if (!pkt->req->isMemMgmt() &&
src/mem/ruby/system/Sequencer.cc:        m_controller->isBlocked(makeLineAddress(pkt->getAddr())) &&
src/mem/ruby/system/Sequencer.cc:    ContextID proc_id = pkt->req->hasContextId() ?
src/mem/ruby/system/Sequencer.cc:        pkt->req->contextId() : InvalidContextID;
src/mem/ruby/system/Sequencer.cc:    if (pkt->req->hasPC()) {
src/mem/ruby/system/Sequencer.cc:        pc = pkt->req->getPC();
src/mem/ruby/system/Sequencer.cc:    if (pkt->req->isMemMgmt()) {
src/mem/ruby/system/Sequencer.cc:        if (pkt->req->isTlbiCmd()) {
src/mem/ruby/system/Sequencer.cc:                msg->m_tlbiTransactionUid = pkt->req->getExtraData();
src/mem/ruby/system/Sequencer.cc:        msg = std::make_shared<RubyRequest>(clockEdge(), pkt->getAddr(),
src/mem/ruby/system/Sequencer.cc:                                            pkt->getSize(), pc, secondary_type,
src/mem/ruby/system/Sequencer.cc:        if (pkt->isAtomicOp() &&
src/mem/ruby/system/Sequencer.cc:            uint32_t offset = getOffset(pkt->getAddr());
src/mem/ruby/system/Sequencer.cc:                                (offset, pkt->getAtomicOp()));
src/mem/ruby/system/Sequencer.cc:            msg->setWriteMask(offset, pkt->getSize(), atomicOps);
src/mem/ruby/system/Sequencer.cc:    if (pkt->isHtmTransactional()) {
src/mem/ruby/system/Sequencer.cc:        msg->m_htmTransactionUid = pkt->getHtmTransactionUid();
src/mem/ruby/system/RubySystem.cc:    Addr address(pkt->getAddr());
src/mem/ruby/system/RubySystem.cc:    assert(requestorToNetwork.count(pkt->requestorId()));
src/mem/ruby/system/RubySystem.cc:    int request_net_id = requestorToNetwork[pkt->requestorId()];
src/mem/ruby/system/RubySystem.cc:    Addr address(pkt->getAddr());
src/mem/ruby/system/RubySystem.cc:    Addr addr(pkt->getAddr());
src/mem/ruby/system/RubySystem.cc:    assert(requestorToNetwork.count(pkt->requestorId()));
src/mem/ruby/system/RubySystem.cc:    int request_net_id = requestorToNetwork[pkt->requestorId()];
src/mem/ruby/system/VIPERCoalescer.cc:    assert((pkt->cmd == MemCmd::MemSyncReq && pkt->req->isInvL1()) ||
src/mem/ruby/system/VIPERCoalescer.cc:            pkt->cmd == MemCmd::ReadReq ||
src/mem/ruby/system/VIPERCoalescer.cc:            pkt->cmd == MemCmd::WriteReq ||
src/mem/ruby/system/VIPERCoalescer.cc:            pkt->cmd == MemCmd::FlushReq ||
src/mem/ruby/system/VIPERCoalescer.cc:            pkt->isAtomicOp());
src/mem/ruby/system/VIPERCoalescer.cc:    if (pkt->req->isInvL1() && m_cache_inv_pkt) {
src/mem/ruby/system/VIPERCoalescer.cc:    if (pkt->req->isInvL1()) {
src/mem/ruby/system/VIPERCoalescer.cc:    if (pkt != NULL && pkt->req->hasContextId()) {
src/mem/ruby/system/VIPERCoalescer.cc:        proc_id = pkt->req->contextId();
src/mem/ruby/system/VIPERCoalescer.cc:    if (pkt->req->hasPC()) {
src/mem/ruby/system/VIPERCoalescer.cc:        pc = pkt->req->getPC();
src/mem/ruby/system/VIPERCoalescer.cc:    Addr line_addr = makeLineAddress(pkt->getAddr());
src/mem/ruby/system/VIPERCoalescer.cc:    if (pkt->isAtomicOp()) {
src/mem/ruby/system/VIPERCoalescer.cc:        msg = std::make_shared<RubyRequest>(clockEdge(), pkt->getAddr(),
src/mem/ruby/system/VIPERCoalescer.cc:                              pkt->getSize(), pc, crequest->getRubyType(),
src/mem/ruby/system/VIPERCoalescer.cc:        msg = std::make_shared<RubyRequest>(clockEdge(), pkt->getAddr(),
src/mem/ruby/system/VIPERCoalescer.cc:                              pkt->getSize(), pc, crequest->getRubyType(),
src/mem/ruby/system/VIPERCoalescer.cc:    if (pkt->cmd == MemCmd::WriteReq) {
src/mem/ruby/system/VIPERCoalescer.cc:        assert(pkt->cmd == MemCmd::WriteReq);
src/mem/ruby/system/VIPERCoalescer.cc:        PacketPtr writeCompletePkt = new Packet(pkt->req,
src/mem/ruby/system/VIPERCoalescer.cc:        writeCompletePkt->setAddr(pkt->getAddr());
src/mem/ruby/system/VIPERCoalescer.cc:        writeCompletePkt->senderState = pkt->senderState;
src/mem/qos/mem_ctrl.hh:                pkt->requestorId(), pkt->getAddr());
src/mem/qos/mem_ctrl.hh:        if (pkt->requestorId() == id) {
src/mem/qos/mem_ctrl.hh:            uint64_t moved_entries = divCeil(pkt->getSize(),
src/mem/qos/mem_ctrl.hh:                    requestors[id], id, pkt->getAddr(),
src/mem/qos/mem_ctrl.hh:                    pkt->getSize(),
src/mem/qos/mem_ctrl.hh:            if (pkt->isRead()) {
src/mem/qos/mem_ctrl.hh:            } else if (pkt->isWrite()) {
src/mem/qos/mem_ctrl.hh:            pkt->qosValue(tgt_prio);
src/mem/qos/mem_ctrl.hh:    pkt->qosValue(pkt_priority);
src/mem/qos/mem_ctrl.hh:            if (requestor.first == pkt->requestorId())
src/mem/qos/mem_ctrl.hh:                _system->getRequestorName(pkt->requestorId()),
src/mem/qos/mem_ctrl.hh:        escalate(queues, queue_entry_size, pkt->requestorId(), pkt_priority);
src/mem/qos/policy.cc:    assert(pkt->req);
src/mem/qos/policy.cc:    return schedule(pkt->req->requestorId(), pkt->getSize());
src/mem/qos/q_policy.cc:        panic_if(!pkt->req,
src/mem/qos/q_policy.cc:        RequestorID requestor_id = pkt->req->requestorId();
src/mem/qos/q_policy.cc:    RequestorID requestor_id = pkt->requestorId();
src/mem/qos/mem_sink.cc:    panic_if(pkt->cacheResponding(),
src/mem/qos/mem_sink.cc:    pkt->pushLabel(name());
src/mem/qos/mem_sink.cc:    pkt->popLabel();
src/mem/qos/mem_sink.cc:    panic_if(!(pkt->isRead() || pkt->isWrite()),
src/mem/qos/mem_sink.cc:    panic_if(pkt->cacheResponding(),
src/mem/qos/mem_sink.cc:            _system->getRequestorName(pkt->req->requestorId()),
src/mem/qos/mem_sink.cc:            pkt->cmdString(), pkt->getAddr(), pkt->getSize());
src/mem/qos/mem_sink.cc:    uint64_t required_entries = divCeil(pkt->getSize(), memoryPacketSize);
src/mem/qos/mem_sink.cc:    if (pkt->isRead()) {
src/mem/qos/mem_sink.cc:        logRequest(pkt->isRead()? READ : WRITE,
src/mem/qos/mem_sink.cc:                   pkt->req->requestorId(),
src/mem/qos/mem_sink.cc:                   pkt->qosValue(),
src/mem/qos/mem_sink.cc:                   pkt->getAddr(),
src/mem/qos/mem_sink.cc:                    "priority queue %d\n", __func__, pkt->getAddr(),
src/mem/qos/mem_sink.cc:                    _system->getRequestorName(pkt->req->requestorId()),
src/mem/qos/mem_sink.cc:    uint64_t removed_entries = divCeil(pkt->getSize(), memoryPacketSize);
src/mem/qos/mem_sink.cc:            "corresponds to %d memory packets\n", __func__, pkt->getAddr(),
src/mem/qos/mem_sink.cc:            _system->getRequestorName(pkt->req->requestorId()),
src/mem/qos/mem_sink.cc:            pkt->getSize(), removed_entries);
src/mem/qos/mem_sink.cc:    panic_if(!pkt->needsResponse(),
src/mem/qos/mem_sink.cc:    logResponse(pkt->isRead()? READ : WRITE,
src/mem/qos/mem_sink.cc:                pkt->req->requestorId(),
src/mem/qos/mem_sink.cc:                pkt->qosValue(),
src/mem/qos/mem_sink.cc:                pkt->getAddr(),
src/mem/qos/mem_sink.cc:    pkt->pushLabel(mem.name());
src/mem/qos/mem_sink.cc:    pkt->popLabel();
src/mem/qos/mem_ctrl.cc:    assert(pkt->req);
src/mem/qos/mem_ctrl.cc:        return schedule(pkt->req->requestorId(), pkt->getSize());
src/mem/qos/mem_ctrl.cc:                pkt->qosValue());
src/mem/qos/mem_ctrl.cc:        return pkt->qosValue();
src/mem/port.cc:    auto ext = pkt->getExtension<TracingExtension>();
src/mem/port.cc:        pkt->setExtension(ext);
src/mem/port.cc:    ext->add(name(), _responsePort->name(), pkt->getAddr());
src/mem/port.cc:    auto ext = pkt->getExtension<TracingExtension>();
src/mem/noncoherent_xbar.cc:    assert(!pkt->isExpressSnoop());
src/mem/noncoherent_xbar.cc:                src_port->name(), pkt->cmdString(), pkt->getAddr());
src/mem/noncoherent_xbar.cc:            src_port->name(), pkt->cmdString(), pkt->getAddr());
src/mem/noncoherent_xbar.cc:    unsigned int pkt_size = pkt->hasData() ? pkt->getSize() : 0;
src/mem/noncoherent_xbar.cc:    unsigned int pkt_cmd = pkt->cmdToIndex();
src/mem/noncoherent_xbar.cc:    Tick old_header_delay = pkt->headerDelay;
src/mem/noncoherent_xbar.cc:    Tick packetFinishTime = clockEdge(Cycles(1)) + pkt->payloadDelay;
src/mem/noncoherent_xbar.cc:    const bool expect_response = pkt->needsResponse() &&
src/mem/noncoherent_xbar.cc:        !pkt->cacheResponding();
src/mem/noncoherent_xbar.cc:                src_port->name(), pkt->cmdString(), pkt->getAddr());
src/mem/noncoherent_xbar.cc:        pkt->headerDelay = old_header_delay;
src/mem/noncoherent_xbar.cc:        assert(routeTo.find(pkt->req) == routeTo.end());
src/mem/noncoherent_xbar.cc:        routeTo[pkt->req] = cpu_side_port_id;
src/mem/noncoherent_xbar.cc:    const auto route_lookup = routeTo.find(pkt->req);
src/mem/noncoherent_xbar.cc:                src_port->name(), pkt->cmdString(), pkt->getAddr());
src/mem/noncoherent_xbar.cc:            src_port->name(), pkt->cmdString(), pkt->getAddr());
src/mem/noncoherent_xbar.cc:    unsigned int pkt_size = pkt->hasData() ? pkt->getSize() : 0;
src/mem/noncoherent_xbar.cc:    unsigned int pkt_cmd = pkt->cmdToIndex();
src/mem/noncoherent_xbar.cc:    Tick packetFinishTime = clockEdge(Cycles(1)) + pkt->payloadDelay;
src/mem/noncoherent_xbar.cc:    Tick latency = pkt->headerDelay;
src/mem/noncoherent_xbar.cc:    pkt->headerDelay = 0;
src/mem/noncoherent_xbar.cc:            cpuSidePorts[cpu_side_port_id]->name(), pkt->getAddr(),
src/mem/noncoherent_xbar.cc:            pkt->cmdString());
src/mem/noncoherent_xbar.cc:    unsigned int pkt_size = pkt->hasData() ? pkt->getSize() : 0;
src/mem/noncoherent_xbar.cc:    unsigned int pkt_cmd = pkt->cmdToIndex();
src/mem/noncoherent_xbar.cc:    if (pkt->isResponse()) {
src/mem/noncoherent_xbar.cc:        pkt_size = pkt->hasData() ? pkt->getSize() : 0;
src/mem/noncoherent_xbar.cc:        pkt_cmd = pkt->cmdToIndex();
src/mem/noncoherent_xbar.cc:    pkt->payloadDelay = response_latency;
src/mem/noncoherent_xbar.cc:    if (!pkt->isPrint()) {
src/mem/noncoherent_xbar.cc:                cpuSidePorts[cpu_side_port_id]->name(), pkt->getAddr(),
src/mem/noncoherent_xbar.cc:                pkt->cmdString());
src/mem/noncoherent_xbar.cc:            if (pkt->needsResponse())
src/mem/noncoherent_xbar.cc:                pkt->makeResponse();
src/mem/mem_ctrl.cc:    if (!dram->getAddrRange().contains(pkt->getAddr())) {
src/mem/mem_ctrl.cc:        panic("Can't handle address range for packet %s\n", pkt->print());
src/mem/mem_ctrl.cc:                     pkt->cmdString(), pkt->getAddr());
src/mem/mem_ctrl.cc:    panic_if(pkt->cacheResponding(), "Should not see packets where cache "
src/mem/mem_ctrl.cc:    if (pkt->hasData()) {
src/mem/mem_ctrl.cc:    assert(!pkt->isWrite());
src/mem/mem_ctrl.cc:    const Addr base_addr = pkt->getAddr();
src/mem/mem_ctrl.cc:                        base_addr + pkt->getSize()) - addr;
src/mem/mem_ctrl.cc:        stats.requestorReadAccesses[pkt->requestorId()]++;
src/mem/mem_ctrl.cc:                        "memory requests\n", pkt->getAddr(), pkt_count);
src/mem/mem_ctrl.cc:            mem_intr->setupRank(mem_pkt->rank, true);
src/mem/mem_ctrl.cc:            mem_pkt->readyTime = MaxTick;
src/mem/mem_ctrl.cc:            mem_pkt->burstHelper = burst_helper;
src/mem/mem_ctrl.cc:            readQueue[mem_pkt->qosValue()].push_back(mem_pkt);
src/mem/mem_ctrl.cc:            logRequest(MemCtrl::READ, pkt->requestorId(),
src/mem/mem_ctrl.cc:                       pkt->qosValue(), mem_pkt->addr, 1);
src/mem/mem_ctrl.cc:    assert(pkt->isWrite());
src/mem/mem_ctrl.cc:    const Addr base_addr = pkt->getAddr();
src/mem/mem_ctrl.cc:                        base_addr + pkt->getSize()) - addr;
src/mem/mem_ctrl.cc:        stats.requestorWriteAccesses[pkt->requestorId()]++;
src/mem/mem_ctrl.cc:            mem_pkt->readyTime = MaxTick;
src/mem/mem_ctrl.cc:            mem_intr->setupRank(mem_pkt->rank, false);
src/mem/mem_ctrl.cc:            writeQueue[mem_pkt->qosValue()].push_back(mem_pkt);
src/mem/mem_ctrl.cc:            logRequest(MemCtrl::WRITE, pkt->requestorId(),
src/mem/mem_ctrl.cc:                       pkt->qosValue(), mem_pkt->addr, 1);
src/mem/mem_ctrl.cc:            pkt->cmdString(), pkt->getAddr(), pkt->getSize());
src/mem/mem_ctrl.cc:    panic_if(pkt->cacheResponding(), "Should not see packets where cache "
src/mem/mem_ctrl.cc:    panic_if(!(pkt->isRead() || pkt->isWrite()),
src/mem/mem_ctrl.cc:    panic_if(!(dram->getAddrRange().contains(pkt->getAddr())),
src/mem/mem_ctrl.cc:             "Can't handle address range for packet %s\n", pkt->print());
src/mem/mem_ctrl.cc:    unsigned size = pkt->getSize();
src/mem/mem_ctrl.cc:    unsigned offset = pkt->getAddr() & (burst_size - 1);
src/mem/mem_ctrl.cc:    if (pkt->isWrite()) {
src/mem/mem_ctrl.cc:        assert(pkt->isRead());
src/mem/mem_ctrl.cc:    mem_intr->respondEvent(mem_pkt->rank);
src/mem/mem_ctrl.cc:    if (mem_pkt->burstHelper) {
src/mem/mem_ctrl.cc:        mem_pkt->burstHelper->burstsServiced++;
src/mem/mem_ctrl.cc:        if (mem_pkt->burstHelper->burstsServiced ==
src/mem/mem_ctrl.cc:            mem_pkt->burstHelper->burstCount) {
src/mem/mem_ctrl.cc:            accessAndRespond(mem_pkt->pkt, frontendLatency + backendLatency,
src/mem/mem_ctrl.cc:            delete mem_pkt->burstHelper;
src/mem/mem_ctrl.cc:            mem_pkt->burstHelper = NULL;
src/mem/mem_ctrl.cc:        accessAndRespond(mem_pkt->pkt, frontendLatency + backendLatency,
src/mem/mem_ctrl.cc:            mem_intr->checkRefreshState(mem_pkt->rank);
src/mem/mem_ctrl.cc:            if (mem_pkt->pseudoChannel != mem_intr->pseudoChannel) {
src/mem/mem_ctrl.cc:                if (mem_pkt->pseudoChannel != mem_intr->pseudoChannel) {
src/mem/mem_ctrl.cc:    DPRINTF(MemCtrl, "Responding to Address %#x.. \n", pkt->getAddr());
src/mem/mem_ctrl.cc:    bool needsResponse = pkt->needsResponse();
src/mem/mem_ctrl.cc:    panic_if(!mem_intr->getAddrRange().contains(pkt->getAddr()),
src/mem/mem_ctrl.cc:             "Can't handle address range for packet %s\n", pkt->print());
src/mem/mem_ctrl.cc:        assert(pkt->isResponse());
src/mem/mem_ctrl.cc:        Tick response_time = curTick() + static_latency + pkt->headerDelay +
src/mem/mem_ctrl.cc:                             pkt->payloadDelay;
src/mem/mem_ctrl.cc:        pkt->headerDelay = pkt->payloadDelay = 0;
src/mem/mem_ctrl.cc:    std::vector<MemPacketQueue>& queue = selQueue(mem_pkt->isRead());
src/mem/mem_ctrl.cc:            mem_pkt->addr, mem_pkt->readyTime, mem_intr->nextBurstAt);
src/mem/mem_ctrl.cc:    if (mem_pkt->isRead()) {
src/mem/mem_ctrl.cc:        stats.requestorReadTotalLat[mem_pkt->requestorId()] +=
src/mem/mem_ctrl.cc:            mem_pkt->readyTime - mem_pkt->entryTime;
src/mem/mem_ctrl.cc:        stats.requestorReadBytes[mem_pkt->requestorId()] += mem_pkt->size;
src/mem/mem_ctrl.cc:        stats.requestorWriteBytes[mem_pkt->requestorId()] += mem_pkt->size;
src/mem/mem_ctrl.cc:        stats.requestorWriteTotalLat[mem_pkt->requestorId()] +=
src/mem/mem_ctrl.cc:            mem_pkt->readyTime - mem_pkt->entryTime;
src/mem/mem_ctrl.cc:            "Command for %#x, issued at %lld.\n", mem_pkt->addr, cmd_at);
src/mem/mem_ctrl.cc:            assert(mem_pkt->readyTime >= curTick());
src/mem/mem_ctrl.cc:                        mem_pkt->qosValue(), mem_pkt->getAddr(), 1,
src/mem/mem_ctrl.cc:                        mem_pkt->readyTime - mem_pkt->entryTime);
src/mem/mem_ctrl.cc:                schedule(resp_event, mem_pkt->readyTime);
src/mem/mem_ctrl.cc:                assert(resp_queue.back()->readyTime <= mem_pkt->readyTime);
src/mem/mem_ctrl.cc:            readQueue[mem_pkt->qosValue()].erase(to_read);
src/mem/mem_ctrl.cc:        "Command for %#x, issued at %lld.\n", mem_pkt->addr, cmd_at);
src/mem/mem_ctrl.cc:        isInWriteQueue.erase(burstAlign(mem_pkt->addr, mem_intr));
src/mem/mem_ctrl.cc:        logResponse(MemCtrl::WRITE, mem_pkt->requestorId(),
src/mem/mem_ctrl.cc:                    mem_pkt->qosValue(), mem_pkt->getAddr(), 1,
src/mem/mem_ctrl.cc:                    mem_pkt->readyTime - mem_pkt->entryTime);
src/mem/mem_ctrl.cc:        writeQueue[mem_pkt->qosValue()].erase(to_write);
src/mem/mem_ctrl.cc:    return (mem_pkt->size <= mem_intr->bytesPerBurst());
src/mem/mem_ctrl.cc:             pkt->print());
src/mem/mem_ctrl.cc:    if (mem_intr->getAddrRange().contains(pkt->getAddr())) {
src/mem/mem_ctrl.cc:    pkt->pushLabel(ctrl.name());
src/mem/mem_ctrl.cc:        pkt->makeResponse();
src/mem/mem_ctrl.cc:    pkt->popLabel();
src/mem/serial_link.cc:            pkt->cmdString(), pkt->getAddr());
src/mem/serial_link.cc:    pkt->headerDelay = pkt->payloadDelay = 0;
src/mem/serial_link.cc:    cycles += Cycles(divCeil(pkt->getSize() * 8, serial_link.num_lanes
src/mem/serial_link.cc:            pkt->cmdString(), pkt->getAddr());
src/mem/serial_link.cc:        bool expects_response = pkt->needsResponse() &&
src/mem/serial_link.cc:            !pkt->cacheResponding();
src/mem/serial_link.cc:            pkt->headerDelay = pkt->payloadDelay = 0;
src/mem/serial_link.cc:            cycles += Cycles(divCeil(pkt->getSize() * 8,
src/mem/serial_link.cc:            pkt->getAddr(), transmitList.size());
src/mem/serial_link.cc:            Cycles cycles = Cycles(divCeil(pkt->getSize() * 8,
src/mem/serial_link.cc:            pkt->getAddr(), outstandingResponses);
src/mem/serial_link.cc:            Cycles cycles = Cycles(divCeil(pkt->getSize() * 8,
src/mem/serial_link.cc:    pkt->pushLabel(name());
src/mem/serial_link.cc:        if (pkt->trySatisfyFunctional((*i).pkt)) {
src/mem/serial_link.cc:            pkt->makeResponse();
src/mem/serial_link.cc:    pkt->popLabel();
src/mem/serial_link.cc:        if (pkt->trySatisfyFunctional((*i).pkt)) {
src/mem/serial_link.cc:            pkt->makeResponse();
src/mem/snoop_filter.cc:            cpu_side_port.name(), cpkt->print());
src/mem/snoop_filter.cc:    bool allocate = !cpkt->req->isUncacheable() && cpu_side_port.isSnooping()
src/mem/snoop_filter.cc:        && cpkt->fromCache();
src/mem/snoop_filter.cc:    Addr line_addr = cpkt->getBlockAddr(linesize);
src/mem/snoop_filter.cc:    if (cpkt->isSecure()) {
src/mem/snoop_filter.cc:    if (cpkt->needsResponse()) {
src/mem/snoop_filter.cc:        if (!cpkt->cacheResponding()) {
src/mem/snoop_filter.cc:    } else { // if (!cpkt->needsResponse())
src/mem/snoop_filter.cc:        assert(cpkt->isEviction());
src/mem/snoop_filter.cc:        if (!cpkt->isBlockCached()) {
src/mem/snoop_filter.cc:    DPRINTF(SnoopFilter, "%s: packet %s\n", __func__, cpkt->print());
src/mem/snoop_filter.cc:    assert(cpkt->isRequest());
src/mem/snoop_filter.cc:    Addr line_addr = cpkt->getBlockAddr(linesize);
src/mem/snoop_filter.cc:    if (cpkt->isSecure()) {
src/mem/snoop_filter.cc:    assert(cpkt->isWriteback() || cpkt->req->isUncacheable() ||
src/mem/snoop_filter.cc:           (cpkt->isInvalidate() == cpkt->needsWritable()) ||
src/mem/snoop_filter.cc:           cpkt->req->isCacheMaintenance());
src/mem/snoop_filter.cc:    if (cpkt->isInvalidate() && sf_item.requested.none()) {
src/mem/snoop_filter.cc:            __func__, rsp_port.name(), req_port.name(), cpkt->print());
src/mem/snoop_filter.cc:    assert(cpkt->isResponse());
src/mem/snoop_filter.cc:    assert(cpkt->cacheResponding());
src/mem/snoop_filter.cc:    if (cpkt->req->isUncacheable() || !req_port.isSnooping()) {
src/mem/snoop_filter.cc:    Addr line_addr = cpkt->getBlockAddr(linesize);
src/mem/snoop_filter.cc:    if (cpkt->isSecure()) {
src/mem/snoop_filter.cc:    if (!cpkt->hasSharers()) {
src/mem/snoop_filter.cc:    assert(!cpkt->isWriteback());
src/mem/snoop_filter.cc:            __func__, rsp_port.name(), req_port.name(), cpkt->print());
src/mem/snoop_filter.cc:    assert(cpkt->isResponse());
src/mem/snoop_filter.cc:    assert(cpkt->cacheResponding());
src/mem/snoop_filter.cc:    Addr line_addr = cpkt->getBlockAddr(linesize);
src/mem/snoop_filter.cc:    if (cpkt->isSecure()) {
src/mem/snoop_filter.cc:    if (!cpkt->hasSharers()) {
src/mem/snoop_filter.cc:            __func__, cpu_side_port.name(), cpkt->print());
src/mem/snoop_filter.cc:    assert(cpkt->isResponse());
src/mem/snoop_filter.cc:    if (cpkt->req->isUncacheable() || !cpu_side_port.isSnooping())
src/mem/snoop_filter.cc:    Addr line_addr = cpkt->getBlockAddr(linesize);
src/mem/snoop_filter.cc:    if (cpkt->isSecure()) {
src/mem/snoop_filter.cc:    if (cpkt->req->isCacheMaintenance()) {
src/mem/snoop_filter.cc:        if (cpkt->isInvalidate()) {
src/mem/abstract_mem.cc:    const RequestPtr &req = pkt->req;
src/mem/abstract_mem.cc:    const RequestPtr &req = pkt->req;
src/mem/abstract_mem.cc:    bool isLLSC = pkt->isLLSC();
src/mem/abstract_mem.cc:    int size = pkt->getSize();
src/mem/abstract_mem.cc:                "%#x %c\n", label, sys->getRequestorName(pkt->req->
src/mem/abstract_mem.cc:                requestorId()), size, pkt->getAddr(),
src/mem/abstract_mem.cc:                pkt->getUintX(byte_order),
src/mem/abstract_mem.cc:                pkt->req->isUncacheable() ? 'U' : 'C');
src/mem/abstract_mem.cc:            label, sys->getRequestorName(pkt->req->requestorId()),
src/mem/abstract_mem.cc:            size, pkt->getAddr(), pkt->req->isUncacheable() ? 'U' : 'C');
src/mem/abstract_mem.cc:    DDUMP(MemoryAccess, pkt->getConstPtr<uint8_t>(), pkt->getSize());
src/mem/abstract_mem.cc:    if (pkt->cacheResponding()) {
src/mem/abstract_mem.cc:                pkt->getAddr());
src/mem/abstract_mem.cc:    if (pkt->cmd == MemCmd::CleanEvict || pkt->cmd == MemCmd::WritebackClean) {
src/mem/abstract_mem.cc:                pkt->getAddr());
src/mem/abstract_mem.cc:    assert(pkt->getAddrRange().isSubset(range));
src/mem/abstract_mem.cc:    uint8_t *host_addr = toHostAddr(pkt->getAddr());
src/mem/abstract_mem.cc:    if (pkt->cmd == MemCmd::SwapReq) {
src/mem/abstract_mem.cc:        if (pkt->isAtomicOp()) {
src/mem/abstract_mem.cc:                pkt->setData(host_addr);
src/mem/abstract_mem.cc:                (*(pkt->getAtomicOp()))(host_addr);
src/mem/abstract_mem.cc:            std::vector<uint8_t> overwrite_val(pkt->getSize());
src/mem/abstract_mem.cc:            pkt->writeData(&overwrite_val[0]);
src/mem/abstract_mem.cc:            pkt->setData(host_addr);
src/mem/abstract_mem.cc:            if (pkt->req->isCondSwap()) {
src/mem/abstract_mem.cc:                if (pkt->getSize() == sizeof(uint64_t)) {
src/mem/abstract_mem.cc:                    condition_val64 = pkt->req->getExtraData();
src/mem/abstract_mem.cc:                } else if (pkt->getSize() == sizeof(uint32_t)) {
src/mem/abstract_mem.cc:                    condition_val32 = (uint32_t)pkt->req->getExtraData();
src/mem/abstract_mem.cc:                std::memcpy(host_addr, &overwrite_val[0], pkt->getSize());
src/mem/abstract_mem.cc:            assert(!pkt->req->isInstFetch());
src/mem/abstract_mem.cc:            stats.numOther[pkt->req->requestorId()]++;
src/mem/abstract_mem.cc:    } else if (pkt->isRead()) {
src/mem/abstract_mem.cc:        assert(!pkt->isWrite());
src/mem/abstract_mem.cc:        if (pkt->isLLSC()) {
src/mem/abstract_mem.cc:            assert(!pkt->fromCache());
src/mem/abstract_mem.cc:            pkt->setData(host_addr);
src/mem/abstract_mem.cc:        TRACE_PACKET(pkt->req->isInstFetch() ? "IFetch" : "Read");
src/mem/abstract_mem.cc:        stats.numReads[pkt->req->requestorId()]++;
src/mem/abstract_mem.cc:        stats.bytesRead[pkt->req->requestorId()] += pkt->getSize();
src/mem/abstract_mem.cc:        if (pkt->req->isInstFetch())
src/mem/abstract_mem.cc:            stats.bytesInstRead[pkt->req->requestorId()] += pkt->getSize();
src/mem/abstract_mem.cc:    } else if (pkt->isInvalidate() || pkt->isClean()) {
src/mem/abstract_mem.cc:        assert(!pkt->isWrite());
src/mem/abstract_mem.cc:    } else if (pkt->isWrite()) {
src/mem/abstract_mem.cc:                pkt->writeData(host_addr);
src/mem/abstract_mem.cc:                        __func__, pkt->print());
src/mem/abstract_mem.cc:            assert(!pkt->req->isInstFetch());
src/mem/abstract_mem.cc:            stats.numWrites[pkt->req->requestorId()]++;
src/mem/abstract_mem.cc:            stats.bytesWritten[pkt->req->requestorId()] += pkt->getSize();
src/mem/abstract_mem.cc:        panic("Unexpected packet %s", pkt->print());
src/mem/abstract_mem.cc:    if (pkt->needsResponse()) {
src/mem/abstract_mem.cc:        pkt->makeResponse();
src/mem/abstract_mem.cc:    assert(pkt->getAddrRange().isSubset(range));
src/mem/abstract_mem.cc:    uint8_t *host_addr = toHostAddr(pkt->getAddr());
src/mem/abstract_mem.cc:    if (pkt->isRead()) {
src/mem/abstract_mem.cc:            pkt->setData(host_addr);
src/mem/abstract_mem.cc:        pkt->makeResponse();
src/mem/abstract_mem.cc:    } else if (pkt->isWrite()) {
src/mem/abstract_mem.cc:            pkt->writeData(host_addr);
src/mem/abstract_mem.cc:        pkt->makeResponse();
src/mem/abstract_mem.cc:    } else if (pkt->isPrint()) {
src/mem/abstract_mem.cc:            dynamic_cast<Packet::PrintReqState*>(pkt->senderState);
src/mem/abstract_mem.cc:              pkt->cmdString());
src/mem/mem_delay.cc:    const Tick receive_delay = pkt->headerDelay + pkt->payloadDelay;
src/mem/mem_delay.cc:    pkt->headerDelay = pkt->payloadDelay = 0;
src/mem/mem_delay.cc:        pkt->makeResponse();
src/mem/mem_delay.cc:    Tick receive_delay = pkt->headerDelay + pkt->payloadDelay;
src/mem/mem_delay.cc:    pkt->headerDelay = pkt->payloadDelay = 0;
src/mem/mem_delay.cc:        pkt->makeResponse();
src/mem/mem_delay.cc:    if (pkt->isRead()) {
src/mem/mem_delay.cc:    } else if (pkt->isWrite()) {
src/mem/mem_delay.cc:    if (pkt->isRead()) {
src/mem/mem_delay.cc:    } else if (pkt->isWrite()) {
src/mem/xbar.cc:    pkt->headerDelay += offset + header_delay;
src/mem/xbar.cc:    panic_if(pkt->headerDelay > sim_clock::as_int::us,
src/mem/xbar.cc:    if (pkt->hasData()) {
src/mem/xbar.cc:        pkt->payloadDelay = std::max<Tick>(pkt->payloadDelay,
src/mem/xbar.cc:                                           divCeil(pkt->getSize(), width) *
src/mem/xbar.cc:            pkt->getExtension<TracingExtension>();
src/mem/packet.cc:    assert(!pkt->cacheResponding() || !cacheResponding());
src/mem/packet.cc:    flags.set(pkt->flags & RESPONDER_FLAGS);
src/mem/packet.cc:    return matchBlockAddr(pkt->getBlockAddr(blk_size), pkt->isSecure(),
src/mem/packet.cc:    return matchAddr(pkt->getAddr(), pkt->isSecure());
src/mem/bridge.cc:            pkt->cmdString(), pkt->getAddr());
src/mem/bridge.cc:    Tick receive_delay = pkt->headerDelay + pkt->payloadDelay;
src/mem/bridge.cc:    pkt->headerDelay = pkt->payloadDelay = 0;
src/mem/bridge.cc:            pkt->cmdString(), pkt->getAddr());
src/mem/bridge.cc:    panic_if(pkt->cacheResponding(), "Should not see packets where cache "
src/mem/bridge.cc:        bool expects_response = pkt->needsResponse();
src/mem/bridge.cc:            Tick receive_delay = pkt->headerDelay + pkt->payloadDelay;
src/mem/bridge.cc:            pkt->headerDelay = pkt->payloadDelay = 0;
src/mem/bridge.cc:            pkt->getAddr(), transmitList.size());
src/mem/bridge.cc:            pkt->getAddr(), outstandingResponses);
src/mem/bridge.cc:    panic_if(pkt->cacheResponding(), "Should not see packets where cache "
src/mem/bridge.cc:    pkt->pushLabel(name());
src/mem/bridge.cc:        if (pkt->trySatisfyFunctional((*i).pkt)) {
src/mem/bridge.cc:            pkt->makeResponse();
src/mem/bridge.cc:    pkt->popLabel();
src/mem/bridge.cc:        if (pkt->trySatisfyFunctional((*i).pkt)) {
src/mem/bridge.cc:            pkt->makeResponse();
src/mem/nvm_interface.cc:        if (!pkt->isDram()) {
src/mem/nvm_interface.cc:            const Bank& bank = ranks[pkt->rank]->banks[pkt->bank];
src/mem/nvm_interface.cc:            const Tick col_allowed_at = pkt->isRead() ? bank.rdAllowedAt :
src/mem/nvm_interface.cc:                        pkt->bank, pkt->rank);
src/mem/nvm_interface.cc:                        pkt->bank, pkt->rank);
src/mem/nvm_interface.cc:        if (pkt->readyTime == MaxTick && !pkt->isDram() && pkt->isRead()) {
src/mem/nvm_interface.cc:           Bank& bank_ref = ranks[pkt->rank]->banks[pkt->bank];
src/mem/nvm_interface.cc:            if (bank_ref.openRow != pkt->row) {
src/mem/nvm_interface.cc:                bank_ref.openRow = pkt->row;
src/mem/nvm_interface.cc:            pkt->readyTime = std::max(cmd_at, bank_ref.actAllowedAt);
src/mem/nvm_interface.cc:                         bank_ref.bank, cmd_at, pkt->readyTime);
src/mem/nvm_interface.cc:                schedule(readReadyEvent, pkt->readyTime);
src/mem/nvm_interface.cc:            } else if (readReadyEvent.when() > pkt->readyTime) {
src/mem/nvm_interface.cc:                reschedule(readReadyEvent, pkt->readyTime);
src/mem/nvm_interface.cc:            readReadyQueue.push_back(pkt->readyTime);
src/mem/nvm_interface.cc:    bool read_rdy =  pkt->isRead() && (ctrl->inReadBusState(true, this)) &&
src/mem/nvm_interface.cc:                (pkt->readyTime <= curTick()) && (numReadDataReady > 0);
src/mem/nvm_interface.cc:    bool write_rdy =  !pkt->isRead() && !ctrl->inReadBusState(true, this) &&
src/mem/nvm_interface.cc:            pkt->addr, pkt->rank, pkt->bank, pkt->row);
src/mem/nvm_interface.cc:    Bank& bank_ref = ranks[pkt->rank]->banks[pkt->bank];
src/mem/nvm_interface.cc:    const Tick bst_allowed_at = pkt->isRead() ?
src/mem/nvm_interface.cc:    if (pkt->isRead() || !twoCycleRdWr) {
src/mem/nvm_interface.cc:    pkt->readyTime = cmd_at + tSEND + tBURST;
src/mem/nvm_interface.cc:            dly_to_rd_cmd = pkt->isRead() ? tBURST : writeToReadDelay();
src/mem/nvm_interface.cc:            dly_to_wr_cmd = pkt->isRead() ? readToWriteDelay() : tBURST;
src/mem/nvm_interface.cc:            if (pkt->rank != n->rank) {
src/mem/nvm_interface.cc:            pkt->addr, pkt->readyTime);
src/mem/nvm_interface.cc:    if (pkt->isRead()) {
src/mem/nvm_interface.cc:        if ((bank_ref.bank == pkt->bank) &&
src/mem/nvm_interface.cc:            (bank_ref.openRow != pkt->row)) {
src/mem/nvm_interface.cc:           bank_ref.openRow = pkt->row;
src/mem/nvm_interface.cc:        bank_ref.actAllowedAt = std::max(pkt->readyTime,
src/mem/nvm_interface.cc:    if (pkt->isRead()) {
src/mem/nvm_interface.cc:        stats.perBankRdBursts[pkt->bankId]++;
src/mem/nvm_interface.cc:        stats.totMemAccLat += pkt->readyTime - pkt->entryTime;
src/mem/nvm_interface.cc:        stats.totQLat += cmd_at - pkt->entryTime;
src/mem/nvm_interface.cc:        stats.perBankWrBursts[pkt->bankId]++;
src/mem/physical.cc:    assert(pkt->isRequest());
src/mem/physical.cc:    const auto& m = addrMap.contains(pkt->getAddrRange());
src/mem/physical.cc:    assert(pkt->isRequest());
src/mem/physical.cc:    const auto& m = addrMap.contains(pkt->getAddrRange());
src/cpu/trace/trace_cpu.cc:    pkt->dataDynamic(pkt_data);
src/cpu/trace/trace_cpu.cc:    if (pkt->isWrite()) {
src/cpu/trace/trace_cpu.cc:        auto graph_itr = depGraph.find(pkt->req->getReqInstSeqNum());
src/cpu/trace/trace_cpu.cc:    pkt->dataDynamic(pkt_data);
src/cpu/base.cc:        assert(pkt->req->hasPaddr());
src/cpu/base.cc:        monitor.pAddr = pkt->getAddr() & mask;
src/cpu/base.cc:    assert(pkt->req->hasPaddr());
src/cpu/base.cc:        if (pAddr == pkt->getAddr()) {
src/cpu/base.cc:                    pkt->getAddr());
src/cpu/minor/lsq.cc:    if (pkt->isInvalidate() || pkt->isWrite()) {
src/cpu/minor/lsq.cc:            if (pkt->isInvalidate() || pkt->isWrite()) {
src/cpu/testers/traffic_gen/base_gen.cc:    pkt->dataDynamic(pkt_data);
src/cpu/testers/traffic_gen/base.cc:            pkt->req->setStreamId(sid);
src/cpu/testers/traffic_gen/base.cc:                pkt->req->setSubstreamId(ssid);
src/cpu/testers/traffic_gen/base.cc:        if (pkt && system->isMemAddr(pkt->getAddr())) {
src/cpu/testers/traffic_gen/base.cc:                    pkt->cmdString(), pkt->getAddr());
src/cpu/testers/traffic_gen/base.cc:    auto iter = waitingResp.find(pkt->req);
src/cpu/testers/traffic_gen/base.cc:               pkt->print(), pkt->req);
src/cpu/testers/traffic_gen/base.cc:    if (pkt->isWrite()) {
src/cpu/testers/traffic_gen/base.cc:        stats.bytesWritten += pkt->req->getSize();
src/cpu/testers/traffic_gen/base.cc:        stats.bytesRead += pkt->req->getSize();
src/cpu/testers/traffic_gen/gups_gen.cc:    pkt->allocate();
src/cpu/testers/traffic_gen/gups_gen.cc:    pkt->allocate();
src/cpu/testers/traffic_gen/gups_gen.cc:    pkt->setData(data);
src/cpu/testers/traffic_gen/gups_gen.cc:    if (pkt->isWrite()) {
src/cpu/testers/traffic_gen/gups_gen.cc:        DPRINTF(GUPSGen, "%s: received a write resp. pkt->addr_range: %s,"
src/cpu/testers/traffic_gen/gups_gen.cc:                        " pkt->data: %d\n", __func__,
src/cpu/testers/traffic_gen/gups_gen.cc:                        pkt->getAddrRange().to_string(),
src/cpu/testers/traffic_gen/gups_gen.cc:                        *pkt->getPtr<uint64_t>());
src/cpu/testers/traffic_gen/gups_gen.cc:        stats.totalWriteLat += curTick() - exitTimes[pkt->req];
src/cpu/testers/traffic_gen/gups_gen.cc:        exitTimes.erase(pkt->req);
src/cpu/testers/traffic_gen/gups_gen.cc:        DPRINTF(GUPSGen, "%s: received a read resp. pkt->addr_range: %s\n",
src/cpu/testers/traffic_gen/gups_gen.cc:                        __func__, pkt->getAddrRange().to_string());
src/cpu/testers/traffic_gen/gups_gen.cc:        stats.totalReadLat += curTick() - exitTimes[pkt->req];
src/cpu/testers/traffic_gen/gups_gen.cc:        exitTimes.erase(pkt->req);
src/cpu/testers/traffic_gen/gups_gen.cc:        uint64_t *updated_value = pkt->getPtr<uint64_t>();
src/cpu/testers/traffic_gen/gups_gen.cc:                            *updated_value, pkt->getAddrRange().to_string());
src/cpu/testers/traffic_gen/gups_gen.cc:        *updated_value ^= updateTable[pkt->req];
src/cpu/testers/traffic_gen/gups_gen.cc:        updateTable.erase(pkt->req);
src/cpu/testers/traffic_gen/gups_gen.cc:        Addr addr = pkt->getAddr();
src/cpu/testers/traffic_gen/gups_gen.cc:        updateTable[pkt->req] = value;
src/cpu/testers/traffic_gen/gups_gen.cc:        exitTimes[pkt->req] = curTick();
src/cpu/testers/traffic_gen/gups_gen.cc:        if (pkt->isWrite()) {
src/cpu/testers/traffic_gen/gups_gen.cc:            DPRINTF(GUPSGen, "%s: Sent write pkt, pkt->addr_range: "
src/cpu/testers/traffic_gen/gups_gen.cc:                    "%s, pkt->data: %lu.\n", __func__,
src/cpu/testers/traffic_gen/gups_gen.cc:                    pkt->getAddrRange().to_string(),
src/cpu/testers/traffic_gen/gups_gen.cc:                    *pkt->getPtr<uint64_t>());
src/cpu/testers/traffic_gen/gups_gen.cc:            DPRINTF(GUPSGen, "%s: Sent read pkt, pkt->addr_range: %s.\n",
src/cpu/testers/traffic_gen/gups_gen.cc:                    __func__, pkt->getAddrRange().to_string());
src/cpu/testers/traffic_gen/base.hh:        assert(waitingResp.find(pkt->req) == waitingResp.end());
src/cpu/testers/traffic_gen/base.hh:        assert(pkt->needsResponse());
src/cpu/testers/traffic_gen/base.hh:        waitingResp[pkt->req] = curTick();
src/cpu/testers/gpu_ruby_test/dma_thread.cc:        pkt->dataDynamic(data);
src/cpu/testers/gpu_ruby_test/dma_thread.cc:        pkt->senderState = new ProtocolTester::SenderState(this);
src/cpu/testers/gpu_ruby_test/dma_thread.cc:        pkt->dataDynamic(writeData);
src/cpu/testers/gpu_ruby_test/dma_thread.cc:        pkt->senderState = new ProtocolTester::SenderState(this);
src/cpu/testers/gpu_ruby_test/dma_thread.cc:    MemCmd resp_cmd = pkt->cmd;
src/cpu/testers/gpu_ruby_test/dma_thread.cc:    Addr addr = pkt->getAddr();
src/cpu/testers/gpu_ruby_test/dma_thread.cc:        Value value = *(pkt->getPtr<Value>());
src/cpu/testers/gpu_ruby_test/dma_thread.cc:        Value value = *(pkt->getPtr<Value>());
src/cpu/testers/gpu_ruby_test/dma_thread.cc:    delete pkt->senderState;
src/cpu/testers/gpu_ruby_test/protocol_tester.cc:                    safe_cast<ProtocolTester::SenderState*>(pkt->senderState);
src/cpu/testers/gpu_ruby_test/gpu_wavefront.cc:            pkt->dataDynamic(data);
src/cpu/testers/gpu_ruby_test/gpu_wavefront.cc:            pkt->senderState = new ProtocolTester::SenderState(this);
src/cpu/testers/gpu_ruby_test/gpu_wavefront.cc:            pkt->dataDynamic(writeData);
src/cpu/testers/gpu_ruby_test/gpu_wavefront.cc:            pkt->senderState = new ProtocolTester::SenderState(this);
src/cpu/testers/gpu_ruby_test/gpu_wavefront.cc:        pkt->dataDynamic(data);
src/cpu/testers/gpu_ruby_test/gpu_wavefront.cc:        pkt->senderState = new ProtocolTester::SenderState(this);
src/cpu/testers/gpu_ruby_test/gpu_wavefront.cc:    pkt->senderState = new ProtocolTester::SenderState(this);
src/cpu/testers/gpu_ruby_test/gpu_wavefront.cc:    MemCmd resp_cmd = pkt->cmd;
src/cpu/testers/gpu_ruby_test/gpu_wavefront.cc:    Addr addr = (resp_cmd == MemCmd::WriteCompleteResp) ? 0 : pkt->getAddr();
src/cpu/testers/gpu_ruby_test/gpu_wavefront.cc:        Value value = *(pkt->getPtr<Value>());
src/cpu/testers/gpu_ruby_test/gpu_wavefront.cc:        Value value = *(pkt->getPtr<Value>());
src/cpu/testers/gpu_ruby_test/gpu_wavefront.cc:        delete pkt->senderState;
src/cpu/testers/memtest/memtest.cc:    const RequestPtr &req = pkt->req;
src/cpu/testers/memtest/memtest.cc:            pkt->isWrite() ? pkt->isAtomicOp() ? "atomic" : "write" : "read",
src/cpu/testers/memtest/memtest.cc:            pkt->isError() ? "error" : "success");
src/cpu/testers/memtest/memtest.cc:    const uint8_t *pkt_data = pkt->getConstPtr<uint8_t>();
src/cpu/testers/memtest/memtest.cc:    if (pkt->isError()) {
src/cpu/testers/memtest/memtest.cc:        if (!functional || !pkt->suppressFuncError() || !suppressFuncErrors)
src/cpu/testers/memtest/memtest.cc:                pkt->isWrite() ? "Write" : "Read", req->getPaddr());
src/cpu/testers/memtest/memtest.cc:        if (pkt->isAtomicOp()) {
src/cpu/testers/memtest/memtest.cc:        } else if (pkt->isRead()) {
src/cpu/testers/memtest/memtest.cc:            assert(pkt->isWrite());
src/cpu/testers/memtest/memtest.cc:        pkt->dataDynamic(pkt_data);
src/cpu/testers/memtest/memtest.cc:            pkt->dataDynamic(pkt_data);
src/cpu/testers/memtest/memtest.cc:            pkt->dataDynamic(pkt_data);
src/cpu/testers/memtest/memtest.cc:        pkt->setSuppressFuncError();
src/cpu/testers/garnet_synthetic_traffic/GarnetSyntheticTraffic.cc:            pkt->isWrite() ? "write" : "read\n",
src/cpu/testers/garnet_synthetic_traffic/GarnetSyntheticTraffic.cc:            pkt->req->getPaddr(),pkt->getPtr<uint8_t>()[0]);
src/cpu/testers/garnet_synthetic_traffic/GarnetSyntheticTraffic.cc:    assert(pkt->isResponse());
src/cpu/testers/garnet_synthetic_traffic/GarnetSyntheticTraffic.cc:    pkt->dataDynamic(buffer_data);
src/cpu/testers/garnet_synthetic_traffic/GarnetSyntheticTraffic.cc:    pkt->senderState = NULL;
src/cpu/testers/directedtest/RubyDirectedTester.cc:    tester->hitCallback(id, pkt->getAddr());
src/cpu/testers/directedtest/InvalidateGenerator.cc:    pkt->allocate();
src/cpu/testers/directedtest/SeriesRequestGenerator.cc:    pkt->allocate();
src/cpu/testers/rubytest/RubyTester.cc:        safe_cast<RubyTester::SenderState*>(pkt->senderState);
src/cpu/testers/rubytest/RubyTester.cc:    delete pkt->senderState;
src/cpu/testers/rubytest/Check.cc:    pkt->dataDynamic(data);
src/cpu/testers/rubytest/Check.cc:    pkt->senderState = new SenderState(m_address, req->getSize());
src/cpu/testers/rubytest/Check.cc:        delete pkt->senderState;
src/cpu/testers/rubytest/Check.cc:    pkt->senderState = new SenderState(m_address, req->getSize());
src/cpu/testers/rubytest/Check.cc:    pkt->dataDynamic(writeData);
src/cpu/testers/rubytest/Check.cc:            *(pkt->getConstPtr<uint8_t>()), *writeData);
src/cpu/testers/rubytest/Check.cc:    pkt->senderState = new SenderState(writeAddr, req->getSize());
src/cpu/testers/rubytest/Check.cc:        delete pkt->senderState;
src/cpu/testers/rubytest/Check.cc:    pkt->dataDynamic(dataArray);
src/cpu/testers/rubytest/Check.cc:    pkt->senderState = new SenderState(m_address, req->getSize());
src/cpu/testers/rubytest/Check.cc:        delete pkt->senderState;
src/cpu/simple/atomic.cc:            __func__, pkt->getAddr(), pkt->cmdString());
src/cpu/simple/atomic.cc:            __func__, pkt->getAddr(), pkt->cmdString());
src/cpu/simple/atomic.cc:    if (pkt->isInvalidate() || pkt->isWrite()) {
src/cpu/simple/atomic.cc:                pkt->getAddr());
src/cpu/simple/atomic.cc:            __func__, pkt->getAddr(), pkt->cmdString());
src/cpu/simple/atomic.cc:    if (pkt->isInvalidate()) {
src/cpu/simple/atomic.cc:                pkt->getAddr());
src/cpu/simple/timing.cc:    const RequestPtr &req = pkt->req;
src/cpu/simple/timing.cc:    if (pkt->isRead() && pkt->req->isLLSC()) {
src/cpu/simple/timing.cc:        thread->getIsaPtr()->handleLockedRead(pkt->req);
src/cpu/simple/timing.cc:    pkt->dataDynamic<uint8_t>(data);
src/cpu/simple/timing.cc:        pkt->setHtmTransactional(t_info.getHtmTransactionUid());
src/cpu/simple/timing.cc:        pkt->makeResponse();
src/cpu/simple/timing.cc:    pkt->dataDynamic<uint8_t>(data);
src/cpu/simple/timing.cc:    pkt->senderState = main_send_state;
src/cpu/simple/timing.cc:    const RequestPtr &req = dcache_pkt->req;
src/cpu/simple/timing.cc:        ifetch_pkt->dataStatic(decoder->moreBytesPtr());
src/cpu/simple/timing.cc:        DPRINTF(SimpleCPU, " -- pkt addr: %#x\n", ifetch_pkt->getAddr());
src/cpu/simple/timing.cc:            pkt->getAddr() : 0);
src/cpu/simple/timing.cc:    panic_if(pkt && pkt->isError(), "Instruction fetch (%s) failed: %s",
src/cpu/simple/timing.cc:            pkt->getAddrRange().to_string(), pkt->print());
src/cpu/simple/timing.cc:        pkt->req->setAccessLatency();
src/cpu/simple/timing.cc:    DPRINTF(SimpleCPU, "Received fetch response %#x\n", pkt->getAddr());
src/cpu/simple/timing.cc:    if (pkt->htmTransactionFailedInCache()) {
src/cpu/simple/timing.cc:    panic_if(pkt->isError(), "Data access (%s) failed: %s",
src/cpu/simple/timing.cc:            pkt->getAddrRange().to_string(), pkt->print());
src/cpu/simple/timing.cc:           pkt->req->getFlags().isSet(Request::NO_ACCESS));
src/cpu/simple/timing.cc:    pkt->req->setAccessLatency();
src/cpu/simple/timing.cc:    if (pkt->senderState) {
src/cpu/simple/timing.cc:        if (pkt->req->isHTMCmd()) {
src/cpu/simple/timing.cc:            dynamic_cast<SplitFragmentSenderState *>(pkt->senderState);
src/cpu/simple/timing.cc:        if (pkt->isHtmTransactional()) {
src/cpu/simple/timing.cc:            big_pkt->setHtmTransactional(
src/cpu/simple/timing.cc:                pkt->getHtmTransactionUid()
src/cpu/simple/timing.cc:        if (pkt->htmTransactionFailedInCache()) {
src/cpu/simple/timing.cc:            big_pkt->setHtmTransactionFailedInCache(
src/cpu/simple/timing.cc:                pkt->getHtmTransactionFailedInCacheRC()
src/cpu/simple/timing.cc:            dynamic_cast<SplitMainSenderState *>(big_pkt->senderState);
src/cpu/simple/timing.cc:            big_pkt->senderState = NULL;
src/cpu/simple/timing.cc:    if (pkt->isHtmTransactional())
src/cpu/simple/timing.cc:        assert (pkt->getHtmTransactionUid() ==
src/cpu/simple/timing.cc:    if (pkt->htmTransactionFailedInCache())
src/cpu/simple/timing.cc:    if (pkt->htmTransactionFailedInCache() && !pkt->isWrite()) {
src/cpu/simple/timing.cc:            pkt->getHtmTransactionFailedInCacheRC();
src/cpu/simple/timing.cc:            htmFailureToStr(htm_rc), pkt->getHtmTransactionUid());
src/cpu/simple/timing.cc:    if (pkt->isInvalidate() || pkt->isWrite()) {
src/cpu/simple/timing.cc:    } else if (pkt->req && pkt->req->isTlbiExtSync()) {
src/cpu/simple/timing.cc:        reply_req->setExtraData(pkt->req->getExtraData());
src/cpu/simple/timing.cc:    DPRINTF(SimpleCPU, "Received load/store response %#x\n", pkt->getAddr());
src/cpu/simple/timing.cc:            dynamic_cast<SplitMainSenderState *>(big_pkt->senderState);
src/cpu/simple/timing.cc:                if ((big_pkt->isRead() && cpu->handleReadPacket(tmp)) ||
src/cpu/simple/timing.cc:                        (big_pkt->isWrite() && cpu->handleWritePacket())) {
src/cpu/o3/dyn_inst.cc:            reqToVerify->setExtraData(pkt->req->getExtraData());
src/cpu/o3/fetch.cc:    ThreadID tid = cpu->contextToThread(pkt->req->contextId());
src/cpu/o3/fetch.cc:        pkt->req != memReq[tid]) {
src/cpu/o3/fetch.cc:    memcpy(fetchBuffer[tid], pkt->getConstPtr<uint8_t>(), fetchBufferSize);
src/cpu/o3/fetch.cc:    pkt->req->setAccessLatency();
src/cpu/o3/fetch.cc:        data_pkt->dataDynamic(new uint8_t[fetchBufferSize]);
src/cpu/o3/fetch.cc:    assert(pkt->req->isUncacheable() ||
src/cpu/o3/fetch.cc:           !(pkt->cacheResponding() && !pkt->hasSharers()));
src/cpu/o3/lsq_unit.cc:    LSQRequest *request = dynamic_cast<LSQRequest*>(pkt->senderState);
src/cpu/o3/lsq_unit.cc:    LSQRequest *request = dynamic_cast<LSQRequest *>(pkt->senderState);
src/cpu/o3/lsq_unit.cc:    if (pkt->isHtmTransactional() && !inst->isSquashed()) {
src/cpu/o3/lsq_unit.cc:        assert(inst->getHtmTransactionUid() == pkt->getHtmTransactionUid());
src/cpu/o3/lsq_unit.cc:    if (pkt->htmTransactionFailedInCache()) {
src/cpu/o3/lsq_unit.cc:            pkt->getHtmTransactionFailedInCacheRC();
src/cpu/o3/lsq_unit.cc:        if (pkt->isWrite()) {
src/cpu/o3/lsq_unit.cc:                pkt->getAddr(), htmFailureToStr(htm_rc),
src/cpu/o3/lsq_unit.cc:                pkt->getHtmTransactionUid());
src/cpu/o3/lsq_unit.cc:                inst->pcState(), pkt->getAddr(),
src/cpu/o3/lsq_unit.cc:                htmFailureToStr(htm_rc), pkt->getHtmTransactionUid());
src/cpu/o3/lsq_unit.cc:            if (pkt->htmTransactionFailedInCache()) {
src/cpu/o3/lsq_unit.cc:                    pkt->getHtmTransactionFailedInCacheRC() );
src/cpu/o3/lsq_unit.cc:    assert(pkt->isInvalidate());
src/cpu/o3/lsq_unit.cc:    DPRINTF(LSQUnit, "Got snoop for address %#x\n", pkt->getAddr());
src/cpu/o3/lsq_unit.cc:    Addr invalidate_addr = pkt->getAddr() & cacheBlockMask;
src/cpu/o3/lsq_unit.cc:                        pkt->getAddr(), ld_inst->seqNum);
src/cpu/o3/lsq_unit.cc:                        pkt->getAddr(), ld_inst->seqNum);
src/cpu/o3/lsq_unit.cc:            main_pkt->dataStatic(inst->memData);
src/cpu/o3/lsq_unit.cc:            } else if (!pkt->htmTransactionFailedInCache()) {
src/cpu/o3/lsq_unit.cc:    LSQRequest *request = dynamic_cast<LSQRequest*>(data_pkt->senderState);
src/cpu/o3/lsq_unit.cc:            data_pkt->print(), request->instruction()->seqNum,
src/cpu/o3/lsq_unit.cc:        main_pkt->dataStatic(load_inst->memData);
src/cpu/o3/lsq_unit.cc:                data_pkt->dataStatic(load_inst->memData);
src/cpu/o3/lsq_unit.cc:                    data_pkt->setHtmTransactional(
src/cpu/o3/lsq_unit.cc:                      data_pkt->req->hasVaddr() ?
src/cpu/o3/lsq_unit.cc:                        data_pkt->req->getVaddr() : 0lu,
src/cpu/o3/lsq_unit.cc:                      data_pkt->getAddr(),
src/cpu/o3/lsq.cc:            pkt->isWrite() ? "write" : "read\n",
src/cpu/o3/lsq.cc:            pkt->req->getPaddr(),pkt->getPtr<uint8_t>()[0]);
src/cpu/o3/lsq.cc:    assert(pkt->isResponse());
src/cpu/o3/lsq.cc:    LSQRequest *request = dynamic_cast<LSQRequest*>(pkt->senderState);
src/cpu/o3/lsq.cc:    if (pkt->isError())
src/cpu/o3/lsq.cc:                pkt->getAddr());
src/cpu/o3/lsq.cc:    LSQRequest *request = dynamic_cast<LSQRequest*>(pkt->senderState);
src/cpu/o3/lsq.cc:    if (pkt->isInvalidate()) {
src/cpu/o3/lsq.cc:                pkt->getAddr());
src/cpu/o3/lsq.cc:    DPRINTF(LSQ, "received pkt for addr:%#x %s\n", pkt->getAddr(),
src/cpu/o3/lsq.cc:            pkt->cmdString());
src/cpu/o3/lsq.cc:    if (pkt->isInvalidate()) {
src/cpu/o3/lsq.cc:                pkt->getAddr());
src/cpu/o3/lsq.cc:    } else if (pkt->req && pkt->req->isTlbiExtSync()) {
src/cpu/o3/lsq.cc:        staleTranslationWaitTxnId = pkt->req->getExtraData();
src/cpu/o3/lsq.cc:                        pkt->setHtmTransactional(
src/cpu/o3/lsq.cc:                pkt->dataStatic(_inst->memData + offset);
src/cpu/o3/lsq.cc:                pkt->dataDynamic(req_data);
src/cpu/o3/lsq.cc:            pkt->senderState = this;
src/cpu/o3/lsq.cc:    return pkt->req->localAccessor(thread, pkt);
src/cpu/o3/lsq.cc:        pkt->dataStatic(mainPkt->getPtr<uint8_t>() + offset);
src/cpu/o3/cpu.cc:    abort_pkt->dataStatic(memData);
src/cpu/o3/cpu.cc:    abort_pkt->setHtmTransactional(htm_uid);
src/cpu/kvm/base.cc:    pkt->dataStatic(data);
src/cpu/checker/cpu_impl.hh:                    pkt->dataStatic(decoder->moreBytesPtr());
src/cpu/checker/cpu.cc:            pkt->dataStatic(data);
src/gpu-compute/gpu_command_processor.cc:    assert(!(disp_pkt->kernel_object & (system()->cacheLineSize() - 1)));
src/gpu-compute/gpu_command_processor.cc:        virt_proxy.readBlob(disp_pkt->kernel_object, (uint8_t*)akc,
src/gpu-compute/gpu_command_processor.cc:        Addr phys_addr = disp_pkt->kernel_object;
src/gpu-compute/gpu_command_processor.cc:                " s:%d\n", disp_pkt->kernel_object, phys_addr,
src/gpu-compute/gpu_command_processor.cc:            dmaReadVirt(disp_pkt->kernel_object, sizeof(AMDKernelCode),
src/gpu-compute/gpu_command_processor.cc:            ChunkGenerator gen(disp_pkt->kernel_object, sizeof(AMDKernelCode),
src/gpu-compute/gpu_command_processor.cc:    Addr machine_code_addr = (Addr)disp_pkt->kernel_object
src/gpu-compute/gpu_command_processor.cc:        "signal addr:%#x\n", dynamic_task_id, disp_pkt->workgroup_size_x,
src/gpu-compute/gpu_command_processor.cc:        disp_pkt->workgroup_size_y, disp_pkt->workgroup_size_z,
src/gpu-compute/gpu_command_processor.cc:        disp_pkt->grid_size_x, disp_pkt->grid_size_y,
src/gpu-compute/gpu_command_processor.cc:        disp_pkt->grid_size_z, disp_pkt->kernarg_address,
src/gpu-compute/gpu_command_processor.cc:        disp_pkt->completion_signal);
src/gpu-compute/gpu_command_processor.cc:    dispatchStartTime.insert({disp_pkt->completion_signal, start_ts});
src/gpu-compute/gpu_command_processor.cc:    if (vendor_pkt->completion_signal) {
src/gpu-compute/gpu_command_processor.cc:        sendCompletionSignal(vendor_pkt->completion_signal);
src/gpu-compute/gpu_command_processor.cc:    if (agent_pkt->type == AgentCmd::Nop) {
src/gpu-compute/gpu_command_processor.cc:    } else if (agent_pkt->type == AgentCmd::Steal) {
src/gpu-compute/gpu_command_processor.cc:        int kid = agent_pkt->arg[0];
src/gpu-compute/gpu_command_processor.cc:        uint64_t return_address = agent_pkt->return_address;
src/gpu-compute/fetch_stage.cc:        safe_cast<ComputeUnit::SQCPort::SenderState*>(pkt->senderState);
src/gpu-compute/fetch_stage.cc:    const unsigned num_instructions = pkt->req->getSize() /
src/gpu-compute/shader.cc:        PacketPtr new_pkt = new Packet(pkt->req, cmd);
src/gpu-compute/shader.cc:        new_pkt->dataStatic(data);
src/gpu-compute/shader.cc:            new_pkt->setSuppressFuncError();
src/gpu-compute/shader.cc:    pkt->senderState =
src/gpu-compute/shader.cc:               safe_cast<GpuTranslationState*>(pkt->senderState);
src/gpu-compute/shader.cc:    delete pkt->senderState;
src/gpu-compute/compute_unit.cc:    SenderState *sender_state = safe_cast<SenderState*>(pkt->senderState);
src/gpu-compute/compute_unit.cc:    if (pkt->cmd == MemCmd::MemSyncResp) {
src/gpu-compute/compute_unit.cc:            assert(pkt->req->isKernel());
src/gpu-compute/compute_unit.cc:            assert(pkt->req->isInvL1());
src/gpu-compute/compute_unit.cc:            delete pkt->senderState;
src/gpu-compute/compute_unit.cc:            assert(pkt->req->isKernel());
src/gpu-compute/compute_unit.cc:            assert(pkt->req->isGL2CacheFlush());
src/gpu-compute/compute_unit.cc:                delete pkt->senderState;
src/gpu-compute/compute_unit.cc:        if (!pkt->req->isKernel()) {
src/gpu-compute/compute_unit.cc:        delete pkt->senderState;
src/gpu-compute/compute_unit.cc:            gpuDynInst->seqNum(), index, pkt->req->getPaddr());
src/gpu-compute/compute_unit.cc:    assert(!pkt->req->isKernel());
src/gpu-compute/compute_unit.cc:    SenderState *sender_state = safe_cast<SenderState*>(pkt->senderState);
src/gpu-compute/compute_unit.cc:    assert(pkt->isRead() || pkt->isWrite());
src/gpu-compute/compute_unit.cc:    delete pkt->senderState;
src/gpu-compute/compute_unit.cc:                pkt->req->getPaddr());
src/gpu-compute/compute_unit.cc:                pkt->req->getPaddr());
src/gpu-compute/compute_unit.cc:    Addr tmp_vaddr = pkt->req->getVaddr();
src/gpu-compute/compute_unit.cc:    pkt->req->setPC(gpuDynInst->wavefront()->pc());
src/gpu-compute/compute_unit.cc:    pkt->req->setReqInstSeqNum(gpuDynInst->seqNum());
src/gpu-compute/compute_unit.cc:    assert(pkt->isRead() || pkt->isWrite());
src/gpu-compute/compute_unit.cc:    bool isDataAccess = pkt->isWrite() || pkt->isRead();
src/gpu-compute/compute_unit.cc:        shader->gpuCmdProc.driver()->setMtype(pkt->req);
src/gpu-compute/compute_unit.cc:    if (pkt->isWrite()) {
src/gpu-compute/compute_unit.cc:    } else if (pkt->isRead()) {
src/gpu-compute/compute_unit.cc:            Addr vaddr = pkt->req->getVaddr();
src/gpu-compute/compute_unit.cc:            unsigned size = pkt->getSize();
src/gpu-compute/compute_unit.cc:        pkt->senderState = new DTLBPort::SenderState(gpuDynInst, index);
src/gpu-compute/compute_unit.cc:                                               pkt->senderState);
src/gpu-compute/compute_unit.cc:        pkt->senderState = translation_state;
src/gpu-compute/compute_unit.cc:                safe_cast<GpuTranslationState*>(pkt->senderState);
src/gpu-compute/compute_unit.cc:            assert(pkt->req->hasPaddr());
src/gpu-compute/compute_unit.cc:            assert(pkt->req->hasSize());
src/gpu-compute/compute_unit.cc:                pkt->dataStatic(tmpData);
src/gpu-compute/compute_unit.cc:            pkt->senderState =
src/gpu-compute/compute_unit.cc:            gpuDynInst->memStatusVector[pkt->getAddr()].push_back(index);
src/gpu-compute/compute_unit.cc:                    gpuDynInst->wfSlotId, index, pkt->req->getPaddr());
src/gpu-compute/compute_unit.cc:        if (pkt->cmd == MemCmd::MemSyncReq) {
src/gpu-compute/compute_unit.cc:        delete pkt->senderState;
src/gpu-compute/compute_unit.cc:        pkt->senderState = new GpuTranslationState(TLB_mode,
src/gpu-compute/compute_unit.cc:        PacketPtr new_pkt = new Packet(pkt->req, pkt->cmd);
src/gpu-compute/compute_unit.cc:        new_pkt->dataStatic(pkt->getPtr<uint8_t>());
src/gpu-compute/compute_unit.cc:                new_pkt->req->getPaddr());
src/gpu-compute/compute_unit.cc:             safe_cast<GpuTranslationState*>(pkt->senderState);
src/gpu-compute/compute_unit.cc:        delete pkt->senderState;
src/gpu-compute/compute_unit.cc:    assert(pkt->isWrite() || pkt->isRead());
src/gpu-compute/compute_unit.cc:    BaseMMU::Mode tlb_mode = pkt->isRead() ? BaseMMU::Read : BaseMMU::Write;
src/gpu-compute/compute_unit.cc:    pkt->senderState =
src/gpu-compute/compute_unit.cc:    pkt->senderState =
src/gpu-compute/compute_unit.cc:                                             pkt->senderState);
src/gpu-compute/compute_unit.cc:                pkt->req->getVaddr());
src/gpu-compute/compute_unit.cc:            pkt->pushSenderState(
src/gpu-compute/compute_unit.cc:                    gpuDynInst->wfSlotId, 0, pkt->req->getPaddr());
src/gpu-compute/compute_unit.cc:          pkt->pushSenderState(
src/gpu-compute/compute_unit.cc:                  gpuDynInst->wfSlotId, 0, pkt->req->getPaddr());
src/gpu-compute/compute_unit.cc:        pkt->pushSenderState(
src/gpu-compute/compute_unit.cc:                pkt->req->getPaddr());
src/gpu-compute/compute_unit.cc:        safe_cast<DataPort::SenderState*>(pkt->senderState);
src/gpu-compute/compute_unit.cc:            pkt->req->getPaddr(), id);
src/gpu-compute/compute_unit.cc:    Addr paddr = pkt->req->getPaddr();
src/gpu-compute/compute_unit.cc:    assert(pkt->cmd != MemCmd::MemSyncResp);
src/gpu-compute/compute_unit.cc:    if (pkt->cmd == MemCmd::WriteResp) {
src/gpu-compute/compute_unit.cc:        if (!FullSystem || !pkt->req->systemReq()) {
src/gpu-compute/compute_unit.cc:            pkt->req->getPaddr(), id);
src/gpu-compute/compute_unit.cc:    gpuDynInst->pAddr = pkt->req->getPaddr();
src/gpu-compute/compute_unit.cc:        if (pkt->isRead()) {
src/gpu-compute/compute_unit.cc:    delete pkt->senderState;
src/gpu-compute/compute_unit.cc:    Addr line = pkt->req->getPaddr();
src/gpu-compute/compute_unit.cc:            pkt->req->getVaddr(), line);
src/gpu-compute/compute_unit.cc:    assert(pkt->senderState);
src/gpu-compute/compute_unit.cc:               safe_cast<GpuTranslationState*>(pkt->senderState);
src/gpu-compute/compute_unit.cc:                 pkt->req->getVaddr());
src/gpu-compute/compute_unit.cc:    pkt->senderState = translation_state->saved;
src/gpu-compute/compute_unit.cc:        safe_cast<DTLBPort::SenderState*>(pkt->senderState);
src/gpu-compute/compute_unit.cc:    Addr vaddr = pkt->req->getVaddr();
src/gpu-compute/compute_unit.cc:    if (pkt->cmd == MemCmd::ReadResp) {
src/gpu-compute/compute_unit.cc:    } else if (pkt->cmd == MemCmd::WriteResp) {
src/gpu-compute/compute_unit.cc:    } else if (pkt->cmd == MemCmd::SwapResp) {
src/gpu-compute/compute_unit.cc:              pkt->cmd.toString());
src/gpu-compute/compute_unit.cc:            prefetch_pkt->dataStatic(&foo);
src/gpu-compute/compute_unit.cc:            prefetch_pkt->senderState =
src/gpu-compute/compute_unit.cc:                         prefetch_pkt->senderState);
src/gpu-compute/compute_unit.cc:    PacketPtr new_pkt = new Packet(pkt->req, requestCmd);
src/gpu-compute/compute_unit.cc:    new_pkt->dataStatic(pkt->getPtr<uint8_t>());
src/gpu-compute/compute_unit.cc:    delete pkt->senderState;
src/gpu-compute/compute_unit.cc:    new_pkt->senderState =
src/gpu-compute/compute_unit.cc:    if (new_pkt->req->systemReq()) {
src/gpu-compute/compute_unit.cc:        new_pkt->req->requestorId(computeUnit->vramRequestorId());
src/gpu-compute/compute_unit.cc:            gpuDynInst->wfSlotId, mp_index, new_pkt->req->getPaddr());
src/gpu-compute/compute_unit.cc:    SenderState *sender_state = safe_cast<SenderState*>(pkt->senderState);
src/gpu-compute/compute_unit.cc:    if (pkt->req->systemReq()) {
src/gpu-compute/compute_unit.cc:                id, pkt->req->getPaddr());
src/gpu-compute/compute_unit.cc:                pkt->req->getPaddr());
src/gpu-compute/compute_unit.cc:    SenderState *sender_state = safe_cast<SenderState*>(pkt->senderState);
src/gpu-compute/compute_unit.cc:    if (pkt->req->systemReq()) {
src/gpu-compute/compute_unit.cc:                gpuDynInst->wfSlotId, pkt->req->getPaddr());
src/gpu-compute/compute_unit.cc:                pkt->req->getPaddr());
src/gpu-compute/compute_unit.cc:        [[maybe_unused]] Addr vaddr = pkt->req->getVaddr();
src/gpu-compute/compute_unit.cc:    assert(pkt->senderState);
src/gpu-compute/compute_unit.cc:        safe_cast<GpuTranslationState*>(pkt->senderState);
src/gpu-compute/compute_unit.cc:            "Translation of vaddr %#x failed\n", pkt->req->getVaddr());
src/gpu-compute/compute_unit.cc:    pkt->senderState = translation_state->saved;
src/gpu-compute/compute_unit.cc:        safe_cast<ScalarDTLBPort::SenderState*>(pkt->senderState);
src/gpu-compute/compute_unit.cc:    delete pkt->senderState;
src/gpu-compute/compute_unit.cc:        w->wfSlotId, w->kernId, pkt->req->getVaddr(), pkt->req->getPaddr());
src/gpu-compute/compute_unit.cc:    if (pkt->cmd == MemCmd::ReadResp) {
src/gpu-compute/compute_unit.cc:    } else if (pkt->cmd == MemCmd::WriteResp) {
src/gpu-compute/compute_unit.cc:            pkt->cmd.toString());
src/gpu-compute/compute_unit.cc:    PacketPtr req_pkt = new Packet(pkt->req, mem_cmd);
src/gpu-compute/compute_unit.cc:    req_pkt->dataStatic(pkt->getPtr<uint8_t>());
src/gpu-compute/compute_unit.cc:    req_pkt->senderState =
src/gpu-compute/compute_unit.cc:    if (req_pkt->req->systemReq()) {
src/gpu-compute/compute_unit.cc:        req_pkt->req->requestorId(computeUnit->vramRequestorId());
src/gpu-compute/compute_unit.cc:    [[maybe_unused]] Addr line = pkt->req->getPaddr();
src/gpu-compute/compute_unit.cc:            computeUnit->cu_id, pkt->req->getVaddr(), line);
src/gpu-compute/compute_unit.cc:    assert(pkt->senderState);
src/gpu-compute/compute_unit.cc:        = safe_cast<GpuTranslationState*>(pkt->senderState);
src/gpu-compute/compute_unit.cc:    pkt->senderState = translation_state->saved;
src/gpu-compute/compute_unit.cc:        safe_cast<ITLBPort::SenderState*>(pkt->senderState);
src/gpu-compute/compute_unit.cc:    delete pkt->senderState;
src/gpu-compute/compute_unit.cc:        assert(pkt->cmd == MemCmd::ReadResp);
src/gpu-compute/compute_unit.cc:        pkt->cmd = MemCmd::ReadReq;
src/gpu-compute/compute_unit.cc:        [[maybe_unused]] Addr vaddr = pkt->req->getVaddr();
src/gpu-compute/compute_unit.cc:            dynamic_cast<ComputeUnit::LDSPort::SenderState*>(pkt->senderState);
src/gpu-compute/compute_unit.cc:                gpuDynInst->wfSlotId, pkt->req->getPaddr());
src/gpu-compute/compute_unit.cc:                gpuDynInst->wfSlotId, pkt->req->getPaddr());
src/gpu-compute/fetch_unit.cc:        pkt->senderState = new ComputeUnit::ITLBPort::SenderState(wavefront);
src/gpu-compute/fetch_unit.cc:        pkt->senderState =
src/gpu-compute/fetch_unit.cc:                                                 false, pkt->senderState);
src/gpu-compute/fetch_unit.cc:        pkt->senderState =
src/gpu-compute/fetch_unit.cc:        if (!pkt->req->systemReq()) {
src/gpu-compute/fetch_unit.cc:            pkt->req->requestorId(computeUnit.vramRequestorId());
src/gpu-compute/fetch_unit.cc:             safe_cast<GpuTranslationState*>(pkt->senderState);
src/gpu-compute/fetch_unit.cc:    assert(pkt->req->hasPaddr());
src/gpu-compute/fetch_unit.cc:    assert(pkt->req->hasSize());
src/gpu-compute/fetch_unit.cc:            pkt->req->getPaddr());
src/gpu-compute/fetch_unit.cc:    if (!fetchBuf.at(wavefront->wfSlotId).isReserved(pkt->req->getVaddr())) {
src/gpu-compute/fetch_unit.cc:    if (!pkt->req->systemReq()) {
src/gpu-compute/fetch_unit.cc:        pkt->req->requestorId(computeUnit.vramRequestorId());
src/gpu-compute/fetch_unit.cc:    pkt->dataStatic(fetchBuf.at(wavefront->wfSlotId)
src/gpu-compute/fetch_unit.cc:                    .reservedBuf(pkt->req->getVaddr()));
src/gpu-compute/fetch_unit.cc:    pkt->senderState = new ComputeUnit::SQCPort::SenderState(wavefront);
src/gpu-compute/fetch_unit.cc:        if (pkt->req->systemReq()) {
src/gpu-compute/fetch_unit.cc:                    pkt->req->getPaddr());
src/gpu-compute/fetch_unit.cc:                    pkt->req->getPaddr());
src/gpu-compute/fetch_unit.cc:        safe_cast<ComputeUnit::SQCPort::SenderState*>(pkt->senderState);
src/gpu-compute/fetch_unit.cc:            wavefront->wfSlotId, pkt->req->getPaddr(), pkt->req->getSize());
src/gpu-compute/fetch_unit.cc:        fetchBuf.at(wavefront->wfSlotId).fetchDone(pkt->req->getVaddr());
src/gpu-compute/fetch_unit.cc:    delete pkt->senderState;
src/dev/x86/i8042.cc:    assert(pkt->getSize() == 1);
src/dev/x86/i8042.cc:    Addr addr = pkt->getAddr();
src/dev/x86/i8042.cc:        pkt->setLE<uint8_t>(data);
src/dev/x86/i8042.cc:        pkt->setLE<uint8_t>((uint8_t)statusReg);
src/dev/x86/i8042.cc:    pkt->makeAtomicResponse();
src/dev/x86/i8042.cc:    assert(pkt->getSize() == 1);
src/dev/x86/i8042.cc:    Addr addr = pkt->getAddr();
src/dev/x86/i8042.cc:    uint8_t data = pkt->getLE<uint8_t>();
src/dev/x86/i8042.cc:    pkt->makeAtomicResponse();
src/dev/x86/i8237.cc:    regs.read(pkt->getAddr(), pkt->getPtr<void>(), pkt->getSize());
src/dev/x86/i8237.cc:    pkt->makeAtomicResponse();
src/dev/x86/i8237.cc:    regs.write(pkt->getAddr(), pkt->getPtr<void>(), pkt->getSize());
src/dev/x86/i8237.cc:    pkt->makeAtomicResponse();
src/dev/x86/i8259.cc:    assert(pkt->getSize() == 1);
src/dev/x86/i8259.cc:    if (pkt->getAddr() == PhysAddrIntA) {
src/dev/x86/i8259.cc:        pkt->setLE<uint8_t>(getVector());
src/dev/x86/i8259.cc:    switch(pkt->getAddr() - pioAddr) {
src/dev/x86/i8259.cc:            pkt->setLE(IRR);
src/dev/x86/i8259.cc:            pkt->setLE(ISR);
src/dev/x86/i8259.cc:        pkt->setLE(IMR);
src/dev/x86/i8259.cc:    pkt->makeAtomicResponse();
src/dev/x86/i8259.cc:    assert(pkt->getSize() == 1);
src/dev/x86/i8259.cc:    uint8_t val = pkt->getLE<uint8_t>();
src/dev/x86/i8259.cc:    switch (pkt->getAddr() - pioAddr) {
src/dev/x86/i8259.cc:    pkt->makeAtomicResponse();
src/dev/x86/speaker.cc:    assert(pkt->getAddr() == pioAddr);
src/dev/x86/speaker.cc:    assert(pkt->getSize() == 1);
src/dev/x86/speaker.cc:    pkt->setLE((uint8_t)controlVal);
src/dev/x86/speaker.cc:    pkt->makeAtomicResponse();
src/dev/x86/speaker.cc:    assert(pkt->getAddr() == pioAddr);
src/dev/x86/speaker.cc:    assert(pkt->getSize() == 1);
src/dev/x86/speaker.cc:    SpeakerControl val = pkt->getLE<uint8_t>();
src/dev/x86/speaker.cc:    pkt->makeAtomicResponse();
src/dev/x86/i8254.cc:    assert(pkt->getSize() == 1);
src/dev/x86/i8254.cc:    Addr offset = pkt->getAddr() - pioAddr;
src/dev/x86/i8254.cc:        pkt->setLE(pit.readCounter(offset));
src/dev/x86/i8254.cc:        pkt->setLE(uint8_t(-1));
src/dev/x86/i8254.cc:    pkt->makeAtomicResponse();
src/dev/x86/i8254.cc:    assert(pkt->getSize() == 1);
src/dev/x86/i8254.cc:    Addr offset = pkt->getAddr() - pioAddr;
src/dev/x86/i8254.cc:        pit.writeCounter(offset, pkt->getLE<uint8_t>());
src/dev/x86/i8254.cc:        pit.writeControl(pkt->getLE<uint8_t>());
src/dev/x86/i8254.cc:    pkt->makeAtomicResponse();
src/dev/x86/cmos.cc:    assert(pkt->getSize() == 1);
src/dev/x86/cmos.cc:    switch(pkt->getAddr() - pioAddr)
src/dev/x86/cmos.cc:        pkt->setLE(address);
src/dev/x86/cmos.cc:        pkt->setLE(readRegister(address.regNum));
src/dev/x86/cmos.cc:    pkt->makeAtomicResponse();
src/dev/x86/cmos.cc:    assert(pkt->getSize() == 1);
src/dev/x86/cmos.cc:    switch(pkt->getAddr() - pioAddr)
src/dev/x86/cmos.cc:        address = pkt->getLE<uint8_t>();
src/dev/x86/cmos.cc:        writeRegister(address.regNum, pkt->getLE<uint8_t>());
src/dev/x86/cmos.cc:    pkt->makeAtomicResponse();
src/dev/x86/i82094aa.cc:    assert(pkt->getSize() == 4);
src/dev/x86/i82094aa.cc:    Addr offset = pkt->getAddr() - pioAddr;
src/dev/x86/i82094aa.cc:        pkt->setLE<uint32_t>(regSel);
src/dev/x86/i82094aa.cc:        pkt->setLE<uint32_t>(readReg(regSel));
src/dev/x86/i82094aa.cc:    pkt->makeAtomicResponse();
src/dev/x86/i82094aa.cc:    assert(pkt->getSize() == 4);
src/dev/x86/i82094aa.cc:    Addr offset = pkt->getAddr() - pioAddr;
src/dev/x86/i82094aa.cc:        regSel = pkt->getLE<uint32_t>();
src/dev/x86/i82094aa.cc:        writeReg(regSel, pkt->getLE<uint32_t>());
src/dev/x86/i82094aa.cc:    pkt->makeAtomicResponse();
src/dev/x86/i82094aa.cc:            msg_copy.vector = pkt->getLE<uint8_t>();
src/dev/x86/intdev.hh:        panic_if(pkt->cmd != MemCmd::WriteReq,
src/dev/x86/intdev.hh:                name(), pkt->cmd.toString(), getPeer());
src/dev/x86/intdev.hh:        pkt->headerDelay = pkt->payloadDelay = 0;
src/dev/x86/intdev.hh:    pkt->allocate();
src/dev/x86/intdev.hh:    pkt->setRaw<T>(payload);
src/dev/x86/intdev.hh:        assert(pkt->isResponse());
src/dev/x86/intdev.hh:        auto *oc = safe_cast<OnCompletion *>(pkt->popSenderState());
src/dev/x86/intdev.hh:            pkt->pushSenderState(new OnCompletion(func));
src/dev/arm/fvp_base_pwr_ctrl.cc:    const Addr addr = pkt->getAddr() - pioAddr;
src/dev/arm/fvp_base_pwr_ctrl.cc:    const size_t size = pkt->getSize();
src/dev/arm/fvp_base_pwr_ctrl.cc:    pkt->setUintX(resp, ByteOrder::little);
src/dev/arm/fvp_base_pwr_ctrl.cc:    pkt->makeResponse();
src/dev/arm/fvp_base_pwr_ctrl.cc:    const Addr addr = pkt->getAddr() - pioAddr;
src/dev/arm/fvp_base_pwr_ctrl.cc:    const size_t size = pkt->getSize();
src/dev/arm/fvp_base_pwr_ctrl.cc:    uint64_t data = pkt->getUintX(ByteOrder::little);
src/dev/arm/fvp_base_pwr_ctrl.cc:    pkt->makeResponse();
src/dev/arm/smmu_v3.cc:        pkt->getAddr(), pkt->getSize());
src/dev/arm/smmu_v3.cc:    pkt->headerDelay = pkt->payloadDelay = 0;
src/dev/arm/smmu_v3.cc:        safe_cast<SMMUProcess *>(pkt->popSenderState());
src/dev/arm/smmu_v3.cc:            a.pkt->getAddr(), a.pkt->getSize());
src/dev/arm/smmu_v3.cc:        pkt->getAddr(), pkt->getSize());
src/dev/arm/smmu_v3.cc:    pkt->headerDelay = pkt->payloadDelay = 0;
src/dev/arm/smmu_v3.cc:        safe_cast<SMMUProcess *>(pkt->popSenderState());
src/dev/arm/smmu_v3.cc:            a.pkt->getAddr(), a.pkt->getSize());
src/dev/arm/smmu_v3.cc:                action.pkt->pushSenderState(proc);
src/dev/arm/smmu_v3.cc:                        action.pkt->getAddr(), action.pkt->getSize());
src/dev/arm/smmu_v3.cc:            action.pkt->pushSenderState(proc);
src/dev/arm/smmu_v3.cc:                    action.pkt->getAddr(), action.pkt->getSize());
src/dev/arm/smmu_v3.cc:            action.pkt->headerDelay = action.pkt->payloadDelay = 0;
src/dev/arm/smmu_v3.cc:                    action.pkt->getAddr(),
src/dev/arm/smmu_v3.cc:                    action.pkt->getSize());
src/dev/arm/smmu_v3.cc:            action.pkt->headerDelay = action.pkt->payloadDelay = 0;
src/dev/arm/smmu_v3.cc:                    action.pkt->getAddr(), action.pkt->getSize());
src/dev/arm/smmu_v3.cc:            pkt->getAddr(), pkt->getSize());
src/dev/arm/smmu_v3.cc:    int offset = pkt->getAddr() - regsMap.start();
src/dev/arm/smmu_v3.cc:    switch (pkt->getSize()) {
src/dev/arm/smmu_v3.cc:        pkt->setLE<uint32_t>(*reinterpret_cast<uint32_t *>(reg_ptr));
src/dev/arm/smmu_v3.cc:        pkt->setLE<uint64_t>(*reinterpret_cast<uint64_t *>(reg_ptr));
src/dev/arm/smmu_v3.cc:        panic("smmu: unallowed access size: %d bytes\n", pkt->getSize());
src/dev/arm/smmu_v3.cc:    pkt->makeAtomicResponse();
src/dev/arm/smmu_v3.cc:    int offset = pkt->getAddr() - regsMap.start();
src/dev/arm/smmu_v3.cc:            pkt->getAddr(), pkt->getSize(),
src/dev/arm/smmu_v3.cc:            pkt->getSize() == sizeof(uint64_t) ?
src/dev/arm/smmu_v3.cc:            pkt->getLE<uint64_t>() : pkt->getLE<uint32_t>());
src/dev/arm/smmu_v3.cc:            assert(pkt->getSize() == sizeof(uint32_t));
src/dev/arm/smmu_v3.cc:            regs.cr0 = regs.cr0ack = pkt->getLE<uint32_t>();
src/dev/arm/smmu_v3.cc:            assert(pkt->getSize() == sizeof(uint32_t));
src/dev/arm/smmu_v3.cc:                regs.irq_ctrl = regs.irq_ctrlack = pkt->getLE<uint32_t>();
src/dev/arm/smmu_v3.cc:            assert(pkt->getSize() == sizeof(uint32_t));
src/dev/arm/smmu_v3.cc:                pkt->getLE<uint32_t>();
src/dev/arm/smmu_v3.cc:            assert(pkt->getSize() == sizeof(uint32_t));
src/dev/arm/smmu_v3.cc:                    pkt->getLE<uint32_t>();
src/dev/arm/smmu_v3.cc:            assert(pkt->getSize() == sizeof(uint32_t));
src/dev/arm/smmu_v3.cc:                pkt->getLE<uint32_t>();
src/dev/arm/smmu_v3.cc:            assert(pkt->getSize() == sizeof(uint64_t));
src/dev/arm/smmu_v3.cc:                pkt->getLE<uint64_t>();
src/dev/arm/smmu_v3.cc:            assert(pkt->getSize() == sizeof(uint64_t));
src/dev/arm/smmu_v3.cc:                    pkt->getLE<uint64_t>();
src/dev/arm/smmu_v3.cc:            assert(pkt->getSize() == sizeof(uint64_t));
src/dev/arm/smmu_v3.cc:                pkt->getLE<uint64_t>();
src/dev/arm/smmu_v3.cc:            assert(pkt->getSize() == sizeof(uint64_t));
src/dev/arm/smmu_v3.cc:                pkt->getLE<uint64_t>();
src/dev/arm/smmu_v3.cc:    pkt->makeAtomicResponse();
src/dev/arm/energy_ctrl.cc:    assert(pkt->getAddr() >= pioAddr && pkt->getAddr() < pioAddr + pioSize);
src/dev/arm/energy_ctrl.cc:    assert(pkt->getSize() == 4);
src/dev/arm/energy_ctrl.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/arm/energy_ctrl.cc:        pkt->setLE<uint32_t>(0);
src/dev/arm/energy_ctrl.cc:        pkt->makeAtomicResponse();
src/dev/arm/energy_ctrl.cc:    pkt->setLE<uint32_t>(result);
src/dev/arm/energy_ctrl.cc:    pkt->makeAtomicResponse();
src/dev/arm/energy_ctrl.cc:    assert(pkt->getAddr() >= pioAddr && pkt->getAddr() < pioAddr + pioSize);
src/dev/arm/energy_ctrl.cc:    assert(pkt->getSize() == 4);
src/dev/arm/energy_ctrl.cc:    data = pkt->getLE<uint32_t>();
src/dev/arm/energy_ctrl.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/arm/energy_ctrl.cc:        pkt->makeAtomicResponse();
src/dev/arm/energy_ctrl.cc:    pkt->makeAtomicResponse();
src/dev/arm/amba_fake.cc:    assert(pkt->getAddr() >= pioAddr && pkt->getAddr() < pioAddr + pioSize);
src/dev/arm/amba_fake.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/arm/amba_fake.cc:    pkt->setLE<uint32_t>(0);
src/dev/arm/amba_fake.cc:    pkt->makeAtomicResponse();
src/dev/arm/amba_fake.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/arm/amba_fake.cc:    pkt->makeAtomicResponse();
src/dev/arm/gic_v2m.cc:    int frame = frameFromAddr(pkt->getAddr());
src/dev/arm/gic_v2m.cc:    Addr offset = pkt->getAddr() - frames[frame]->addr;
src/dev/arm/gic_v2m.cc:        pkt->setLE<uint32_t>((frames[frame]->spi_base << 16) |
src/dev/arm/gic_v2m.cc:        pkt->setLE<uint32_t>(0x4 | ((4+log2framenum) << 4));
src/dev/arm/gic_v2m.cc:        pkt->setLE<uint32_t>(0);
src/dev/arm/gic_v2m.cc:    pkt->makeAtomicResponse();
src/dev/arm/gic_v2m.cc:    int frame = frameFromAddr(pkt->getAddr());
src/dev/arm/gic_v2m.cc:    Addr offset = pkt->getAddr() - frames[frame]->addr;
src/dev/arm/gic_v2m.cc:        uint32_t m = pkt->getLE<uint32_t>();
src/dev/arm/gic_v2m.cc:    pkt->makeAtomicResponse();
src/dev/arm/vgic.cc:    Addr addr = pkt->getAddr();
src/dev/arm/vgic.cc:        panic("Read to unknown address %#x\n", pkt->getAddr());
src/dev/arm/vgic.cc:    Addr addr = pkt->getAddr();
src/dev/arm/vgic.cc:        panic("Write to unknown address %#x\n", pkt->getAddr());
src/dev/arm/vgic.cc:    Addr daddr = pkt->getAddr() - vcpuAddr;
src/dev/arm/vgic.cc:    ContextID ctx_id = pkt->req->contextId();
src/dev/arm/vgic.cc:        pkt->setLE<uint32_t>(vid->vctrl);
src/dev/arm/vgic.cc:              pkt->setLE<uint32_t>(1023); // "No int" marker
src/dev/arm/vgic.cc:              pkt->setLE<uint32_t>(lr->VirtualID |
src/dev/arm/vgic.cc:        pkt->setLE<uint32_t>(gicvIIDR);
src/dev/arm/vgic.cc:    pkt->makeAtomicResponse();
src/dev/arm/vgic.cc:    Addr daddr = pkt->getAddr() - hvAddr;
src/dev/arm/vgic.cc:    ContextID ctx_id = pkt->req->contextId();
src/dev/arm/vgic.cc:        pkt->setLE<uint32_t>(vid->hcr);
src/dev/arm/vgic.cc:        pkt->setLE<uint32_t>(0x44000000 | (NUM_LR - 1));
src/dev/arm/vgic.cc:        pkt->setLE<uint32_t>(
src/dev/arm/vgic.cc:        pkt->setLE<uint32_t>(getMISR(vid));
src/dev/arm/vgic.cc:        pkt->setLE<uint32_t>(vid->eisr & 0xffffffff);
src/dev/arm/vgic.cc:        pkt->setLE<uint32_t>(vid->eisr >> 32);
src/dev/arm/vgic.cc:          pkt->setLE<uint32_t>(bm);
src/dev/arm/vgic.cc:          pkt->setLE<uint32_t>(bm);
src/dev/arm/vgic.cc:        pkt->setLE<uint32_t>(0);
src/dev/arm/vgic.cc:        pkt->setLE<uint32_t>(vid->LR[(daddr - GICH_LR0) >> 2]);
src/dev/arm/vgic.cc:    pkt->makeAtomicResponse();
src/dev/arm/vgic.cc:    Addr daddr = pkt->getAddr() - vcpuAddr;
src/dev/arm/vgic.cc:    ContextID ctx_id = pkt->req->contextId();
src/dev/arm/vgic.cc:            daddr, pkt->getLE<uint32_t>());
src/dev/arm/vgic.cc:        vid->vctrl = pkt->getLE<uint32_t>();
src/dev/arm/vgic.cc:        vid->VMPriMask = pkt->getLE<uint32_t>();
src/dev/arm/vgic.cc:          uint32_t w = pkt->getLE<uint32_t>();
src/dev/arm/vgic.cc:                pkt->getLE<uint32_t>(), daddr);
src/dev/arm/vgic.cc:    pkt->makeAtomicResponse();
src/dev/arm/vgic.cc:    Addr daddr = pkt->getAddr() - hvAddr;
src/dev/arm/vgic.cc:    ContextID ctx_id = pkt->req->contextId();
src/dev/arm/vgic.cc:            daddr, pkt->getLE<uint32_t>());
src/dev/arm/vgic.cc:        vid->hcr = pkt->getLE<uint32_t>();
src/dev/arm/vgic.cc:          uint32_t d = pkt->getLE<uint32_t>();
src/dev/arm/vgic.cc:        vid->LR[(daddr - GICH_LR0) >> 2] = pkt->getLE<uint32_t>();
src/dev/arm/vgic.cc:    pkt->makeAtomicResponse();
src/dev/arm/gic_v3.cc:    const Addr addr = pkt->getAddr();
src/dev/arm/gic_v3.cc:    const size_t size = pkt->getSize();
src/dev/arm/gic_v3.cc:    bool is_secure_access = pkt->isSecure();
src/dev/arm/gic_v3.cc:                pkt->req->contextId(), daddr, size, is_secure_access, resp);
src/dev/arm/gic_v3.cc:                redist->processorNumber(), pkt->req->contextId(), daddr, size,
src/dev/arm/gic_v3.cc:    pkt->setUintX(resp, ByteOrder::little);
src/dev/arm/gic_v3.cc:    pkt->makeAtomicResponse();
src/dev/arm/gic_v3.cc:    const size_t size = pkt->getSize();
src/dev/arm/gic_v3.cc:    uint64_t data = pkt->getUintX(ByteOrder::little);
src/dev/arm/gic_v3.cc:    const Addr addr = pkt->getAddr();
src/dev/arm/gic_v3.cc:    bool is_secure_access = pkt->isSecure();
src/dev/arm/gic_v3.cc:                pkt->req->contextId(), daddr, size, is_secure_access, data);
src/dev/arm/gic_v3.cc:                redist->processorNumber(), pkt->req->contextId(), daddr, size,
src/dev/arm/gic_v3.cc:    pkt->makeAtomicResponse();
src/dev/arm/amba_device.cc:    Addr daddr = pkt->getAddr() - pio_addr;
src/dev/arm/amba_device.cc:            pkt->getAddr() - pio_addr, byte);
src/dev/arm/amba_device.cc:    pkt->setUintX((amba_id >> byte) & 0xFF, ByteOrder::little);
src/dev/arm/gic_v3_its.cc:    a.pkt->dataStatic(ptr);
src/dev/arm/gic_v3_its.cc:    assert(pkt->getSize() >= size);
src/dev/arm/gic_v3_its.cc:    a.pkt->dataStatic(ptr);
src/dev/arm/gic_v3_its.cc:    assert(pkt->getSize() >= size);
src/dev/arm/gic_v3_its.cc:    const uint32_t device_id = pkt->req->streamId();
src/dev/arm/gic_v3_its.cc:    const uint32_t event_id = pkt->getLE<uint32_t>();
src/dev/arm/gic_v3_its.cc:    const Addr addr = pkt->getAddr() - pioAddr;
src/dev/arm/gic_v3_its.cc:    pkt->setUintX(value, ByteOrder::little);
src/dev/arm/gic_v3_its.cc:    pkt->makeAtomicResponse();
src/dev/arm/gic_v3_its.cc:    Addr addr = pkt->getAddr() - pioAddr;
src/dev/arm/gic_v3_its.cc:        assert(pkt->getSize() == sizeof(uint32_t));
src/dev/arm/gic_v3_its.cc:        gitsControl = (pkt->getLE<uint32_t>() & ~CTLR_QUIESCENT);
src/dev/arm/gic_v3_its.cc:        if (pkt->getSize() == sizeof(uint32_t)) {
src/dev/arm/gic_v3_its.cc:            gitsCbaser.low = pkt->getLE<uint32_t>();
src/dev/arm/gic_v3_its.cc:            assert(pkt->getSize() == sizeof(uint64_t));
src/dev/arm/gic_v3_its.cc:            gitsCbaser = pkt->getLE<uint64_t>();
src/dev/arm/gic_v3_its.cc:        assert(pkt->getSize() == sizeof(uint32_t));
src/dev/arm/gic_v3_its.cc:        gitsCbaser.high = pkt->getLE<uint32_t>();
src/dev/arm/gic_v3_its.cc:        if (pkt->getSize() == sizeof(uint32_t)) {
src/dev/arm/gic_v3_its.cc:            gitsCwriter.low = pkt->getLE<uint32_t>();
src/dev/arm/gic_v3_its.cc:            assert(pkt->getSize() == sizeof(uint64_t));
src/dev/arm/gic_v3_its.cc:            gitsCwriter = pkt->getLE<uint64_t>();
src/dev/arm/gic_v3_its.cc:        assert(pkt->getSize() == sizeof(uint32_t));
src/dev/arm/gic_v3_its.cc:        gitsCwriter.high = pkt->getLE<uint32_t>();
src/dev/arm/gic_v3_its.cc:            const uint64_t val = pkt->getLE<uint64_t>() & w_mask;
src/dev/arm/gic_v3_its.cc:    pkt->makeAtomicResponse();
src/dev/arm/gic_v3_its.cc:    pkt->headerDelay = pkt->payloadDelay = 0;
src/dev/arm/gic_v3_its.cc:        safe_cast<ItsProcess *>(pkt->popSenderState());
src/dev/arm/gic_v3_its.cc:        action.pkt->pushSenderState(proc);
src/dev/arm/hdlcd.cc:    assert(pkt->getAddr() >= pioAddr &&
src/dev/arm/hdlcd.cc:           pkt->getAddr() < pioAddr + pioSize);
src/dev/arm/hdlcd.cc:    const Addr daddr = pkt->getAddr() - pioAddr;
src/dev/arm/hdlcd.cc:    panic_if(pkt->getSize() != 4,
src/dev/arm/hdlcd.cc:             daddr, pkt->getSize());
src/dev/arm/hdlcd.cc:    pkt->setLE<uint32_t>(data);
src/dev/arm/hdlcd.cc:    pkt->makeAtomicResponse();
src/dev/arm/hdlcd.cc:    assert(pkt->getAddr() >= pioAddr &&
src/dev/arm/hdlcd.cc:           pkt->getAddr() < pioAddr + pioSize);
src/dev/arm/hdlcd.cc:    const Addr daddr = pkt->getAddr() - pioAddr;
src/dev/arm/hdlcd.cc:    panic_if(pkt->getSize() != 4,
src/dev/arm/hdlcd.cc:             daddr, pkt->getSize());
src/dev/arm/hdlcd.cc:    const uint32_t data = pkt->getLE<uint32_t>();
src/dev/arm/hdlcd.cc:    pkt->makeAtomicResponse();
src/dev/arm/a9scu.cc:    assert(pkt->getAddr() >= pioAddr && pkt->getAddr() < pioAddr + pioSize);
src/dev/arm/a9scu.cc:    assert(pkt->getSize() == 4);
src/dev/arm/a9scu.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/arm/a9scu.cc:        pkt->setLE(1); // SCU already enabled
src/dev/arm/a9scu.cc:            pkt->setLE(smp_bits << 4 | core_cnt);
src/dev/arm/a9scu.cc:    pkt->makeAtomicResponse();
src/dev/arm/a9scu.cc:    assert(pkt->getAddr() >= pioAddr && pkt->getAddr() < pioAddr + pioSize);
src/dev/arm/a9scu.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/arm/a9scu.cc:    pkt->makeAtomicResponse();
src/dev/arm/vio_mmio.cc:    const Addr offset = pkt->getAddr() - pioAddr;
src/dev/arm/vio_mmio.cc:    const unsigned size(pkt->getSize());
src/dev/arm/vio_mmio.cc:    pkt->makeResponse();
src/dev/arm/vio_mmio.cc:    pkt->setLE<uint32_t>(value);
src/dev/arm/vio_mmio.cc:    const Addr offset = pkt->getAddr() - pioAddr;
src/dev/arm/vio_mmio.cc:    const unsigned size(pkt->getSize());
src/dev/arm/vio_mmio.cc:    DPRINTF(VIOIface, "    value: 0x%x\n", pkt->getLE<uint32_t>());
src/dev/arm/vio_mmio.cc:    pkt->makeResponse();
src/dev/arm/vio_mmio.cc:    write(offset, pkt->getLE<uint32_t>());
src/dev/arm/timer_cpulocal.cc:    assert(pkt->getAddr() >= pioAddr && pkt->getAddr() < pioAddr + pioSize);
src/dev/arm/timer_cpulocal.cc:    assert(pkt->getSize() == 4);
src/dev/arm/timer_cpulocal.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/arm/timer_cpulocal.cc:    ContextID cpu_id = pkt->req->contextId();
src/dev/arm/timer_cpulocal.cc:    pkt->makeAtomicResponse();
src/dev/arm/timer_cpulocal.cc:        pkt->setLE<uint32_t>(timerLoadValue);
src/dev/arm/timer_cpulocal.cc:        pkt->setLE<uint32_t>(time);
src/dev/arm/timer_cpulocal.cc:        pkt->setLE<uint32_t>(timerControl);
src/dev/arm/timer_cpulocal.cc:        pkt->setLE<uint32_t>(rawIntTimer);
src/dev/arm/timer_cpulocal.cc:        pkt->setLE<uint32_t>(watchdogLoadValue);
src/dev/arm/timer_cpulocal.cc:        pkt->setLE<uint32_t>(time);
src/dev/arm/timer_cpulocal.cc:        pkt->setLE<uint32_t>(watchdogControl);
src/dev/arm/timer_cpulocal.cc:        pkt->setLE<uint32_t>(rawIntWatchdog);
src/dev/arm/timer_cpulocal.cc:        pkt->setLE<uint32_t>(rawResetWatchdog);
src/dev/arm/timer_cpulocal.cc:    assert(pkt->getAddr() >= pioAddr && pkt->getAddr() < pioAddr + pioSize);
src/dev/arm/timer_cpulocal.cc:    assert(pkt->getSize() == 4);
src/dev/arm/timer_cpulocal.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/arm/timer_cpulocal.cc:    ContextID cpu_id = pkt->req->contextId();
src/dev/arm/timer_cpulocal.cc:    pkt->makeAtomicResponse();
src/dev/arm/timer_cpulocal.cc:        timerLoadValue = pkt->getLE<uint32_t>();
src/dev/arm/timer_cpulocal.cc:        restartTimerCounter(pkt->getLE<uint32_t>());
src/dev/arm/timer_cpulocal.cc:        timerControl = pkt->getLE<uint32_t>();
src/dev/arm/timer_cpulocal.cc:        watchdogLoadValue = pkt->getLE<uint32_t>();
src/dev/arm/timer_cpulocal.cc:            restartWatchdogCounter(pkt->getLE<uint32_t>());
src/dev/arm/timer_cpulocal.cc:        watchdogControl = pkt->getLE<uint32_t>();
src/dev/arm/timer_cpulocal.cc:        watchdogDisableReg = pkt->getLE<uint32_t>();
src/dev/arm/gic_v2.cc:    const Addr addr = pkt->getAddr();
src/dev/arm/gic_v2.cc:        panic("Read to unknown address %#x\n", pkt->getAddr());
src/dev/arm/gic_v2.cc:    const Addr addr = pkt->getAddr();
src/dev/arm/gic_v2.cc:        panic("Write to unknown address %#x\n", pkt->getAddr());
src/dev/arm/gic_v2.cc:    const Addr daddr = pkt->getAddr() - distRange.start();
src/dev/arm/gic_v2.cc:    const ContextID ctx = pkt->req->contextId();
src/dev/arm/gic_v2.cc:    const uint32_t resp = readDistributor(ctx, daddr, pkt->getSize());
src/dev/arm/gic_v2.cc:    switch (pkt->getSize()) {
src/dev/arm/gic_v2.cc:        pkt->setLE<uint8_t>(resp);
src/dev/arm/gic_v2.cc:        pkt->setLE<uint16_t>(resp);
src/dev/arm/gic_v2.cc:        pkt->setLE<uint32_t>(resp);
src/dev/arm/gic_v2.cc:               pkt->getSize());
src/dev/arm/gic_v2.cc:    pkt->makeAtomicResponse();
src/dev/arm/gic_v2.cc:    const Addr daddr = pkt->getAddr() - cpuRange.start();
src/dev/arm/gic_v2.cc:    assert(pkt->req->hasContextId());
src/dev/arm/gic_v2.cc:    const ContextID ctx = pkt->req->contextId();
src/dev/arm/gic_v2.cc:    pkt->setLE<uint32_t>(readCpu(ctx, daddr));
src/dev/arm/gic_v2.cc:    pkt->makeAtomicResponse();
src/dev/arm/gic_v2.cc:    const Addr daddr = pkt->getAddr() - distRange.start();
src/dev/arm/gic_v2.cc:    assert(pkt->req->hasContextId());
src/dev/arm/gic_v2.cc:    const ContextID ctx = pkt->req->contextId();
src/dev/arm/gic_v2.cc:    const size_t data_sz = pkt->getSize();
src/dev/arm/gic_v2.cc:        pkt_data = pkt->getLE<uint8_t>();
src/dev/arm/gic_v2.cc:        pkt_data = pkt->getLE<uint16_t>();
src/dev/arm/gic_v2.cc:        pkt_data = pkt->getLE<uint32_t>();
src/dev/arm/gic_v2.cc:    pkt->makeAtomicResponse();
src/dev/arm/gic_v2.cc:    const Addr daddr = pkt->getAddr() - cpuRange.start();
src/dev/arm/gic_v2.cc:    assert(pkt->req->hasContextId());
src/dev/arm/gic_v2.cc:    const ContextID ctx = pkt->req->contextId();
src/dev/arm/gic_v2.cc:    const uint32_t data = pkt->getLE<uint32_t>();
src/dev/arm/gic_v2.cc:    pkt->makeAtomicResponse();
src/dev/arm/amba.hh:    return pkt->req->requestorId();
src/dev/arm/smmu_v3_ports.cc:    Addr addr = pkt->getAddr();
src/dev/arm/smmu_v3_ports.cc:    unsigned size = pkt->getSize();
src/dev/arm/smmu_v3_ports.cc:    pkt->headerDelay = pkt->payloadDelay = 0;
src/dev/arm/smmu_v3_ports.cc:    return pkt->isRead() ? smmu.readControl(pkt) : smmu.writeControl(pkt);
src/dev/arm/smmu_v3_deviceifc.cc:            devicePort->getPeer(), pkt->getAddr(), pkt->getSize());
src/dev/arm/smmu_v3_deviceifc.cc:            devicePort->getPeer(), pkt->getAddr(), pkt->getSize());
src/dev/arm/smmu_v3_deviceifc.cc:    pkt->headerDelay = pkt->payloadDelay = 0;
src/dev/arm/smmu_v3_deviceifc.cc:        (pkt->getSize() + (portWidth-1)) / portWidth;
src/dev/arm/smmu_v3_deviceifc.cc:        (pkt->isWrite() && wrBufSlotsRemaining < nbeats))
src/dev/arm/smmu_v3_deviceifc.cc:    if (pkt->isWrite())
src/dev/arm/smmu_v3_deviceifc.cc:            pkt->getAddr(), pkt->getSize());
src/dev/arm/smmu_v3_deviceifc.cc:            pkt->getAddr(), pkt->getSize());
src/dev/arm/smmu_v3_deviceifc.cc:    pkt->headerDelay = pkt->payloadDelay = 0;
src/dev/arm/smmu_v3_deviceifc.cc:            pkt->getAddr(), pkt->getSize());
src/dev/arm/smmu_v3_deviceifc.cc:    pkt->headerDelay = pkt->payloadDelay = 0;
src/dev/arm/smmu_v3_deviceifc.cc:        safe_cast<SMMUProcess *>(pkt->popSenderState());
src/dev/arm/gpu_nomali.cc:    assert(pkt->getAddr() >= pioAddr);
src/dev/arm/gpu_nomali.cc:    const Addr addr(pkt->getAddr() - pioAddr);
src/dev/arm/gpu_nomali.cc:    const unsigned size(pkt->getSize());
src/dev/arm/gpu_nomali.cc:    pkt->setLE<uint32_t>(readReg(addr));
src/dev/arm/gpu_nomali.cc:    pkt->makeResponse();
src/dev/arm/gpu_nomali.cc:    assert(pkt->getAddr() >= pioAddr);
src/dev/arm/gpu_nomali.cc:    const Addr addr(pkt->getAddr() - pioAddr);
src/dev/arm/gpu_nomali.cc:    const unsigned size(pkt->getSize());
src/dev/arm/gpu_nomali.cc:    writeReg(addr, pkt->getLE<uint32_t>());
src/dev/arm/gpu_nomali.cc:    pkt->makeAtomicResponse();
src/dev/arm/rtc_pl031.cc:    assert(pkt->getAddr() >= pioAddr && pkt->getAddr() < pioAddr + pioSize);
src/dev/arm/rtc_pl031.cc:    assert(pkt->getSize() <= 4);
src/dev/arm/rtc_pl031.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/arm/rtc_pl031.cc:            data = pkt->getUintX(ByteOrder::little);
src/dev/arm/rtc_pl031.cc:    pkt->setUintX(data, ByteOrder::little);
src/dev/arm/rtc_pl031.cc:    pkt->makeAtomicResponse();
src/dev/arm/rtc_pl031.cc:    assert(pkt->getAddr() >= pioAddr && pkt->getAddr() < pioAddr + pioSize);
src/dev/arm/rtc_pl031.cc:    assert(pkt->getSize() <= 4);
src/dev/arm/rtc_pl031.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/arm/rtc_pl031.cc:        matchVal = pkt->getLE<uint32_t>();
src/dev/arm/rtc_pl031.cc:        timeVal = pkt->getLE<uint32_t>();
src/dev/arm/rtc_pl031.cc:        maskInt = pkt->getLE<uint32_t>();
src/dev/arm/rtc_pl031.cc:        if (pkt->getLE<uint32_t>()) {
src/dev/arm/rtc_pl031.cc:    pkt->makeAtomicResponse();
src/dev/arm/ufs_device.cc:    switch (pkt->getAddr() & 0xFF)
src/dev/arm/ufs_device.cc:    pkt->setLE<uint32_t>(data);
src/dev/arm/ufs_device.cc:    pkt->makeResponse();
src/dev/arm/ufs_device.cc:    assert(pkt->getSize() <= 4);
src/dev/arm/ufs_device.cc:    const uint32_t data = pkt->getUintX(ByteOrder::little);
src/dev/arm/ufs_device.cc:    switch (pkt->getAddr() & 0xFF)
src/dev/arm/ufs_device.cc:    pkt->makeResponse();
src/dev/arm/smmu_v3_transl.cc:    req.addr         = pkt->getAddr();
src/dev/arm/smmu_v3_transl.cc:    req.size         = pkt->getSize();
src/dev/arm/smmu_v3_transl.cc:    req.sid          = pkt->req->streamId();
src/dev/arm/smmu_v3_transl.cc:    req.ssid         = pkt->req->hasSubstreamId() ?
src/dev/arm/smmu_v3_transl.cc:        pkt->req->substreamId() : 0;
src/dev/arm/smmu_v3_transl.cc:    req.isWrite      = pkt->isWrite();
src/dev/arm/smmu_v3_transl.cc:            request.pkt->makeAtomicResponse();
src/dev/arm/smmu_v3_transl.cc:            request.pkt->makeTimingResponse();
src/dev/arm/smmu_v3_transl.cc:    a.pkt->setAddr(tr.addr);
src/dev/arm/smmu_v3_transl.cc:    a.pkt->req->setPaddr(tr.addr);
src/dev/arm/smmu_v3_transl.cc:        pkt->setAddr(request.addr);
src/dev/arm/rv_ctrl.cc:    assert(pkt->getAddr() >= pioAddr && pkt->getAddr() < pioAddr + pioSize);
src/dev/arm/rv_ctrl.cc:    assert(pkt->getSize() == 4);
src/dev/arm/rv_ctrl.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/arm/rv_ctrl.cc:        pkt->setLE(params().proc_id0);
src/dev/arm/rv_ctrl.cc:        pkt->setLE(params().proc_id1);
src/dev/arm/rv_ctrl.cc:        pkt->setLE((uint32_t)(clk));
src/dev/arm/rv_ctrl.cc:        pkt->setLE((uint32_t)(clk100));
src/dev/arm/rv_ctrl.cc:        pkt->setLE<uint32_t>(0);
src/dev/arm/rv_ctrl.cc:        pkt->setLE<uint32_t>(0x00001F00);
src/dev/arm/rv_ctrl.cc:        pkt->setLE<uint32_t>(0x00012C5C);
src/dev/arm/rv_ctrl.cc:        pkt->setLE<uint32_t>(0x00002CC0);
src/dev/arm/rv_ctrl.cc:        pkt->setLE<uint32_t>(0x00002C75);
src/dev/arm/rv_ctrl.cc:        pkt->setLE<uint32_t>(0x00020211);
src/dev/arm/rv_ctrl.cc:        pkt->setLE<uint32_t>(0x00002C75);
src/dev/arm/rv_ctrl.cc:        pkt->setLE<uint32_t>(sysLock);
src/dev/arm/rv_ctrl.cc:        pkt->setLE<uint32_t>(flags);
src/dev/arm/rv_ctrl.cc:        pkt->setLE<uint32_t>(params().idreg);
src/dev/arm/rv_ctrl.cc:        pkt->setLE<uint32_t>(1);
src/dev/arm/rv_ctrl.cc:        pkt->setLE<uint32_t>(scData);
src/dev/arm/rv_ctrl.cc:        pkt->setLE<uint32_t>(0); // not busy
src/dev/arm/rv_ctrl.cc:        pkt->setLE<uint32_t>(0);
src/dev/arm/rv_ctrl.cc:    pkt->makeAtomicResponse();
src/dev/arm/rv_ctrl.cc:    assert(pkt->getAddr() >= pioAddr && pkt->getAddr() < pioAddr + pioSize);
src/dev/arm/rv_ctrl.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/arm/rv_ctrl.cc:        sysLock.lockVal = pkt->getLE<uint16_t>();
src/dev/arm/rv_ctrl.cc:        flags = pkt->getLE<uint32_t>();
src/dev/arm/rv_ctrl.cc:        scData = pkt->getLE<uint32_t>();
src/dev/arm/rv_ctrl.cc:          CfgCtrlReg req = pkt->getLE<uint32_t>();
src/dev/arm/rv_ctrl.cc:             daddr, pkt->getLE<uint32_t>());
src/dev/arm/rv_ctrl.cc:    pkt->makeAtomicResponse();
src/dev/arm/pl011.cc:    assert(pkt->getAddr() >= pioAddr && pkt->getAddr() < pioAddr + pioSize);
src/dev/arm/pl011.cc:    assert(pkt->getSize() <= 4);
src/dev/arm/pl011.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/arm/pl011.cc:    DPRINTF(Uart, " read register %#x size=%d\n", daddr, pkt->getSize());
src/dev/arm/pl011.cc:            data = pkt->getUintX(ByteOrder::little);
src/dev/arm/pl011.cc:    pkt->setUintX(data, ByteOrder::little);
src/dev/arm/pl011.cc:    pkt->makeAtomicResponse();
src/dev/arm/pl011.cc:    assert(pkt->getAddr() >= pioAddr && pkt->getAddr() < pioAddr + pioSize);
src/dev/arm/pl011.cc:    assert(pkt->getSize() <= 4);
src/dev/arm/pl011.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/arm/pl011.cc:            pkt->getLE<uint8_t>(), pkt->getSize());
src/dev/arm/pl011.cc:    const uint32_t data = pkt->getUintX(ByteOrder::little);
src/dev/arm/pl011.cc:    pkt->makeAtomicResponse();
src/dev/arm/smmu_v3_proc.cc:    a.pkt->dataStatic(ptr);
src/dev/arm/smmu_v3_proc.cc:    assert(pkt->getSize() >= size);
src/dev/arm/smmu_v3_proc.cc:    a.pkt->dataStatic(ptr);
src/dev/arm/watchdog_sp805.cc:    const Addr addr = pkt->getAddr() - pioAddr;
src/dev/arm/watchdog_sp805.cc:    const size_t size = pkt->getSize();
src/dev/arm/watchdog_sp805.cc:            resp = pkt->getUintX(ByteOrder::little);
src/dev/arm/watchdog_sp805.cc:    pkt->setUintX(resp, ByteOrder::little);
src/dev/arm/watchdog_sp805.cc:    pkt->makeResponse();
src/dev/arm/watchdog_sp805.cc:    const Addr addr = pkt->getAddr() - pioAddr;
src/dev/arm/watchdog_sp805.cc:    const size_t size = pkt->getSize();
src/dev/arm/watchdog_sp805.cc:    uint64_t data = pkt->getUintX(ByteOrder::little);
src/dev/arm/watchdog_sp805.cc:    pkt->makeResponse();
src/dev/arm/pl111.cc:    assert(pkt->getAddr() >= pioAddr &&
src/dev/arm/pl111.cc:           pkt->getAddr() < pioAddr + pioSize);
src/dev/arm/pl111.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/arm/pl111.cc:    DPRINTF(PL111, " read register %#x size=%d\n", daddr, pkt->getSize());
src/dev/arm/pl111.cc:            data = pkt->getUintX(ByteOrder::little);
src/dev/arm/pl111.cc:    pkt->setUintX(data, ByteOrder::little);
src/dev/arm/pl111.cc:    pkt->makeAtomicResponse();
src/dev/arm/pl111.cc:    const uint32_t data = pkt->getUintX(ByteOrder::little);
src/dev/arm/pl111.cc:    assert(pkt->getAddr() >= pioAddr &&
src/dev/arm/pl111.cc:           pkt->getAddr() < pioAddr + pioSize);
src/dev/arm/pl111.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/arm/pl111.cc:            pkt->getLE<uint8_t>(), pkt->getSize());
src/dev/arm/pl111.cc:    pkt->makeAtomicResponse();
src/dev/arm/timer_sp804.cc:    assert(pkt->getAddr() >= pioAddr && pkt->getAddr() < pioAddr + pioSize);
src/dev/arm/timer_sp804.cc:    assert(pkt->getSize() == 4);
src/dev/arm/timer_sp804.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/arm/timer_sp804.cc:    pkt->makeAtomicResponse();
src/dev/arm/timer_sp804.cc:        pkt->setLE<uint32_t>(loadValue);
src/dev/arm/timer_sp804.cc:        pkt->setLE<uint32_t>(time);
src/dev/arm/timer_sp804.cc:        pkt->setLE<uint32_t>(control);
src/dev/arm/timer_sp804.cc:        pkt->setLE<uint32_t>(rawInt);
src/dev/arm/timer_sp804.cc:        pkt->setLE<uint32_t>(pendingInt);
src/dev/arm/timer_sp804.cc:        pkt->setLE<uint32_t>(loadValue);
src/dev/arm/timer_sp804.cc:            pkt->getLE<uint32_t>(), daddr);
src/dev/arm/timer_sp804.cc:    assert(pkt->getAddr() >= pioAddr && pkt->getAddr() < pioAddr + pioSize);
src/dev/arm/timer_sp804.cc:    assert(pkt->getSize() == 4);
src/dev/arm/timer_sp804.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/arm/timer_sp804.cc:    pkt->makeAtomicResponse();
src/dev/arm/timer_sp804.cc:            pkt->getLE<uint32_t>(), daddr);
src/dev/arm/timer_sp804.cc:        loadValue = pkt->getLE<uint32_t>();
src/dev/arm/timer_sp804.cc:        control = pkt->getLE<uint32_t>();
src/dev/arm/timer_sp804.cc:        loadValue = pkt->getLE<uint32_t>();
src/dev/arm/generic_timer.cc:    const Addr addr = pkt->getAddr();
src/dev/arm/generic_timer.cc:    const size_t size = pkt->getSize();
src/dev/arm/generic_timer.cc:    const bool is_sec = pkt->isSecure();
src/dev/arm/generic_timer.cc:    pkt->setUintX(resp, ByteOrder::little);
src/dev/arm/generic_timer.cc:    pkt->makeResponse();
src/dev/arm/generic_timer.cc:    const Addr addr = pkt->getAddr();
src/dev/arm/generic_timer.cc:    const size_t size = pkt->getSize();
src/dev/arm/generic_timer.cc:    const bool is_sec = pkt->isSecure();
src/dev/arm/generic_timer.cc:    const uint64_t data = pkt->getUintX(ByteOrder::little);
src/dev/arm/generic_timer.cc:    pkt->makeResponse();
src/dev/arm/generic_timer.cc:    const Addr addr = pkt->getAddr();
src/dev/arm/generic_timer.cc:    const size_t size = pkt->getSize();
src/dev/arm/generic_timer.cc:    const bool is_sec = pkt->isSecure();
src/dev/arm/generic_timer.cc:    pkt->setUintX(resp, ByteOrder::little);
src/dev/arm/generic_timer.cc:    pkt->makeResponse();
src/dev/arm/generic_timer.cc:    const Addr addr = pkt->getAddr();
src/dev/arm/generic_timer.cc:    const size_t size = pkt->getSize();
src/dev/arm/generic_timer.cc:    const bool is_sec = pkt->isSecure();
src/dev/arm/generic_timer.cc:    const uint64_t data = pkt->getUintX(ByteOrder::little);
src/dev/arm/generic_timer.cc:    pkt->makeResponse();
src/dev/arm/css/mhu.cc:    const Addr addr = pkt->getAddr() - pioAddr;
src/dev/arm/css/mhu.cc:    const bool secure = pkt->isSecure();
src/dev/arm/css/mhu.cc:    pkt->setUintX(value, ByteOrder::little);
src/dev/arm/css/mhu.cc:    pkt->makeAtomicResponse();
src/dev/arm/css/mhu.cc:    const Addr addr = pkt->getAddr() - pioAddr;
src/dev/arm/css/mhu.cc:    assert(pkt->getSize() == sizeof(uint32_t));
src/dev/arm/css/mhu.cc:    const uint32_t value = pkt->getLE<uint32_t>();
src/dev/arm/css/mhu.cc:    pkt->makeAtomicResponse();
src/dev/arm/ssc.cc:    assert(pkt->getAddr() >= pioAddr && pkt->getAddr() < pioAddr + pioSize);
src/dev/arm/ssc.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/arm/ssc.cc:    regBank.read(daddr, pkt->getPtr<void>(), pkt->getSize());
src/dev/arm/ssc.cc:    pkt->makeAtomicResponse();
src/dev/arm/ssc.cc:    assert(pkt->getAddr() >= pioAddr && pkt->getAddr() < pioAddr + pioSize);
src/dev/arm/ssc.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/arm/ssc.cc:    regBank.write(daddr, pkt->getPtr<void>(), pkt->getSize());
src/dev/arm/ssc.cc:    pkt->makeAtomicResponse();
src/dev/arm/kmi.cc:    assert(pkt->getAddr() >= pioAddr && pkt->getAddr() < pioAddr + pioSize);
src/dev/arm/kmi.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/arm/kmi.cc:            data = pkt->getUintX(ByteOrder::little);
src/dev/arm/kmi.cc:    pkt->setUintX(data, ByteOrder::little);
src/dev/arm/kmi.cc:    pkt->makeAtomicResponse();
src/dev/arm/kmi.cc:    assert(pkt->getAddr() >= pioAddr && pkt->getAddr() < pioAddr + pioSize);
src/dev/arm/kmi.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/arm/kmi.cc:    const uint32_t data = pkt->getUintX(ByteOrder::little);
src/dev/arm/kmi.cc:    panic_if(pkt->getSize() != 1,
src/dev/arm/kmi.cc:             daddr, data, pkt->getSize());
src/dev/arm/kmi.cc:    pkt->makeAtomicResponse();
src/dev/arm/watchdog_generic.cc:    const Addr addr = pkt->getAddr();
src/dev/arm/watchdog_generic.cc:    const size_t size = pkt->getSize();
src/dev/arm/watchdog_generic.cc:    pkt->setUintX(resp, ByteOrder::little);
src/dev/arm/watchdog_generic.cc:    pkt->makeResponse();
src/dev/arm/watchdog_generic.cc:    const Addr addr = pkt->getAddr();
src/dev/arm/watchdog_generic.cc:    const size_t size = pkt->getSize();
src/dev/arm/watchdog_generic.cc:    uint32_t data = pkt->getUintX(ByteOrder::little);
src/dev/arm/watchdog_generic.cc:    pkt->makeResponse();
src/dev/io_device.hh:        Tick receive_delay = pkt->headerDelay + pkt->payloadDelay;
src/dev/io_device.hh:        pkt->headerDelay = pkt->payloadDelay = 0;
src/dev/io_device.hh:            pkt->isRead() ? device->read(pkt) : device->write(pkt);
src/dev/io_device.hh:        assert(pkt->isResponse() || pkt->isError());
src/dev/storage/ide_ctrl.cc:    int offset = pkt->getAddr() & PCI_CONFIG_SIZE;
src/dev/storage/ide_ctrl.cc:    size_t size = pkt->getSize();
src/dev/storage/ide_ctrl.cc:    configSpaceRegs.read(offset, pkt->getPtr<void>(), size);
src/dev/storage/ide_ctrl.cc:            pkt->getUintX(ByteOrder::little));
src/dev/storage/ide_ctrl.cc:    pkt->makeAtomicResponse();
src/dev/storage/ide_ctrl.cc:    int offset = pkt->getAddr() & PCI_CONFIG_SIZE;
src/dev/storage/ide_ctrl.cc:    size_t size = pkt->getSize();
src/dev/storage/ide_ctrl.cc:            offset, size, pkt->getUintX(ByteOrder::little));
src/dev/storage/ide_ctrl.cc:    configSpaceRegs.write(offset, pkt->getConstPtr<void>(), size);
src/dev/storage/ide_ctrl.cc:    pkt->makeAtomicResponse();
src/dev/storage/ide_ctrl.cc:    if (pkt->getSize() != 1 && pkt->getSize() != 2 && pkt->getSize() !=4)
src/dev/storage/ide_ctrl.cc:         panic("Bad IDE read size: %d\n", pkt->getSize());
src/dev/storage/ide_ctrl.cc:    Addr addr = pkt->getAddr();
src/dev/storage/ide_ctrl.cc:    int size = pkt->getSize();
src/dev/storage/ide_ctrl.cc:    uint8_t *dataPtr = pkt->getPtr<uint8_t>();
src/dev/storage/ide_ctrl.cc:    if (pkt->getSize() == 1)
src/dev/storage/ide_ctrl.cc:        data = pkt->getLE<uint8_t>();
src/dev/storage/ide_ctrl.cc:    else if (pkt->getSize() == 2)
src/dev/storage/ide_ctrl.cc:        data = pkt->getLE<uint16_t>();
src/dev/storage/ide_ctrl.cc:        data = pkt->getLE<uint32_t>();
src/dev/storage/ide_ctrl.cc:            read ? "Read" : "Write", pkt->getAddr(), pkt->getSize(), data);
src/dev/storage/ide_ctrl.cc:    pkt->makeAtomicResponse();
src/dev/sparc/mm_disk.cc:    assert(pkt->getAddr() >= pioAddr && pkt->getAddr() < pioAddr + pioSize);
src/dev/sparc/mm_disk.cc:    accessAddr = pkt->getAddr() - pioAddr;
src/dev/sparc/mm_disk.cc:    switch (pkt->getSize()) {
src/dev/sparc/mm_disk.cc:        pkt->setRaw(diskData[accessAddr % SectorSize]);
src/dev/sparc/mm_disk.cc:        pkt->setRaw(d16);
src/dev/sparc/mm_disk.cc:        pkt->setRaw(d32);
src/dev/sparc/mm_disk.cc:        pkt->setRaw(d64);
src/dev/sparc/mm_disk.cc:    pkt->makeAtomicResponse();
src/dev/sparc/mm_disk.cc:    assert(pkt->getAddr() >= pioAddr && pkt->getAddr() < pioAddr + pioSize);
src/dev/sparc/mm_disk.cc:    accessAddr = pkt->getAddr() - pioAddr;
src/dev/sparc/mm_disk.cc:    switch (pkt->getSize()) {
src/dev/sparc/mm_disk.cc:        diskData[accessAddr % SectorSize] = htobe(pkt->getRaw<uint8_t>());
src/dev/sparc/mm_disk.cc:        d16 = pkt->getRaw<uint16_t>();
src/dev/sparc/mm_disk.cc:        d32 = pkt->getRaw<uint32_t>();
src/dev/sparc/mm_disk.cc:        d64 = pkt->getRaw<uint64_t>();
src/dev/sparc/mm_disk.cc:    pkt->makeAtomicResponse();
src/dev/sparc/dtod.cc:    assert(pkt->getAddr() >= pioAddr && pkt->getAddr() < pioAddr + pioSize);
src/dev/sparc/dtod.cc:    assert(pkt->getSize() == 8);
src/dev/sparc/dtod.cc:    pkt->setBE(todTime);
src/dev/sparc/dtod.cc:    pkt->makeAtomicResponse();
src/dev/sparc/iob.cc:    if (pkt->getAddr() >= iobManAddr && pkt->getAddr() < iobManAddr + iobManSize)
src/dev/sparc/iob.cc:    else if (pkt->getAddr() >= iobJBusAddr && pkt->getAddr() < iobJBusAddr+iobJBusSize)
src/dev/sparc/iob.cc:    pkt->makeAtomicResponse();
src/dev/sparc/iob.cc:        Addr accessAddr = pkt->getAddr() - iobManAddr;
src/dev/sparc/iob.cc:            pkt->setBE(data);
src/dev/sparc/iob.cc:            pkt->setBE(data);
src/dev/sparc/iob.cc:            pkt->setBE(jIntVec);
src/dev/sparc/iob.cc:        Addr accessAddr = pkt->getAddr() - iobJBusAddr;
src/dev/sparc/iob.cc:        ContextID cpuid = pkt->req->contextId();
src/dev/sparc/iob.cc:            pkt->setBE(jBusData0[index]);
src/dev/sparc/iob.cc:            pkt->setBE(jBusData1[index]);
src/dev/sparc/iob.cc:            pkt->setBE(jBusData0[cpuid]);
src/dev/sparc/iob.cc:            pkt->setBE(jBusData1[cpuid]);
src/dev/sparc/iob.cc:            pkt->setBE(data);
src/dev/sparc/iob.cc:            pkt->setBE(data);
src/dev/sparc/iob.cc:    if (pkt->getAddr() >= iobManAddr && pkt->getAddr() < iobManAddr + iobManSize)
src/dev/sparc/iob.cc:    else if (pkt->getAddr() >= iobJBusAddr && pkt->getAddr() < iobJBusAddr+iobJBusSize)
src/dev/sparc/iob.cc:    pkt->makeAtomicResponse();
src/dev/sparc/iob.cc:        Addr accessAddr = pkt->getAddr() - iobManAddr;
src/dev/sparc/iob.cc:            data = pkt->getBE<uint64_t>();
src/dev/sparc/iob.cc:            data = pkt->getBE<uint64_t>();
src/dev/sparc/iob.cc:            jIntVec = bits(pkt->getBE<uint64_t>(), 5,0);
src/dev/sparc/iob.cc:            data = pkt->getBE<uint64_t>();
src/dev/sparc/iob.cc:        Addr accessAddr = pkt->getAddr() - iobJBusAddr;
src/dev/sparc/iob.cc:        ContextID cpuid = pkt->req->contextId();
src/dev/sparc/iob.cc:            data = pkt->getBE<uint64_t>();
src/dev/sparc/iob.cc:            data = pkt->getBE<uint64_t>();
src/dev/serial/uart8250.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/serial/uart8250.cc:    registers.read(daddr, pkt->getPtr<void>(), pkt->getSize());
src/dev/serial/uart8250.cc:    pkt->makeAtomicResponse();
src/dev/serial/uart8250.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/serial/uart8250.cc:            pkt->getRaw<uint8_t>());
src/dev/serial/uart8250.cc:    registers.write(daddr, pkt->getPtr<void>(), pkt->getSize());
src/dev/serial/uart8250.cc:    pkt->makeAtomicResponse();
src/dev/serial/simple.cc:    assert(pkt->getAddr() >= pioAddr && pkt->getAddr() < pioAddr + pioSize);
src/dev/serial/simple.cc:    pkt->setUintX(data, byteOrder);
src/dev/serial/simple.cc:    pkt->makeAtomicResponse();
src/dev/serial/simple.cc:    assert(pkt->getAddr() >= pioAddr && pkt->getAddr() < pioAddr + pioSize);
src/dev/serial/simple.cc:    uint8_t data = (uint8_t)pkt->getUintX(byteOrder);
src/dev/serial/simple.cc:    pkt->makeAtomicResponse();
src/dev/riscv/vio_mmio.cc:    const Addr offset = pkt->getAddr() - pioAddr;
src/dev/riscv/vio_mmio.cc:    const unsigned size(pkt->getSize());
src/dev/riscv/vio_mmio.cc:    pkt->makeResponse();
src/dev/riscv/vio_mmio.cc:    pkt->setLE<uint32_t>(value);
src/dev/riscv/vio_mmio.cc:    const Addr offset = pkt->getAddr() - pioAddr;
src/dev/riscv/vio_mmio.cc:    const unsigned size(pkt->getSize());
src/dev/riscv/vio_mmio.cc:    DPRINTF(VirtIOMMIO, "    value: 0x%x\n", pkt->getLE<uint32_t>());
src/dev/riscv/vio_mmio.cc:    pkt->makeResponse();
src/dev/riscv/vio_mmio.cc:    write(offset, pkt->getLE<uint32_t>());
src/dev/riscv/clint.cc:    bool is_atomic = pkt->isAtomicOp() && pkt->cmd == MemCmd::SwapReq;
src/dev/riscv/clint.cc:        pkt->getAddr(), pkt->getSize(), is_atomic);
src/dev/riscv/clint.cc:    registers.read(pkt->getAddr(), pkt->getPtr<void>(), pkt->getSize());
src/dev/riscv/clint.cc:        (*(pkt->getAtomicOp()))(pkt->getPtr<uint8_t>());
src/dev/riscv/clint.cc:        pkt->makeResponse();
src/dev/riscv/clint.cc:        pkt->getAddr(), pkt->getSize());
src/dev/riscv/clint.cc:    registers.write(pkt->getAddr(), pkt->getPtr<void>(), pkt->getSize());
src/dev/riscv/clint.cc:    pkt->makeResponse();
src/dev/riscv/plic.cc:    bool is_atomic = pkt->isAtomicOp() && pkt->cmd == MemCmd::SwapReq;
src/dev/riscv/plic.cc:        pkt->getAddr(), pkt->getSize(), is_atomic);
src/dev/riscv/plic.cc:    registers.read(pkt->getAddr(), pkt->getPtr<void>(), pkt->getSize());
src/dev/riscv/plic.cc:        (*(pkt->getAtomicOp()))(pkt->getPtr<uint8_t>());
src/dev/riscv/plic.cc:        pkt->makeResponse();
src/dev/riscv/plic.cc:        pkt->getAddr(), pkt->getSize());
src/dev/riscv/plic.cc:    registers.write(pkt->getAddr(), pkt->getPtr<void>(), pkt->getSize());
src/dev/riscv/plic.cc:    pkt->makeResponse();
src/dev/pci/device.cc:    int offset = pkt->getAddr() & PCI_CONFIG_SIZE;
src/dev/pci/device.cc:        switch (pkt->getSize()) {
src/dev/pci/device.cc:                pkt->setLE<uint8_t>(0);
src/dev/pci/device.cc:                pkt->setLE<uint16_t>(0);
src/dev/pci/device.cc:                pkt->setLE<uint32_t>(0);
src/dev/pci/device.cc:    switch (pkt->getSize()) {
src/dev/pci/device.cc:        pkt->setLE<uint8_t>(config.data[offset]);
src/dev/pci/device.cc:            (uint32_t)pkt->getLE<uint8_t>());
src/dev/pci/device.cc:        pkt->setLE<uint16_t>(*(uint16_t*)&config.data[offset]);
src/dev/pci/device.cc:            (uint32_t)pkt->getLE<uint16_t>());
src/dev/pci/device.cc:        pkt->setLE<uint32_t>(*(uint32_t*)&config.data[offset]);
src/dev/pci/device.cc:            (uint32_t)pkt->getLE<uint32_t>());
src/dev/pci/device.cc:    pkt->makeAtomicResponse();
src/dev/pci/device.cc:    int offset = pkt->getAddr() & PCI_CONFIG_SIZE;
src/dev/pci/device.cc:        switch (pkt->getSize()) {
src/dev/pci/device.cc:    switch (pkt->getSize()) {
src/dev/pci/device.cc:            config.interruptLine = pkt->getLE<uint8_t>();
src/dev/pci/device.cc:            config.cacheLineSize = pkt->getLE<uint8_t>();
src/dev/pci/device.cc:            config.latencyTimer = pkt->getLE<uint8_t>();
src/dev/pci/device.cc:            (uint32_t)pkt->getLE<uint8_t>());
src/dev/pci/device.cc:            config.command = pkt->getLE<uint8_t>();
src/dev/pci/device.cc:            config.status = pkt->getLE<uint8_t>();
src/dev/pci/device.cc:            config.cacheLineSize = pkt->getLE<uint8_t>();
src/dev/pci/device.cc:            (uint32_t)pkt->getLE<uint16_t>());
src/dev/pci/device.cc:                    htole(bar->write(hostInterface, pkt->getLE<uint32_t>()));
src/dev/pci/device.cc:            if (letoh(pkt->getLE<uint32_t>()) == 0xfffffffe)
src/dev/pci/device.cc:                config.expansionROM = pkt->getLE<uint32_t>();
src/dev/pci/device.cc:            config.command = pkt->getLE<uint32_t>();
src/dev/pci/device.cc:            (uint32_t)pkt->getLE<uint32_t>());
src/dev/pci/device.cc:    pkt->makeAtomicResponse();
src/dev/pci/host.cc:    const auto dev_addr(decodeAddress(pkt->getAddr() - confBase));
src/dev/pci/host.cc:    const Addr size(pkt->getSize());
src/dev/pci/host.cc:        pkt->headerDelay = pkt->payloadDelay = 0;
src/dev/pci/host.cc:        uint8_t *pkt_data(pkt->getPtr<uint8_t>());
src/dev/pci/host.cc:        pkt->makeAtomicResponse();
src/dev/pci/host.cc:    const auto dev_addr(decodeAddress(pkt->getAddr() - confBase));
src/dev/pci/host.cc:            pkt->getSize());
src/dev/pci/host.cc:    pkt->headerDelay = pkt->payloadDelay = 0;
src/dev/pci/copy_engine.cc:    if (!getBAR(pkt->getAddr(), bar, daddr))
src/dev/pci/copy_engine.cc:    int size = pkt->getSize();
src/dev/pci/copy_engine.cc:        panic("Unknown size for MMIO access: %d\n", pkt->getSize());
src/dev/pci/copy_engine.cc:            pkt->setLE<uint8_t>(regs.chanCount);
src/dev/pci/copy_engine.cc:            pkt->setLE<uint8_t>(regs.xferCap);
src/dev/pci/copy_engine.cc:            pkt->setLE<uint8_t>(regs.intrctrl());
src/dev/pci/copy_engine.cc:            pkt->setLE<uint32_t>(regs.attnStatus);
src/dev/pci/copy_engine.cc:        pkt->makeAtomicResponse();
src/dev/pci/copy_engine.cc:    pkt->makeAtomicResponse();
src/dev/pci/copy_engine.cc:        pkt->setLE<uint16_t>(cr.ctrl());
src/dev/pci/copy_engine.cc:        pkt->setLE<uint64_t>(cr.status() | (busy ? 0 : 1));
src/dev/pci/copy_engine.cc:            pkt->setLE<uint64_t>(cr.descChainAddr);
src/dev/pci/copy_engine.cc:            pkt->setLE<uint32_t>(bits(cr.descChainAddr,0,31));
src/dev/pci/copy_engine.cc:        pkt->setLE<uint32_t>(bits(cr.descChainAddr,32,63));
src/dev/pci/copy_engine.cc:        pkt->setLE<uint32_t>(cr.command());
src/dev/pci/copy_engine.cc:            pkt->setLE<uint64_t>(cr.completionAddr);
src/dev/pci/copy_engine.cc:            pkt->setLE<uint32_t>(bits(cr.completionAddr,0,31));
src/dev/pci/copy_engine.cc:        pkt->setLE<uint32_t>(bits(cr.completionAddr,32,63));
src/dev/pci/copy_engine.cc:        pkt->setLE<uint32_t>(cr.error());
src/dev/pci/copy_engine.cc:    if (!getBAR(pkt->getAddr(), bar, daddr))
src/dev/pci/copy_engine.cc:    int size = pkt->getSize();
src/dev/pci/copy_engine.cc:        [[maybe_unused]] uint64_t val = pkt->getLE<uint64_t>();
src/dev/pci/copy_engine.cc:        [[maybe_unused]] uint32_t val = pkt->getLE<uint32_t>();
src/dev/pci/copy_engine.cc:        [[maybe_unused]] uint16_t val = pkt->getLE<uint16_t>();
src/dev/pci/copy_engine.cc:        [[maybe_unused]] uint8_t val = pkt->getLE<uint8_t>();
src/dev/pci/copy_engine.cc:            regs.intrctrl.master_int_enable(bits(pkt->getLE<uint8_t>(), 0, 1));
src/dev/pci/copy_engine.cc:        pkt->makeAtomicResponse();
src/dev/pci/copy_engine.cc:    pkt->makeAtomicResponse();
src/dev/pci/copy_engine.cc:        cr.ctrl(pkt->getLE<uint16_t>());
src/dev/pci/copy_engine.cc:            cr.descChainAddr = pkt->getLE<uint64_t>();
src/dev/pci/copy_engine.cc:            cr.descChainAddr =  (uint64_t)pkt->getLE<uint32_t>() |
src/dev/pci/copy_engine.cc:        cr.descChainAddr =  ((uint64_t)pkt->getLE<uint32_t>() << 32) |
src/dev/pci/copy_engine.cc:        cr.command(pkt->getLE<uint8_t>());
src/dev/pci/copy_engine.cc:            cr.completionAddr = pkt->getLE<uint64_t>();
src/dev/pci/copy_engine.cc:            cr.completionAddr =  pkt->getLE<uint32_t>() |
src/dev/pci/copy_engine.cc:        cr.completionAddr =  ((uint64_t)pkt->getLE<uint32_t>() <<32) |
src/dev/pci/copy_engine.cc:        cr.error(~pkt->getLE<uint32_t>() & cr.error());
src/dev/hsa/hsa_packet_processor.cc:    assert(pkt->getAddr() >= pioAddr && pkt->getAddr() < pioAddr + pioSize);
src/dev/hsa/hsa_packet_processor.cc:    [[maybe_unused]] Addr daddr = pkt->getAddr() - pioAddr;
src/dev/hsa/hsa_packet_processor.cc:          __FUNCTION__, pkt->getSize(), daddr, daddr);
src/dev/hsa/hsa_packet_processor.cc:    assert(gpu_device->driver()->doorbellSize() == pkt->getSize());
src/dev/hsa/hsa_packet_processor.cc:    if (pkt->getSize() == 8)
src/dev/hsa/hsa_packet_processor.cc:        doorbell_reg = pkt->getLE<uint64_t>() + 1;
src/dev/hsa/hsa_packet_processor.cc:    else if (pkt->getSize() == 4)
src/dev/hsa/hsa_packet_processor.cc:        doorbell_reg = pkt->getLE<uint32_t>();
src/dev/hsa/hsa_packet_processor.cc:    pkt->makeAtomicResponse();
src/dev/hsa/hsa_packet_processor.cc:    pkt->makeAtomicResponse();
src/dev/hsa/hsa_packet_processor.cc:    pkt->setBadAddress();
src/dev/hsa/hsa_packet_processor.cc:            if (bar_and_pkt->dep_signal[i]) {
src/dev/hsa/hsa_packet_processor.cc:                    (uint64_t) (((uint64_t *) bar_and_pkt->dep_signal[i]) + 1);
src/dev/hsa/hsa_packet_processor.cc:            if (bar_and_pkt->completion_signal != 0) {
src/dev/hsa/hsa_packet_processor.cc:                       bar_and_pkt->completion_signal);
src/dev/hsa/hsa_packet_processor.cc:                    bar_and_pkt->completion_signal);
src/dev/hsa/hsa_packet_processor.cc:            (uint64_t) (((uint64_t *)agent_pkt->completion_signal) + 1);
src/dev/virtio/rng 2.cc:    pkt->makeResponse();
src/dev/virtio/rng.cc:    pkt->makeResponse();
src/dev/virtio/base.cc:    const unsigned size(pkt->getSize());
src/dev/virtio/base.cc:    pkt->makeResponse();
src/dev/virtio/base.cc:    pkt->setData(const_cast<uint8_t *>(cfg) + cfgOffset);
src/dev/virtio/base.cc:    const unsigned size(pkt->getSize());
src/dev/virtio/base.cc:    pkt->makeResponse();
src/dev/virtio/base.cc:    pkt->writeData((uint8_t *)cfg + cfgOffset);
src/dev/virtio/pci.cc:    [[maybe_unused]] const unsigned size(pkt->getSize());
src/dev/virtio/pci.cc:    if (!getBAR(pkt->getAddr(), bar, offset))
src/dev/virtio/pci.cc:    pkt->makeResponse();
src/dev/virtio/pci.cc:        pkt->setLE<uint32_t>(vio.deviceFeatures);
src/dev/virtio/pci.cc:        pkt->setLE<uint32_t>(vio.getGuestFeatures());
src/dev/virtio/pci.cc:        pkt->setLE<uint32_t>(vio.getQueueAddress());
src/dev/virtio/pci.cc:        pkt->setLE<uint16_t>(vio.getQueueSize());
src/dev/virtio/pci.cc:        pkt->setLE<uint16_t>(vio.getQueueSelect());
src/dev/virtio/pci.cc:        pkt->setLE<uint16_t>(queueNotify);
src/dev/virtio/pci.cc:        pkt->setLE<uint8_t>(vio.getDeviceStatus());
src/dev/virtio/pci.cc:          pkt->setLE<uint8_t>(isr_status);
src/dev/virtio/pci.cc:    [[maybe_unused]] const unsigned size(pkt->getSize());
src/dev/virtio/pci.cc:    if (!getBAR(pkt->getAddr(), bar, offset))
src/dev/virtio/pci.cc:    pkt->makeResponse();
src/dev/virtio/pci.cc:        vio.setGuestFeatures(pkt->getLE<uint32_t>());
src/dev/virtio/pci.cc:        vio.setQueueAddress(pkt->getLE<uint32_t>());
src/dev/virtio/pci.cc:        vio.setQueueSelect(pkt->getLE<uint16_t>());
src/dev/virtio/pci.cc:        queueNotify = pkt->getLE<uint16_t>();
src/dev/virtio/pci.cc:          uint8_t status(pkt->getLE<uint8_t>());
src/dev/qemu/fw_cfg.cc:    const Addr addr = pkt->getAddr();
src/dev/qemu/fw_cfg.cc:    const auto size = pkt->getSize();
src/dev/qemu/fw_cfg.cc:    pkt->makeResponse();
src/dev/qemu/fw_cfg.cc:    std::memset(pkt->getPtr<uint8_t>(), 0, size);
src/dev/qemu/fw_cfg.cc:            readItem(pkt->getPtr<void>(), size);
src/dev/qemu/fw_cfg.cc:    const Addr addr = pkt->getAddr();
src/dev/qemu/fw_cfg.cc:    const auto size = pkt->getSize();
src/dev/qemu/fw_cfg.cc:    pkt->makeResponse();
src/dev/qemu/fw_cfg.cc:            auto key = pkt->getLE<uint16_t>();
src/dev/qemu/fw_cfg.cc:    const Addr addr = pkt->getAddr();
src/dev/qemu/fw_cfg.cc:    const auto size = pkt->getSize();
src/dev/qemu/fw_cfg.cc:    pkt->makeResponse();
src/dev/qemu/fw_cfg.cc:    std::memset(pkt->getPtr<uint8_t>(), 0, size);
src/dev/qemu/fw_cfg.cc:            readItem(pkt->getPtr<void>(), size);
src/dev/qemu/fw_cfg.cc:    const Addr addr = pkt->getAddr();
src/dev/qemu/fw_cfg.cc:    const auto size = pkt->getSize();
src/dev/qemu/fw_cfg.cc:    pkt->makeResponse();
src/dev/qemu/fw_cfg.cc:            auto key = pkt->getBE<uint16_t>();
src/dev/isa_fake.cc:    pkt->makeAtomicResponse();
src/dev/isa_fake.cc:                name(), pkt->getAddr(), pkt->getSize());
src/dev/isa_fake.cc:                pkt->getAddr(), pkt->getSize());
src/dev/isa_fake.cc:        pkt->setBadAddress();
src/dev/isa_fake.cc:        assert(pkt->getAddr() >= pioAddr && pkt->getAddr() < pioAddr + pioSize);
src/dev/isa_fake.cc:                pkt->getAddr(), pkt->getSize());
src/dev/isa_fake.cc:        switch (pkt->getSize()) {
src/dev/isa_fake.cc:             pkt->setLE(retData64);
src/dev/isa_fake.cc:             pkt->setLE(retData32);
src/dev/isa_fake.cc:             pkt->setLE(retData16);
src/dev/isa_fake.cc:             pkt->setLE(retData8);
src/dev/isa_fake.cc:                 std::memset(pkt->getPtr<uint8_t>(), 0, pkt->getSize());
src/dev/isa_fake.cc:    pkt->makeAtomicResponse();
src/dev/isa_fake.cc:        switch (pkt->getSize()) {
src/dev/isa_fake.cc:            data = pkt->getLE<uint64_t>();
src/dev/isa_fake.cc:            data = pkt->getLE<uint32_t>();
src/dev/isa_fake.cc:            data = pkt->getLE<uint16_t>();
src/dev/isa_fake.cc:            data = pkt->getLE<uint8_t>();
src/dev/isa_fake.cc:            panic("invalid access size: %u\n", pkt->getSize());
src/dev/isa_fake.cc:                name(), pkt->getAddr(), pkt->getSize(), data);
src/dev/isa_fake.cc:                pkt->getAddr(), pkt->getSize());
src/dev/isa_fake.cc:        pkt->setBadAddress();
src/dev/isa_fake.cc:                pkt->getAddr(), pkt->getSize());
src/dev/isa_fake.cc:            switch (pkt->getSize()) {
src/dev/isa_fake.cc:                retData64 = pkt->getLE<uint64_t>();
src/dev/isa_fake.cc:                retData32 = pkt->getLE<uint32_t>();
src/dev/isa_fake.cc:                retData16 = pkt->getLE<uint16_t>();
src/dev/isa_fake.cc:                retData8 = pkt->getLE<uint8_t>();
src/dev/dma_device.cc:    assert(pkt->isResponse());
src/dev/dma_device.cc:    warn_if(pkt->isError(), "Response pkt error.");
src/dev/dma_device.cc:    auto *state = dynamic_cast<DmaReqState*>(pkt->senderState);
src/dev/dma_device.cc:    handleResp(state, pkt->getAddr(), pkt->req->getSize(), delay);
src/dev/dma_device.cc:        pkt->dataStatic(data + gen.complete());
src/dev/dma_device.cc:    pkt->senderState = this;
src/dev/dma_device.cc:    assert(pkt->req->isUncacheable() ||
src/dev/dma_device.cc:           !(pkt->cacheResponding() && !pkt->hasSharers()));
src/dev/dma_device.cc:    DPRINTF(DMA, "Trying to send %s addr %#x\n", pkt->cmdString(),
src/dev/dma_device.cc:            pkt->getAddr());
src/dev/i2c/bus.cc:    assert(pkt->getAddr() == pioAddr + SB_CONTROLS);
src/dev/i2c/bus.cc:    pkt->setRaw<uint8_t>((sda << 1) | scl);
src/dev/i2c/bus.cc:    pkt->makeAtomicResponse();
src/dev/i2c/bus.cc:    assert(pkt->getAddr() == pioAddr + SB_CONTROLS ||
src/dev/i2c/bus.cc:           pkt->getAddr() == pioAddr + SB_CONTROLC);
src/dev/i2c/bus.cc:    uint8_t msg = pkt->getRaw<uint8_t>();
src/dev/i2c/bus.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/i2c/bus.cc:    uint8_t msg = pkt->getRaw<uint8_t>();
src/dev/i2c/bus.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/i2c/bus.cc:    uint8_t msg = pkt->getRaw<uint8_t>();
src/dev/i2c/bus.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/i2c/bus.cc:    uint8_t msg = pkt->getRaw<uint8_t>();
src/dev/i2c/bus.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/net/i8254xGBe.cc:    int offset = pkt->getAddr() & PCI_CONFIG_SIZE;
src/dev/net/i8254xGBe.cc:    if (!getBAR(pkt->getAddr(), bar, daddr))
src/dev/net/i8254xGBe.cc:    assert(pkt->getSize() == 4);
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.ctrl());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.sts());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.eecd());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.eerd());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.ctrl_ext());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.mdic());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.icr());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(0);
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.itr());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.rctl());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.fcttv());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.tctl());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.pba());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(0); // We don't care, so just return 0
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.fcrtl());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.fcrth());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.rdba.rdbal());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.rdba.rdbah());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.rdlen());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.srrctl());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.rdh());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.rdt());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.rdtr());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.rxdctl());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.radv());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.tdba.tdbal());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.tdba.tdbah());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.tdlen());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.tdh());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.txdca_ctl());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.tdt());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.tidv());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.txdctl());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.tadv());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.tdwba & mask(32));
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.tdwba >> 32);
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.rxcsum());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.rlpml);
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.rfctl());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.manc());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.swsm());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.fwsm());
src/dev/net/i8254xGBe.cc:        pkt->setLE<uint32_t>(regs.sw_fw_sync);
src/dev/net/i8254xGBe.cc:            pkt->setLE<uint32_t>(0);
src/dev/net/i8254xGBe.cc:    pkt->makeAtomicResponse();
src/dev/net/i8254xGBe.cc:    if (!getBAR(pkt->getAddr(), bar, daddr))
src/dev/net/i8254xGBe.cc:    assert(pkt->getSize() == sizeof(uint32_t));
src/dev/net/i8254xGBe.cc:            daddr, pkt->getLE<uint32_t>());
src/dev/net/i8254xGBe.cc:    uint32_t val = pkt->getLE<uint32_t>();
src/dev/net/i8254xGBe.cc:    pkt->makeAtomicResponse();
src/dev/net/i8254xGBe.cc:    etherDeviceStats.rxBytes += pkt->length;
src/dev/net/i8254xGBe.cc:    if (pktOffset == pkt->length) {
src/dev/net/dist_etherlink.cc:    Tick delay = (Tick)ceil(((double)pkt->simLength * ticksPerByte) + 1.0);
src/dev/net/pktfifo.cc:        while (offset >= pkt->length) {
src/dev/net/pktfifo.cc:            offset -= pkt->length;
src/dev/net/pktfifo.cc:        unsigned size = std::min(pkt->length - offset, len);
src/dev/net/pktfifo.cc:        memcpy(data, pkt->data, size);
src/dev/net/etherbus.cc:    DPRINTF(Ethernet, "ethernet packet sent: length=%d\n", pkt->length);
src/dev/net/etherbus.cc:    DDUMP(EthernetData, pkt->data, pkt->length);
src/dev/net/etherbus.cc:    int delay = (int)ceil(((double)pkt->simLength * ticksPerByte) + 1.0);
src/dev/net/ns_gige.cc:    int offset = pkt->getAddr() & PCI_CONFIG_SIZE;
src/dev/net/ns_gige.cc:    Addr daddr = pkt->getAddr() & 0xfff;
src/dev/net/ns_gige.cc:            daddr, pkt->getAddr(), pkt->getSize());
src/dev/net/ns_gige.cc:        pkt->setLE<uint32_t>(0);
src/dev/net/ns_gige.cc:        pkt->makeAtomicResponse();
src/dev/net/ns_gige.cc:    assert(pkt->getSize() == sizeof(uint32_t));
src/dev/net/ns_gige.cc:        uint32_t &reg = *pkt->getPtr<uint32_t>();
src/dev/net/ns_gige.cc:    pkt->makeAtomicResponse();
src/dev/net/ns_gige.cc:    Addr daddr = pkt->getAddr() & 0xfff;
src/dev/net/ns_gige.cc:            daddr, pkt->getAddr(), pkt->getSize());
src/dev/net/ns_gige.cc:    if (pkt->getSize() == sizeof(uint32_t)) {
src/dev/net/ns_gige.cc:        uint32_t reg = pkt->getLE<uint32_t>();
src/dev/net/ns_gige.cc:    pkt->makeAtomicResponse();
src/dev/net/sinic.cc:    Addr daddr = pkt->getAddr();
src/dev/net/sinic.cc:    ContextID cpu = pkt->req->contextId();
src/dev/net/sinic.cc:              cpu, index, daddr, pkt->getAddr(), pkt->getSize());
src/dev/net/sinic.cc:              info.name, cpu, index, daddr, pkt->getAddr(), pkt->getSize());
src/dev/net/sinic.cc:    if (info.size != pkt->getSize()) {
src/dev/net/sinic.cc:              info.name, cpu, index, daddr, pkt->getAddr(), pkt->getSize());
src/dev/net/sinic.cc:    if (pkt->getSize() == 4) {
src/dev/net/sinic.cc:        pkt->setLE(reg);
src/dev/net/sinic.cc:    if (pkt->getSize() == 8) {
src/dev/net/sinic.cc:        pkt->setLE(reg);
src/dev/net/sinic.cc:            info.name, cpu, index, daddr, pkt->getAddr(), pkt->getSize(), value);
src/dev/net/sinic.cc:    Addr daddr = pkt->getAddr();
src/dev/net/sinic.cc:    ContextID cpu = pkt->req->contextId();
src/dev/net/sinic.cc:                cpu, daddr, pkt->getAddr(), pkt->getSize());
src/dev/net/sinic.cc:              info.name, cpu, index, daddr, pkt->getAddr(), pkt->getSize());
src/dev/net/sinic.cc:    if (pkt->getSize() != info.size)
src/dev/net/sinic.cc:              info.name, cpu, index, daddr, pkt->getAddr(), pkt->getSize());
src/dev/net/sinic.cc:            pkt->getLE<uint32_t>() : pkt->getLE<uint64_t>(),
src/dev/net/sinic.cc:            daddr, pkt->getAddr(), pkt->getSize());
src/dev/net/sinic.cc:        changeConfig(pkt->getLE<uint32_t>());
src/dev/net/sinic.cc:        command(pkt->getLE<uint32_t>());
src/dev/net/sinic.cc:                pkt->getLE<uint32_t>());
src/dev/net/sinic.cc:        devIntrChangeMask(pkt->getLE<uint32_t>());
src/dev/net/sinic.cc:        vnic.RxData = pkt->getLE<uint64_t>();
src/dev/net/sinic.cc:        if (registers::get_RxData_Vaddr(pkt->getLE<uint64_t>())) {
src/dev/net/sinic.cc:        if (registers::get_TxData_Vaddr(pkt->getLE<uint64_t>())) {
src/dev/net/etherlink.cc:    DPRINTF(Ethernet, "packet sent: len=%d\n", pkt->length);
src/dev/net/etherlink.cc:    DDUMP(EthernetData, pkt->data, pkt->length);
src/dev/net/etherlink.cc:    Tick delay = (Tick)ceil(((double)pkt->simLength * ticksPerByte) + 1.0);
src/dev/net/dist_iface.cc:    header.dataPacketLength = pkt->length;
src/dev/net/dist_iface.cc:    header.simLength = pkt->simLength;
src/dev/net/dist_iface.cc:            pkt->length, send_delay);
src/dev/amdgpu/sdma_engine.cc:    pkt->count++;
src/dev/amdgpu/sdma_engine.cc:    DPRINTF(SDMAEngine, "Write %d dwords to %lx\n", pkt->count, pkt->dest);
src/dev/amdgpu/sdma_engine.cc:    uint32_t *dmaBuffer = new uint32_t[pkt->count];
src/dev/amdgpu/sdma_engine.cc:    dmaReadVirt(q->rptr(), sizeof(uint32_t) * pkt->count, cb,
src/dev/amdgpu/sdma_engine.cc:    int bufferSize = sizeof(uint32_t) * pkt->count;
src/dev/amdgpu/sdma_engine.cc:    for (int i = 0; i < pkt->count; ++i) {
src/dev/amdgpu/sdma_engine.cc:    if (gpuDevice->getVM().inMMHUB(pkt->dest)) {
src/dev/amdgpu/sdma_engine.cc:        Addr mmhubAddr = pkt->dest - gpuDevice->getVM().getMMHUBBase();
src/dev/amdgpu/sdma_engine.cc:        pkt->dest = getGARTAddr(pkt->dest);
src/dev/amdgpu/sdma_engine.cc:        dmaWriteVirt(pkt->dest, bufferSize, cb, (void *)dmaBuffer);
src/dev/amdgpu/sdma_engine.cc:            pkt->dest, pkt->count);
src/dev/amdgpu/sdma_engine.cc:            pkt->source, pkt->dest, pkt->count);
src/dev/amdgpu/sdma_engine.cc:    pkt->count++;
src/dev/amdgpu/sdma_engine.cc:    DPRINTF(SDMAEngine, "Getting GART addr for %lx\n", pkt->source);
src/dev/amdgpu/sdma_engine.cc:    pkt->source = getGARTAddr(pkt->source);
src/dev/amdgpu/sdma_engine.cc:    DPRINTF(SDMAEngine, "GART addr %lx\n", pkt->source);
src/dev/amdgpu/sdma_engine.cc:    uint8_t *dmaBuffer = new uint8_t[pkt->count];
src/dev/amdgpu/sdma_engine.cc:    Addr device_addr = getDeviceAddress(pkt->source);
src/dev/amdgpu/sdma_engine.cc:        ChunkGenerator gen(pkt->source, pkt->count, AMDGPU_MMHUB_PAGE_SIZE);
src/dev/amdgpu/sdma_engine.cc:        dmaReadVirt(pkt->source, pkt->count, cb, (void *)dmaBuffer);
src/dev/amdgpu/sdma_engine.cc:    DPRINTF(SDMAEngine, "Last:  %016lx\n", dmaBuffer64[(pkt->count/8)-1]);
src/dev/amdgpu/sdma_engine.cc:    for (int i = 0; i < pkt->count/8; ++i) {
src/dev/amdgpu/sdma_engine.cc:    Addr device_addr = getDeviceAddress(pkt->dest);
src/dev/amdgpu/sdma_engine.cc:        ChunkGenerator gen(pkt->dest, pkt->count, AMDGPU_MMHUB_PAGE_SIZE);
src/dev/amdgpu/sdma_engine.cc:        dmaWriteVirt(pkt->dest, pkt->count, cb, (void *)dmaBuffer);
src/dev/amdgpu/sdma_engine.cc:            pkt->dest, pkt->count);
src/dev/amdgpu/sdma_engine.cc:    q->ib()->base(getGARTAddr(pkt->base));
src/dev/amdgpu/sdma_engine.cc:    q->ib()->size(pkt->size * sizeof(uint32_t) + 1);
src/dev/amdgpu/sdma_engine.cc:    q->ib()->setWptr(pkt->size * sizeof(uint32_t));
src/dev/amdgpu/sdma_engine.cc:    pkt->dest = getGARTAddr(pkt->dest);
src/dev/amdgpu/sdma_engine.cc:        [ = ] (const uint32_t &) { fenceDone(q, pkt); }, pkt->data);
src/dev/amdgpu/sdma_engine.cc:    dmaWriteVirt(pkt->dest, sizeof(pkt->data), cb, &cb->dmaBuffer);
src/dev/amdgpu/sdma_engine.cc:            pkt->dest, pkt->data);
src/dev/amdgpu/sdma_engine.cc:    DPRINTF(SDMAEngine, "Trap contextId: %p\n", pkt->intrContext);
src/dev/amdgpu/sdma_engine.cc:    gpuDevice->getIH()->prepareInterruptCookie(pkt->intrContext, ring_id,
src/dev/amdgpu/sdma_engine.cc:    [[maybe_unused]] uint32_t reg_addr = pkt->regAddr << 2;
src/dev/amdgpu/sdma_engine.cc:    pkt->data &= reg_mask;
src/dev/amdgpu/sdma_engine.cc:            reg_addr, pkt->data);
src/dev/amdgpu/sdma_engine.cc:            header->op, pkt->address, pkt->ref, pkt->mask, pkt->retryCount,
src/dev/amdgpu/sdma_engine.cc:            pkt->pollInt);
src/dev/amdgpu/sdma_engine.cc:            dmaReadVirt(pkt->address, sizeof(uint32_t), cb,
src/dev/amdgpu/sdma_engine.cc:    if (!pollRegMemFunc(dma_buffer, pkt->ref, header->func) &&
src/dev/amdgpu/sdma_engine.cc:        ((count < (pkt->retryCount + 1) && pkt->retryCount != 0xfff) ||
src/dev/amdgpu/sdma_engine.cc:         pkt->retryCount == 0xfff)) {
src/dev/amdgpu/sdma_engine.cc:                pkt->address, dma_buffer, pkt->ref);
src/dev/amdgpu/sdma_engine.cc:        dmaReadVirt(pkt->address, sizeof(uint32_t), cb,
src/dev/amdgpu/sdma_engine.cc:                pkt->address, dma_buffer, pkt->ref);
src/dev/amdgpu/sdma_engine.cc:    pkt->count++;
src/dev/amdgpu/sdma_engine.cc:            pkt->initValue, pkt->increment, pkt->count);
src/dev/amdgpu/sdma_engine.cc:    // Generating pkt->count double dwords using the initial value, increment
src/dev/amdgpu/sdma_engine.cc:    uint64_t *dmaBuffer = new uint64_t[pkt->count];
src/dev/amdgpu/sdma_engine.cc:    for (int i = 0; i < pkt->count; i++) {
src/dev/amdgpu/sdma_engine.cc:        dmaBuffer[i] = (pkt->mask | (pkt->initValue + (i * pkt->increment)));
src/dev/amdgpu/sdma_engine.cc:    if (gpuDevice->getVM().inMMHUB(pkt->dest)) {
src/dev/amdgpu/sdma_engine.cc:        Addr mmhubAddr = pkt->dest - gpuDevice->getVM().getMMHUBBase();
src/dev/amdgpu/sdma_engine.cc:                                             sizeof(uint64_t) * pkt->count, 0,
src/dev/amdgpu/sdma_engine.cc:        dmaWriteVirt(pkt->dest, sizeof(uint64_t) * pkt->count, cb,
src/dev/amdgpu/sdma_engine.cc:            pkt->dest, pkt->count);
src/dev/amdgpu/sdma_engine.cc:            " %d loopInt: %d\n", header->opcode, pkt->addr, pkt->srcData,
src/dev/amdgpu/sdma_engine.cc:            pkt->cmpData, header->loop, pkt->loopInt);
src/dev/amdgpu/sdma_engine.cc:    // Read the data at pkt->addr
src/dev/amdgpu/sdma_engine.cc:    dmaReadVirt(pkt->addr, sizeof(uint64_t), cb, (void *)dmaBuffer);
src/dev/amdgpu/sdma_engine.cc:            header->opcode, pkt->addr, *dmaBuffer);
src/dev/amdgpu/sdma_engine.cc:        int64_t src_data = pkt->srcData;
src/dev/amdgpu/sdma_engine.cc:        dmaWriteVirt(pkt->addr, sizeof(uint64_t), cb, (void *)dmaBuffer);
src/dev/amdgpu/sdma_engine.cc:            header->opcode, pkt->addr, *dmaBuffer);
src/dev/amdgpu/sdma_engine.cc:            pkt->addr, pkt->srcData, pkt->count, fill_header.fillsize,
src/dev/amdgpu/sdma_engine.cc:    int fill_bytes = (pkt->count + 1) * (1 << fill_header.fillsize);
src/dev/amdgpu/sdma_engine.cc:    memset(fill_data, pkt->srcData, fill_bytes);
src/dev/amdgpu/sdma_engine.cc:    Addr device_addr = getDeviceAddress(pkt->addr);
src/dev/amdgpu/sdma_engine.cc:                fill_bytes, pkt->srcData, pkt->addr);
src/dev/amdgpu/sdma_engine.cc:        ChunkGenerator gen(pkt->addr, fill_bytes, AMDGPU_MMHUB_PAGE_SIZE);
src/dev/amdgpu/sdma_engine.cc:                fill_bytes, pkt->srcData, pkt->addr);
src/dev/amdgpu/sdma_engine.cc:        dmaWriteVirt(pkt->addr, fill_bytes, cb, (void *)fill_data);
src/dev/amdgpu/sdma_engine.cc:    DPRINTF(SDMAEngine, "ConstFill to %lx done\n", pkt->addr);
src/dev/amdgpu/sdma_engine.cc:            pkt->getLE<uint32_t>());
src/dev/amdgpu/sdma_engine.cc:        setGfxBaseLo(pkt->getLE<uint32_t>());
src/dev/amdgpu/sdma_engine.cc:        setGfxBaseHi(pkt->getLE<uint32_t>());
src/dev/amdgpu/sdma_engine.cc:        setGfxRptrLo(pkt->getLE<uint32_t>());
src/dev/amdgpu/sdma_engine.cc:        setGfxRptrHi(pkt->getLE<uint32_t>());
src/dev/amdgpu/sdma_engine.cc:        setGfxDoorbellLo(pkt->getLE<uint32_t>());
src/dev/amdgpu/sdma_engine.cc:        setGfxDoorbellOffsetLo(pkt->getLE<uint32_t>());
src/dev/amdgpu/sdma_engine.cc:        uint32_t rb_size = bits(pkt->getLE<uint32_t>(), 6, 1);
src/dev/amdgpu/sdma_engine.cc:        setGfxWptrLo(pkt->getLE<uint32_t>());
src/dev/amdgpu/sdma_engine.cc:        setGfxWptrHi(pkt->getLE<uint32_t>());
src/dev/amdgpu/sdma_engine.cc:        setPageBaseLo(pkt->getLE<uint32_t>());
src/dev/amdgpu/sdma_engine.cc:        setPageRptrLo(pkt->getLE<uint32_t>());
src/dev/amdgpu/sdma_engine.cc:        setPageRptrHi(pkt->getLE<uint32_t>());
src/dev/amdgpu/sdma_engine.cc:        setPageDoorbellLo(pkt->getLE<uint32_t>());
src/dev/amdgpu/sdma_engine.cc:        setPageDoorbellOffsetLo(pkt->getLE<uint32_t>());
src/dev/amdgpu/sdma_engine.cc:        uint32_t rb_size = bits(pkt->getLE<uint32_t>(), 6, 1);
src/dev/amdgpu/sdma_engine.cc:        setPageWptrLo(pkt->getLE<uint32_t>());
src/dev/amdgpu/amdgpu_nbio.cc:          uint32_t value = pkt->getLE<uint32_t>() | 0x1;
src/dev/amdgpu/amdgpu_nbio.cc:          pkt->setLE<uint32_t>(value);
src/dev/amdgpu/amdgpu_nbio.cc:        //pkt->setLE<uint32_t>(regs[mm_index_reg]);
src/dev/amdgpu/amdgpu_nbio.cc:        pkt->setLE<uint32_t>(gpuDevice->getRegVal(mm_index_reg));
src/dev/amdgpu/amdgpu_nbio.cc:        pkt->setLE<uint32_t>(0x10001);
src/dev/amdgpu/amdgpu_nbio.cc:        pkt->setLE<uint32_t>(0x1);
src/dev/amdgpu/amdgpu_nbio.cc:        pkt->setLE<uint32_t>(0x80000000);
src/dev/amdgpu/amdgpu_nbio.cc:            pkt->setLE<uint32_t>(triggered_reads[offset]);
src/dev/amdgpu/amdgpu_nbio.cc:            pkt->setLE<uint32_t>(reg_val);
src/dev/amdgpu/amdgpu_nbio.cc:                    pkt->getAddr());
src/dev/amdgpu/amdgpu_nbio.cc:        assert(pkt->getSize() == 4);
src/dev/amdgpu/amdgpu_nbio.cc:                                  pkt->getLE<uint32_t>());
src/dev/amdgpu/amdgpu_nbio.cc:        assert(pkt->getSize() == 4);
src/dev/amdgpu/amdgpu_nbio.cc:                                  pkt->getLE<uint32_t>());
src/dev/amdgpu/amdgpu_nbio.cc:                mm_index_reg, pkt->getLE<uint32_t>());
src/dev/amdgpu/amdgpu_nbio.cc:        gpuDevice->setRegVal(AMDGPU_MM_DATA, pkt->getLE<uint32_t>());
src/dev/amdgpu/amdgpu_nbio.cc:        if (pkt->getLE<uint32_t>() == 0x10000) {
src/dev/amdgpu/amdgpu_nbio.cc:            0x80000000 + pkt->getLE<uint32_t>();
src/dev/amdgpu/amdgpu_nbio.cc:        psp_ring = insertBits(psp_ring, 31, 0, pkt->getLE<uint32_t>());
src/dev/amdgpu/amdgpu_nbio.cc:        psp_ring = insertBits(psp_ring, 63, 32, pkt->getLE<uint32_t>());
src/dev/amdgpu/amdgpu_nbio.cc:        psp_ring_size = pkt->getLE<uint32_t>();
src/dev/amdgpu/amdgpu_nbio.cc:        pkt->setUintX(psp_ring_value, ByteOrder::little);
src/dev/amdgpu/amdgpu_nbio.cc:                "%ld\n", pkt->getSize(), pkt->getUintX(ByteOrder::little));
src/dev/amdgpu/amdgpu_nbio.cc:        if (pkt->getSize() == 4) {
src/dev/amdgpu/amdgpu_nbio.cc:            psp_ring_dev_addr = pkt->getLE<uint32_t>();
src/dev/amdgpu/amdgpu_nbio.cc:        } else if (pkt->getSize() == 8) {
src/dev/amdgpu/amdgpu_nbio.cc:            psp_ring_dev_addr = pkt->getUintX(ByteOrder::little)
src/dev/amdgpu/mmio_reader.cc:    pkt->setUintX(value, ByteOrder::little);
src/dev/amdgpu/amdgpu_device.cc:    Addr rom_offset = pkt->getAddr() & (ROM_SIZE - 1);
src/dev/amdgpu/amdgpu_device.cc:    memcpy(&rom_data, rom.data() + rom_offset, pkt->getSize());
src/dev/amdgpu/amdgpu_device.cc:    pkt->setUintX(rom_data, ByteOrder::little);
src/dev/amdgpu/amdgpu_device.cc:            pkt->getAddr(), rom_offset, rom_data);
src/dev/amdgpu/amdgpu_device.cc:    assert(isROM(pkt->getAddr()));
src/dev/amdgpu/amdgpu_device.cc:    Addr rom_offset = pkt->getAddr() - romRange.start();
src/dev/amdgpu/amdgpu_device.cc:    uint64_t rom_data = pkt->getUintX(ByteOrder::little);
src/dev/amdgpu/amdgpu_device.cc:    memcpy(rom.data() + rom_offset, &rom_data, pkt->getSize());
src/dev/amdgpu/amdgpu_device.cc:            pkt->getAddr(), rom_offset, rom_data);
src/dev/amdgpu/amdgpu_device.cc:    int offset = pkt->getAddr() & PCI_CONFIG_SIZE;
src/dev/amdgpu/amdgpu_device.cc:            switch (pkt->getSize()) {
src/dev/amdgpu/amdgpu_device.cc:                    pkt->setLE<uint8_t>(pxcap.data[pxcap_offset]);
src/dev/amdgpu/amdgpu_device.cc:                        (uint32_t)pkt->getLE<uint8_t>());
src/dev/amdgpu/amdgpu_device.cc:                    pkt->setLE<uint16_t>(
src/dev/amdgpu/amdgpu_device.cc:                        (uint32_t)pkt->getLE<uint16_t>());
src/dev/amdgpu/amdgpu_device.cc:                    pkt->setLE<uint32_t>(
src/dev/amdgpu/amdgpu_device.cc:                        (uint32_t)pkt->getLE<uint32_t>());
src/dev/amdgpu/amdgpu_device.cc:                          pkt->getSize(), pxcap_offset);
src/dev/amdgpu/amdgpu_device.cc:            pkt->makeAtomicResponse();
src/dev/amdgpu/amdgpu_device.cc:    [[maybe_unused]] int offset = pkt->getAddr() & PCI_CONFIG_SIZE;
src/dev/amdgpu/amdgpu_device.cc:            "data: %#x\n", offset, pkt->getSize(),
src/dev/amdgpu/amdgpu_device.cc:            pkt->getUintX(ByteOrder::little));
src/dev/amdgpu/amdgpu_device.cc:                pxcap_offset, pkt->getSize());
src/dev/amdgpu/amdgpu_device.cc:        memcpy(pxcap_data + pxcap_offset, pkt->getConstPtr<void>(),
src/dev/amdgpu/amdgpu_device.cc:               pkt->getSize());
src/dev/amdgpu/amdgpu_device.cc:    pkt->makeAtomicResponse();
src/dev/amdgpu/amdgpu_device.cc:            read ? "Read" : "Write", pkt->getAddr(), pkt->getSize(),
src/dev/amdgpu/amdgpu_device.cc:            pkt->getUintX(ByteOrder::little));
src/dev/amdgpu/amdgpu_device.cc:    pkt->makeAtomicResponse();
src/dev/amdgpu/amdgpu_device.cc:    RequestPtr req = std::make_shared<Request>(offset, pkt->getSize(), 0,
src/dev/amdgpu/amdgpu_device.cc:    uint8_t *dataPtr = new uint8_t[pkt->getSize()];
src/dev/amdgpu/amdgpu_device.cc:    pkt->setUintX(readPkt->getUintX(ByteOrder::little), ByteOrder::little);
src/dev/amdgpu/amdgpu_device.cc:        pkt->setUintX(value, ByteOrder::little);
src/dev/amdgpu/amdgpu_device.cc:        gpuvm.gartTable[aperture_offset] = pkt->getUintX(ByteOrder::little);
src/dev/amdgpu/amdgpu_device.cc:    RequestPtr req = std::make_shared<Request>(offset, pkt->getSize(), 0,
src/dev/amdgpu/amdgpu_device.cc:    uint8_t *dataPtr = new uint8_t[pkt->getSize()];
src/dev/amdgpu/amdgpu_device.cc:    std::memcpy(dataPtr, pkt->getPtr<uint8_t>(),
src/dev/amdgpu/amdgpu_device.cc:                pkt->getSize() * sizeof(uint8_t));
src/dev/amdgpu/amdgpu_device.cc:                                pkt->getLE<uint64_t>());
src/dev/amdgpu/amdgpu_device.cc:                                pkt->getLE<uint64_t>());
src/dev/amdgpu/amdgpu_device.cc:            sdmaEng->processGfx(pkt->getLE<uint64_t>());
src/dev/amdgpu/amdgpu_device.cc:            sdmaEng->processPage(pkt->getLE<uint64_t>());
src/dev/amdgpu/amdgpu_device.cc:                pkt->getLE<uint64_t>() + 1);
src/dev/amdgpu/amdgpu_device.cc:            pm4PktProc->updateReadIndex(offset, pkt->getLE<uint64_t>() + 1);
src/dev/amdgpu/amdgpu_device.cc:            deviceIH->updateRptr(pkt->getLE<uint32_t>());
src/dev/amdgpu/amdgpu_device.cc:            sdmaEng->processRLC(offset, pkt->getLE<uint64_t>());
src/dev/amdgpu/amdgpu_device.cc:        RequestPtr pending_req(pkt->req);
src/dev/amdgpu/amdgpu_device.cc:        uint8_t *pending_data = new uint8_t[pkt->getSize()];
src/dev/amdgpu/amdgpu_device.cc:        pending_pkt->dataDynamic(pending_data);
src/dev/amdgpu/amdgpu_device.cc:                (getSDMAById(idx)->*mptr)(pkt->getLE<uint32_t>());
src/dev/amdgpu/amdgpu_device.cc:    if (isROM(pkt->getAddr())) {
src/dev/amdgpu/amdgpu_device.cc:        getBAR(pkt->getAddr(), barnum, offset);
src/dev/amdgpu/amdgpu_device.cc:    if (isROM(pkt->getAddr())) {
src/dev/amdgpu/amdgpu_device.cc:    getBAR(pkt->getAddr(), barnum, offset);
src/dev/amdgpu/amdgpu_device.cc:    uint64_t data = pkt->getUintX(ByteOrder::little);
src/dev/amdgpu/amdgpu_device.cc:                            pkt->getAddr(), data);
src/dev/amdgpu/amdgpu_gfx.cc:        pkt->setLE<uint32_t>(captured_clock_count);
src/dev/amdgpu/amdgpu_gfx.cc:        pkt->setLE<uint32_t>(captured_clock_count >> 32);
src/dev/amdgpu/interrupt_handler.cc:        setCntl(pkt->getLE<uint32_t>());
src/dev/amdgpu/interrupt_handler.cc:        setBase(pkt->getLE<uint32_t>());
src/dev/amdgpu/interrupt_handler.cc:        setBaseHi(pkt->getLE<uint32_t>());
src/dev/amdgpu/interrupt_handler.cc:        setRptr(pkt->getLE<uint32_t>());
src/dev/amdgpu/interrupt_handler.cc:        setWptr(pkt->getLE<uint32_t>());
src/dev/amdgpu/interrupt_handler.cc:        setWptrAddrLo(pkt->getLE<uint32_t>());
src/dev/amdgpu/interrupt_handler.cc:        setWptrAddrHi(pkt->getLE<uint32_t>());
src/dev/amdgpu/interrupt_handler.cc:        setDoorbellOffset(pkt->getLE<uint32_t>());
src/dev/amdgpu/interrupt_handler.cc:        if (bits(pkt->getLE<uint32_t>(), 28, 28)) {
src/dev/amdgpu/system_hub.cc:    outstandingReqs[pkt->getAddr()].push_back(this_req);
src/dev/amdgpu/system_hub.cc:    if (outstandingReqs[pkt->getAddr()].size () > 1) {
src/dev/amdgpu/system_hub.cc:                pkt->getAddr());
src/dev/amdgpu/system_hub.cc:    if (pkt->isAtomicOp()) {
src/dev/amdgpu/system_hub.cc:        dmaRead(pkt->getAddr(), pkt->getSize(), atomicRespEvent,
src/dev/amdgpu/system_hub.cc:                pkt->getPtr<uint8_t>(), 0, 0, delay);
src/dev/amdgpu/system_hub.cc:    } else if (pkt->isWrite()) {
src/dev/amdgpu/system_hub.cc:        dmaWrite(pkt->getAddr(), pkt->getSize(), dmaRespEvent,
src/dev/amdgpu/system_hub.cc:                 pkt->getPtr<uint8_t>(), 0, 0, delay);
src/dev/amdgpu/system_hub.cc:        assert(pkt->isRead());
src/dev/amdgpu/system_hub.cc:        dmaRead(pkt->getAddr(), pkt->getSize(), dmaRespEvent,
src/dev/amdgpu/system_hub.cc:                pkt->getPtr<uint8_t>(), 0, 0, delay);
src/dev/amdgpu/system_hub.cc:            req_type.c_str(), pkt->getAddr(), pkt->getSize());
src/dev/amdgpu/system_hub.cc:            pkt->getAddr(), pkt->getSize());
src/dev/amdgpu/system_hub.cc:    systemHub.sendNextRequest(pkt->getAddr(), pkt);
src/dev/amdgpu/system_hub.cc:        std::make_shared<Request>(pkt->getAddr(), pkt->getSize(), 0,
src/dev/amdgpu/system_hub.cc:                                  pkt->requestorId());
src/dev/amdgpu/system_hub.cc:    uint8_t *write_data = new uint8_t[pkt->getSize()];
src/dev/amdgpu/system_hub.cc:    std::memcpy(write_data, pkt->getPtr<uint8_t>(), pkt->getSize());
src/dev/amdgpu/system_hub.cc:    write_pkt->dataDynamic(write_data);
src/dev/amdgpu/system_hub.cc:    assert(pkt->isAtomicOp());
src/dev/amdgpu/system_hub.cc:    (*pkt->getAtomicOp())(write_pkt->getPtr<uint8_t>());
src/dev/amdgpu/system_hub.cc:    systemHub.dmaWrite(write_pkt->getAddr(), write_pkt->getSize(),
src/dev/amdgpu/system_hub.cc:        dmaRespEvent, write_pkt->getPtr<uint8_t>(), 0, 0, delay);
src/dev/amdgpu/system_hub.cc:    if (write_pkt->getSize() == 8) {
src/dev/amdgpu/system_hub.cc:        req_data = write_pkt->getLE<uint64_t>();
src/dev/amdgpu/system_hub.cc:    } else if (pkt->getSize() == 4) {
src/dev/amdgpu/system_hub.cc:        req_data = write_pkt->getLE<uint32_t>();
src/dev/amdgpu/system_hub.cc:            write_pkt->getAddr(), req_data, write_pkt->getSize());
src/dev/amdgpu/memory_manager.cc:        pkt->dataDynamic<uint8_t>(dataPtr);
src/dev/amdgpu/memory_manager.cc:        pkt->pushSenderState(
src/dev/amdgpu/memory_manager.cc:        pkt->dataStatic<uint8_t>(dataPtr);
src/dev/amdgpu/memory_manager.cc:        pkt->pushSenderState(
src/dev/amdgpu/memory_manager.cc:        safe_cast<SenderState*>(pkt->senderState);
src/dev/amdgpu/memory_manager.cc:    delete pkt->senderState;
src/dev/amdgpu/memory_manager.cc:            DPRINTF(AMDGPUMem, "Retry for %#lx sent\n", pkt->getAddr());
src/dev/amdgpu/pm4_packet_processor.cc:    Addr addr = getGARTAddr(pkt->destAddr);
src/dev/amdgpu/pm4_packet_processor.cc:            pkt->data);
src/dev/amdgpu/pm4_packet_processor.cc:    //TODO: the specs indicate that pkt->data holds the number of dword that
src/dev/amdgpu/pm4_packet_processor.cc:    dmaWriteVirt(addr, sizeof(uint32_t), cb, &pkt->data);
src/dev/amdgpu/pm4_packet_processor.cc:    if (!pkt->writeConfirm)
src/dev/amdgpu/pm4_packet_processor.cc:            pkt->data);
src/dev/amdgpu/pm4_packet_processor.cc:    if (pkt->writeConfirm)
src/dev/amdgpu/pm4_packet_processor.cc:            " %d, mqdAddr: %lx, wptrAddr: %lx\n", pkt->queueSel, pkt->vmid,
src/dev/amdgpu/pm4_packet_processor.cc:            pkt->me, pkt->pipe, pkt->queueSlot, pkt->queueType,
src/dev/amdgpu/pm4_packet_processor.cc:            pkt->allocFormat, pkt->engineSel, pkt->numQueues,
src/dev/amdgpu/pm4_packet_processor.cc:            pkt->checkDisable, pkt->doorbellOffset, pkt->mqdAddr,
src/dev/amdgpu/pm4_packet_processor.cc:            pkt->wptrAddr);
src/dev/amdgpu/pm4_packet_processor.cc:    if (pkt->engineSel == 0 || pkt->engineSel == 1 || pkt->engineSel == 4) {
src/dev/amdgpu/pm4_packet_processor.cc:        Addr addr = getGARTAddr(pkt->mqdAddr + 96 * sizeof(uint32_t));
src/dev/amdgpu/pm4_packet_processor.cc:                addr, pkt->mqdAddr, pkt->vmid, gpuDevice->lastVMID());
src/dev/amdgpu/pm4_packet_processor.cc:        gpuDevice->mapDoorbellToVMID(pkt->doorbellOffset << 2,
src/dev/amdgpu/pm4_packet_processor.cc:    } else if (pkt->engineSel == 2 || pkt->engineSel == 3) {
src/dev/amdgpu/pm4_packet_processor.cc:        Addr addr = getGARTAddr(pkt->mqdAddr);
src/dev/amdgpu/pm4_packet_processor.cc:        panic("Unknown engine for MQD: %d\n", pkt->engineSel);
src/dev/amdgpu/pm4_packet_processor.cc:    assert(pkt->engineSel == 2 || pkt->engineSel == 3);
src/dev/amdgpu/pm4_packet_processor.cc:    SDMAEngine *sdma_eng = gpuDevice->getSDMAById(pkt->engineSel - 2);
src/dev/amdgpu/pm4_packet_processor.cc:    sdma_eng->registerRLCQueue(pkt->doorbellOffset << 2, addr, mqd);
src/dev/amdgpu/pm4_packet_processor.cc:    gpuDevice->setSDMAEngine(pkt->doorbellOffset << 2, sdma_eng);
src/dev/amdgpu/pm4_packet_processor.cc:    gpuDevice->setDoorbellType(pkt->doorbellOffset << 2, RLC);
src/dev/amdgpu/pm4_packet_processor.cc:    gpuDevice->processPendingDoorbells(pkt->doorbellOffset << 2);
src/dev/amdgpu/pm4_packet_processor.cc:    Addr addr = getGARTAddr(pkt->addr);
src/dev/amdgpu/pm4_packet_processor.cc:            pkt->event, pkt->eventIdx, pkt->intSelect, pkt->destSelect,
src/dev/amdgpu/pm4_packet_processor.cc:            pkt->dataSelect, addr, pkt->dataLo, pkt->intCtxId);
src/dev/amdgpu/pm4_packet_processor.cc:    if (pkt->dataSelect == 1) {
src/dev/amdgpu/pm4_packet_processor.cc:            pkt->dataLo);
src/dev/amdgpu/pm4_packet_processor.cc:            pkt->dataLo, addr);
src/dev/amdgpu/pm4_packet_processor.cc:    if (pkt->intSelect == 2) {
src/dev/amdgpu/pm4_packet_processor.cc:                "pipe: %d, queueSlot:%d\n", q->id(), pkt->intCtxId, q->me(),
src/dev/amdgpu/pm4_packet_processor.cc:        gpuDevice->getIH()->prepareInterruptCookie(pkt->intCtxId, ringId,
src/dev/amdgpu/pm4_packet_processor.cc:            pkt->queueSel, pkt->numQueues, pkt->pasid, pkt->doorbellOffset0);
src/dev/amdgpu/pm4_packet_processor.cc:    switch (pkt->queueSel) {
src/dev/amdgpu/pm4_packet_processor.cc:        switch (pkt->numQueues) {
src/dev/amdgpu/pm4_packet_processor.cc:                    gpuDevice->getVMID(pkt->doorbellOffset0));
src/dev/amdgpu/pm4_packet_processor.cc:                    gpuDevice->getVMID(pkt->doorbellOffset1));
src/dev/amdgpu/pm4_packet_processor.cc:                    gpuDevice->getVMID(pkt->doorbellOffset2));
src/dev/amdgpu/pm4_packet_processor.cc:                    gpuDevice->getVMID(pkt->doorbellOffset3));
src/dev/amdgpu/pm4_packet_processor.cc:                    gpuDevice->getVMID(pkt->doorbellOffset1));
src/dev/amdgpu/pm4_packet_processor.cc:                    gpuDevice->getVMID(pkt->doorbellOffset2));
src/dev/amdgpu/pm4_packet_processor.cc:                    gpuDevice->getVMID(pkt->doorbellOffset3));
src/dev/amdgpu/pm4_packet_processor.cc:                    gpuDevice->getVMID(pkt->doorbellOffset2));
src/dev/amdgpu/pm4_packet_processor.cc:                    gpuDevice->getVMID(pkt->doorbellOffset3));
src/dev/amdgpu/pm4_packet_processor.cc:                    gpuDevice->getVMID(pkt->doorbellOffset3));
src/dev/amdgpu/pm4_packet_processor.cc:            panic("Unrecognized number of queues %d\n", pkt->numQueues);
src/dev/amdgpu/pm4_packet_processor.cc:        gpuDevice->deallocatePasid(pkt->pasid);
src/dev/amdgpu/pm4_packet_processor.cc:            "%d pt: %p signal: %p\n", pkt->pasid, pkt->processQuantum,
src/dev/amdgpu/pm4_packet_processor.cc:            pkt->ptBase, pkt->completionSignal);
src/dev/amdgpu/pm4_packet_processor.cc:    mapProcess(pkt->pasid, pkt->ptBase, pkt->shMemBases);
src/dev/amdgpu/pm4_packet_processor.cc:            "%d pt: %p signal: %p\n", pkt->pasid, pkt->processQuantum,
src/dev/amdgpu/pm4_packet_processor.cc:            pkt->ptBase, pkt->completionSignal);
src/dev/amdgpu/pm4_packet_processor.cc:    mapProcess(pkt->pasid, pkt->ptBase, pkt->shMemBases);
src/dev/amdgpu/pm4_packet_processor.cc:            pkt->ibBase, pkt->ibSize);
src/dev/amdgpu/pm4_packet_processor.cc:    q->ibBase(pkt->ibBase);
src/dev/amdgpu/pm4_packet_processor.cc:    q->wptr(pkt->ibSize * sizeof(uint32_t));
src/dev/amdgpu/pm4_packet_processor.cc:            pkt->ibBase);
src/dev/amdgpu/pm4_packet_processor.cc:    q->ibBase(pkt->ibBase);
src/dev/amdgpu/pm4_packet_processor.cc:    q->wptr(pkt->ibSize * sizeof(uint32_t));
src/dev/amdgpu/pm4_packet_processor.cc:    // SET_UCONFIG_REG_START and pkt->offset are dword addresses
src/dev/amdgpu/pm4_packet_processor.cc:    uint32_t reg_addr = (PACKET3_SET_UCONFIG_REG_START + pkt->offset) * 4;
src/dev/amdgpu/pm4_packet_processor.cc:    gpuDevice->setRegVal(reg_addr, pkt->data);
src/dev/amdgpu/pm4_packet_processor.cc:            "%d\n", pkt->function, pkt->memSpace, pkt->operation);
src/dev/amdgpu/pm4_packet_processor.cc:    DPRINTF(PM4PacketProcessor, "    AddrLo/Reg1: %lx\n", pkt->memAddrLo);
src/dev/amdgpu/pm4_packet_processor.cc:    DPRINTF(PM4PacketProcessor, "    AddrHi/Reg2: %lx\n", pkt->memAddrHi);
src/dev/amdgpu/pm4_packet_processor.cc:    DPRINTF(PM4PacketProcessor, "    Reference: %lx\n", pkt->reference);
src/dev/amdgpu/pm4_packet_processor.cc:    DPRINTF(PM4PacketProcessor, "    Mask: %lx\n", pkt->mask);
src/dev/amdgpu/pm4_packet_processor.cc:    DPRINTF(PM4PacketProcessor, "    Poll Interval: %lx\n", pkt->pollInterval);
src/dev/amdgpu/pm4_packet_processor.cc:            "addr: %lx, data: %lx\n", pkt->contextId, pkt->interruptSel,
src/dev/amdgpu/pm4_packet_processor.cc:            pkt->command, pkt->pasid, pkt->doorbellOffset, pkt->engineSel,
src/dev/amdgpu/pm4_packet_processor.cc:            pkt->addr, pkt->data);
src/dev/amdgpu/pm4_packet_processor.cc:    if (pkt->interruptSel == 0 && pkt->command == 2) {
src/dev/amdgpu/pm4_packet_processor.cc:        Addr addr = getGARTAddr(pkt->addr);
src/dev/amdgpu/pm4_packet_processor.cc:            [ = ] (const uint64_t &) { queryStatusDone(q, pkt); }, pkt->data);
src/dev/amdgpu/pm4_packet_processor.cc:              pkt->interruptSel, pkt->command);
src/dev/amdgpu/pm4_packet_processor.cc:        setHqdVmid(pkt->getLE<uint32_t>());
src/dev/amdgpu/pm4_packet_processor.cc:        setHqdActive(pkt->getLE<uint32_t>());
src/dev/amdgpu/pm4_packet_processor.cc:        setHqdPqBase(pkt->getLE<uint32_t>());
src/dev/amdgpu/pm4_packet_processor.cc:        setHqdPqBaseHi(pkt->getLE<uint32_t>());
src/dev/amdgpu/pm4_packet_processor.cc:        setHqdPqDoorbellCtrl(pkt->getLE<uint32_t>());
src/dev/amdgpu/pm4_packet_processor.cc:        setHqdPqPtr(pkt->getLE<uint32_t>());
src/dev/amdgpu/pm4_packet_processor.cc:        setHqdPqWptrLo(pkt->getLE<uint32_t>());
src/dev/amdgpu/pm4_packet_processor.cc:        setHqdPqWptrHi(pkt->getLE<uint32_t>());
src/dev/amdgpu/pm4_packet_processor.cc:        setHqdPqRptrReportAddr(pkt->getLE<uint32_t>());
src/dev/amdgpu/pm4_packet_processor.cc:        setHqdPqRptrReportAddrHi(pkt->getLE<uint32_t>());
src/dev/amdgpu/pm4_packet_processor.cc:        setHqdPqWptrPollAddr(pkt->getLE<uint32_t>());
src/dev/amdgpu/pm4_packet_processor.cc:        setHqdPqWptrPollAddrHi(pkt->getLE<uint32_t>());
src/dev/amdgpu/pm4_packet_processor.cc:        setHqdPqControl(pkt->getLE<uint32_t>());
src/dev/amdgpu/pm4_packet_processor.cc:        setHqdIbCtrl(pkt->getLE<uint32_t>());
src/dev/amdgpu/pm4_packet_processor.cc:        setRbVmid(pkt->getLE<uint32_t>());
src/dev/amdgpu/pm4_packet_processor.cc:        setRbCntl(pkt->getLE<uint32_t>());
src/dev/amdgpu/pm4_packet_processor.cc:        setRbWptrLo(pkt->getLE<uint32_t>());
src/dev/amdgpu/pm4_packet_processor.cc:        setRbWptrHi(pkt->getLE<uint32_t>());
src/dev/amdgpu/pm4_packet_processor.cc:        setRbRptrAddrLo(pkt->getLE<uint32_t>());
src/dev/amdgpu/pm4_packet_processor.cc:        setRbRptrAddrHi(pkt->getLE<uint32_t>());
src/dev/amdgpu/pm4_packet_processor.cc:        setRbWptrPollAddrLo(pkt->getLE<uint32_t>());
src/dev/amdgpu/pm4_packet_processor.cc:        setRbWptrPollAddrHi(pkt->getLE<uint32_t>());
src/dev/amdgpu/pm4_packet_processor.cc:        setRbBaseLo(pkt->getLE<uint32_t>());
src/dev/amdgpu/pm4_packet_processor.cc:        setRbBaseHi(pkt->getLE<uint32_t>());
src/dev/amdgpu/pm4_packet_processor.cc:        setRbDoorbellCntrl(pkt->getLE<uint32_t>());
src/dev/amdgpu/pm4_packet_processor.cc:        setRbDoorbellRangeLo(pkt->getLE<uint32_t>());
src/dev/amdgpu/pm4_packet_processor.cc:        setRbDoorbellRangeHi(pkt->getLE<uint32_t>());
src/dev/amdgpu/amdgpu_vm.cc:    uint32_t value = pkt->getLE<uint32_t>();
src/dev/amdgpu/amdgpu_vm.cc:        pkt->setLE<uint32_t>(1);
src/dev/amdgpu/amdgpu_vm.cc:        pkt->setLE<uint32_t>(1);
src/dev/amdgpu/amdgpu_vm.cc:        pkt->setLE<uint32_t>(1);
src/dev/amdgpu/amdgpu_vm.cc:        vmContext0.ptBaseL = pkt->getLE<uint32_t>();
src/dev/amdgpu/amdgpu_vm.cc:        vmContext0.ptBaseH = pkt->getLE<uint32_t>();
src/dev/amdgpu/amdgpu_vm.cc:        vmContext0.ptStartL = pkt->getLE<uint32_t>();
src/dev/amdgpu/amdgpu_vm.cc:        vmContext0.ptStartH = pkt->getLE<uint32_t>();
src/dev/amdgpu/amdgpu_vm.cc:        vmContext0.ptEndL = pkt->getLE<uint32_t>();
src/dev/amdgpu/amdgpu_vm.cc:        vmContext0.ptEndH = pkt->getLE<uint32_t>();
src/dev/amdgpu/amdgpu_vm.cc:        uint32_t val = pkt->getLE<uint32_t>();
src/dev/amdgpu/amdgpu_vm.cc:        uint32_t val = pkt->getLE<uint32_t>();
src/dev/amdgpu/amdgpu_vm.cc:        uint32_t val = pkt->getLE<uint32_t>();
src/dev/amdgpu/amdgpu_vm.cc:        uint32_t val = pkt->getLE<uint32_t>();
src/dev/amdgpu/amdgpu_vm.cc:        uint32_t val = pkt->getLE<uint32_t>();
src/dev/amdgpu/amdgpu_vm.cc:        uint32_t val = pkt->getLE<uint32_t>();
src/dev/amdgpu/amdgpu_vm.cc:        uint32_t val = pkt->getLE<uint32_t>();
src/dev/amdgpu/amdgpu_vm.cc:        uint32_t val = pkt->getLE<uint32_t>();
src/dev/lupio/lupio_sys.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/lupio/lupio_sys.cc:        "Read request - addr: %#x, size: %#x\n", daddr, pkt->getSize());
src/dev/lupio/lupio_sys.cc:    pkt->setUintX(sys_read, byteOrder);
src/dev/lupio/lupio_sys.cc:    pkt->makeResponse();
src/dev/lupio/lupio_sys.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/lupio/lupio_sys.cc:            pkt->getUintX(byteOrder));
src/dev/lupio/lupio_sys.cc:    lupioSYSWrite(daddr, pkt->getUintX(byteOrder));
src/dev/lupio/lupio_sys.cc:    DPRINTF(LupioSYS, "Packet Write Value: %d\n", pkt->getUintX(byteOrder));
src/dev/lupio/lupio_sys.cc:    pkt->makeResponse();
src/dev/lupio/lupio_blk.cc:    Addr addr = pkt->getAddr() - pioAddr;
src/dev/lupio/lupio_blk.cc:        "Read request - addr: %#x, size: %#x\n", addr, pkt->getSize());
src/dev/lupio/lupio_blk.cc:    pkt->setUintX(read_request, byteOrder);
src/dev/lupio/lupio_blk.cc:    pkt->makeResponse();
src/dev/lupio/lupio_blk.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/lupio/lupio_blk.cc:            pkt->getUintX(byteOrder));
src/dev/lupio/lupio_blk.cc:    lupioBLKWrite(daddr, pkt->getUintX(byteOrder));
src/dev/lupio/lupio_blk.cc:    DPRINTF(LupioBLK, "Packet Write Value: %d\n", pkt->getUintX(byteOrder));
src/dev/lupio/lupio_blk.cc:    pkt->makeResponse();
src/dev/lupio/lupio_rng.cc:    Addr rng_addr = pkt->getAddr() - pioAddr;
src/dev/lupio/lupio_rng.cc:        "Read request - addr: %#x, size: %#x\n", rng_addr, pkt->getSize());
src/dev/lupio/lupio_rng.cc:    pkt->setUintX(rand_read, byteOrder);
src/dev/lupio/lupio_rng.cc:    pkt->makeResponse();
src/dev/lupio/lupio_rng.cc:    Addr daddr = pkt->getAddr() - pioAddr;
src/dev/lupio/lupio_rng.cc:            pkt->getUintX(byteOrder));
src/dev/lupio/lupio_rng.cc:    lupioRNGWrite(daddr, pkt->getUintX(byteOrder));
src/dev/lupio/lupio_rng.cc:    DPRINTF(LupioRNG, "Packet Write Value: %d\n", pkt->getUintX(byteOrder));
src/dev/lupio/lupio_rng.cc:    pkt->makeResponse();
src/dev/lupio/lupio_tmr.cc:    Addr tmr_addr = pkt->getAddr() - pioAddr;
src/dev/lupio/lupio_tmr.cc:        "Read request - addr: %#x, size: %#x\n", tmr_addr, pkt->getSize());
src/dev/lupio/lupio_tmr.cc:    uint64_t read_val = lupioTMRRead(tmr_addr, pkt->getSize());
src/dev/lupio/lupio_tmr.cc:    pkt->setUintX(read_val, byteOrder);
src/dev/lupio/lupio_tmr.cc:    pkt->makeResponse();
src/dev/lupio/lupio_tmr.cc:    Addr tmr_addr = pkt->getAddr() - pioAddr;
src/dev/lupio/lupio_tmr.cc:            pkt->getUintX(byteOrder));
src/dev/lupio/lupio_tmr.cc:    lupioTMRWrite(tmr_addr, pkt->getUintX(byteOrder), pkt->getSize());
src/dev/lupio/lupio_tmr.cc:    DPRINTF(LupioTMR, "Packet Write Value: %d\n", pkt->getUintX(byteOrder));
src/dev/lupio/lupio_tmr.cc:    pkt->makeResponse();
src/dev/lupio/lupio_ipi.cc:    Addr ipi_addr = pkt->getAddr() - pioAddr;
src/dev/lupio/lupio_ipi.cc:        "Read request - addr: %#x, size: %#x\n", ipi_addr, pkt->getSize());
src/dev/lupio/lupio_ipi.cc:    uint64_t read_val = lupioIPIRead(ipi_addr, pkt->getSize());
src/dev/lupio/lupio_ipi.cc:    pkt->setUintX(read_val, byteOrder);
src/dev/lupio/lupio_ipi.cc:    pkt->makeResponse();
src/dev/lupio/lupio_ipi.cc:    Addr ipi_addr = pkt->getAddr() - pioAddr;
src/dev/lupio/lupio_ipi.cc:            pkt->getUintX(byteOrder));
src/dev/lupio/lupio_ipi.cc:    lupioIPIWrite(ipi_addr, pkt->getUintX(byteOrder), pkt->getSize());
src/dev/lupio/lupio_ipi.cc:    DPRINTF(LupioIPI, "Packet Write Value: %d\n", pkt->getUintX(byteOrder));
src/dev/lupio/lupio_ipi.cc:    pkt->makeResponse();
src/dev/lupio/lupio_tty.cc:    Addr tty_addr = pkt->getAddr() - pioAddr;
src/dev/lupio/lupio_tty.cc:        "Read request - addr: %#x, size: %#x\n", tty_addr, pkt->getSize());
src/dev/lupio/lupio_tty.cc:    pkt->setUintX(val, byteOrder);
src/dev/lupio/lupio_tty.cc:    pkt->makeResponse();
src/dev/lupio/lupio_tty.cc:    Addr tty_addr = pkt->getAddr() - pioAddr;
src/dev/lupio/lupio_tty.cc:         pkt->getAddr(), pkt->getUintX(byteOrder));
src/dev/lupio/lupio_tty.cc:    lupioTTYWrite(tty_addr, pkt->getUintX(byteOrder));
src/dev/lupio/lupio_tty.cc:    pkt->makeResponse();
src/dev/lupio/lupio_pic.cc:    Addr pic_addr = pkt->getAddr() - pioAddr;
src/dev/lupio/lupio_pic.cc:        "Read request - addr: %#x, size: %#x\n", pic_addr, pkt->getSize());
src/dev/lupio/lupio_pic.cc:    pkt->setUintX(read_val, byteOrder);
src/dev/lupio/lupio_pic.cc:    pkt->makeResponse();
src/dev/lupio/lupio_pic.cc:    Addr pic_addr = pkt->getAddr() - pioAddr;
src/dev/lupio/lupio_pic.cc:            pkt->getUintX(byteOrder));
src/dev/lupio/lupio_pic.cc:    lupioPicWrite(pic_addr, pkt->getUintX(byteOrder));
src/dev/lupio/lupio_pic.cc:    DPRINTF(LupioPIC, "Packet Write Value: %d\n", pkt->getUintX(byteOrder));
src/dev/lupio/lupio_pic.cc:    pkt->makeResponse();
src/dev/lupio/lupio_rtc.cc:    bool is_atomic = pkt->isAtomicOp() && pkt->cmd == MemCmd::SwapReq;
src/dev/lupio/lupio_rtc.cc:        pkt->getAddr(), pkt->getSize(), is_atomic);
src/dev/lupio/lupio_rtc.cc:    Addr rtc_addr = pkt->getAddr() - pioAddr;
src/dev/lupio/lupio_rtc.cc:    pkt->setData(&time_val);
src/dev/lupio/lupio_rtc.cc:        pkt->makeAtomicResponse();
src/dev/lupio/lupio_rtc.cc:        pkt->makeResponse();
src/sim/system.cc:    if (!deviceMemMap.count(pkt->requestorId())) {
src/sim/system.cc:    const RequestorID& rid = pkt->requestorId();
src/sim/system.cc:        if (pkt->getAddrRange().isSubset(mem->getAddrRange())) {
src/sim/pseudo_inst.cc:    pkt->dataDynamic(buffer_data);
src/sim/pseudo_inst.cc:    pkt->senderState = req;
src/sim/probe/mem.hh:        cmd(pkt->cmd),
src/sim/probe/mem.hh:        addr(pkt->getAddr()),
src/sim/probe/mem.hh:        size(pkt->getSize()),
src/sim/probe/mem.hh:        flags(pkt->req->getFlags()),
src/sim/probe/mem.hh:        pc(pkt->req->hasPC() ? pkt->req->getPC() : 0),
src/sim/probe/mem.hh:        id(pkt->req->requestorId())  { }
src/learning_gem5/part2/simple_cache.cc:    DPRINTF(SimpleCache, "Sending %s to CPU\n", pkt->print());
src/learning_gem5/part2/simple_cache.cc:    DPRINTF(SimpleCache, "Got request %s\n", pkt->print());
src/learning_gem5/part2/simple_cache.cc:    DPRINTF(SimpleCache, "Retrying response pkt %s\n", pkt->print());
src/learning_gem5/part2/simple_cache.cc:    DPRINTF(SimpleCache, "Got request for addr %#x\n", pkt->getAddr());
src/learning_gem5/part2/simple_cache.cc:    DPRINTF(SimpleCache, "Got response for addr %#x\n", pkt->getAddr());
src/learning_gem5/part2/simple_cache.cc:    DPRINTF(SimpleCache, "Sending resp for addr %#x\n", pkt->getAddr());
src/learning_gem5/part2/simple_cache.cc:        pkt->makeResponse();
src/learning_gem5/part2/simple_cache.cc:            pkt->print());
src/learning_gem5/part2/simple_cache.cc:        DDUMP(SimpleCache, pkt->getConstPtr<uint8_t>(), pkt->getSize());
src/learning_gem5/part2/simple_cache.cc:        pkt->makeResponse();
src/learning_gem5/part2/simple_cache.cc:        Addr addr = pkt->getAddr();
src/learning_gem5/part2/simple_cache.cc:        Addr block_addr = pkt->getBlockAddr(blockSize);
src/learning_gem5/part2/simple_cache.cc:        unsigned size = pkt->getSize();
src/learning_gem5/part2/simple_cache.cc:            assert(pkt->needsResponse());
src/learning_gem5/part2/simple_cache.cc:            if (pkt->isWrite() || pkt->isRead()) {
src/learning_gem5/part2/simple_cache.cc:            PacketPtr new_pkt = new Packet(pkt->req, cmd, blockSize);
src/learning_gem5/part2/simple_cache.cc:            new_pkt->allocate();
src/learning_gem5/part2/simple_cache.cc:            assert(new_pkt->getAddr() == new_pkt->getBlockAddr(blockSize));
src/learning_gem5/part2/simple_cache.cc:    Addr block_addr = pkt->getBlockAddr(blockSize);
src/learning_gem5/part2/simple_cache.cc:        if (pkt->isWrite()) {
src/learning_gem5/part2/simple_cache.cc:            pkt->writeDataToBlock(it->second, blockSize);
src/learning_gem5/part2/simple_cache.cc:        } else if (pkt->isRead()) {
src/learning_gem5/part2/simple_cache.cc:            pkt->setDataFromBlock(it->second, blockSize);
src/learning_gem5/part2/simple_cache.cc:    assert(pkt->getAddr() ==  pkt->getBlockAddr(blockSize));
src/learning_gem5/part2/simple_cache.cc:    assert(cacheStore.find(pkt->getAddr()) == cacheStore.end());
src/learning_gem5/part2/simple_cache.cc:    assert(pkt->isResponse());
src/learning_gem5/part2/simple_cache.cc:        new_pkt->dataDynamic(block->second); // This will be deleted later
src/learning_gem5/part2/simple_cache.cc:        DPRINTF(SimpleCache, "Writing packet back %s\n", pkt->print());
src/learning_gem5/part2/simple_cache.cc:    DPRINTF(SimpleCache, "Inserting %s\n", pkt->print());
src/learning_gem5/part2/simple_cache.cc:    DDUMP(SimpleCache, pkt->getConstPtr<uint8_t>(), blockSize);
src/learning_gem5/part2/simple_cache.cc:    cacheStore[pkt->getAddr()] = data;
src/learning_gem5/part2/simple_cache.cc:    pkt->writeDataToBlock(data, blockSize);
src/learning_gem5/part2/simple_memobj.cc:    DPRINTF(SimpleMemobj, "Got request for addr %#x\n", pkt->getAddr());
src/learning_gem5/part2/simple_memobj.cc:    DPRINTF(SimpleMemobj, "Got response for addr %#x\n", pkt->getAddr());
src/learning_gem5/part2/simple_memobj.cc:    if (pkt->req->isInstFetch()) {
src/sst/outgoing_request_bridge.cc:    uint8_t* ptr = pkt->getPtr<uint8_t>();
src/sst/outgoing_request_bridge.cc:    uint64_t size = pkt->getSize();
src/sst/outgoing_request_bridge.cc:    initData.push_back(std::make_pair(pkt->getAddr(), data));
src/systemc/tlm_bridge/sc_ext.cc:                    pkt->req->setFlags(Request::PRIVILEGED);
src/systemc/tlm_bridge/sc_ext.cc:                    pkt->req->clearFlags(Request::PRIVILEGED);
src/systemc/tlm_bridge/sc_ext.cc:                    pkt->req->setFlags(Request::SECURE);
src/systemc/tlm_bridge/sc_ext.cc:                    pkt->req->clearFlags(Request::SECURE);
src/systemc/tlm_bridge/sc_ext.cc:                    pkt->req->setFlags(Request::INST_FETCH);
src/systemc/tlm_bridge/sc_ext.cc:                    pkt->req->clearFlags(Request::INST_FETCH);
src/systemc/tlm_bridge/sc_ext.cc:                pkt->qosValue(control_ex->getQos());
src/systemc/tlm_bridge/sc_ext.cc:                    pkt->req->setStreamId(control_ex->getStreamId().value());
src/systemc/tlm_bridge/sc_ext.cc:                    pkt->req->setSubstreamId(
src/systemc/tlm_bridge/sc_ext.cc:                control_ex->setPrivileged(pkt->req->isPriv());
src/systemc/tlm_bridge/sc_ext.cc:                control_ex->setSecure(pkt->req->isSecure());
src/systemc/tlm_bridge/sc_ext.cc:                control_ex->setInstruction(pkt->req->isInstFetch());
src/systemc/tlm_bridge/sc_ext.cc:                control_ex->setQos(pkt->qosValue());
src/systemc/tlm_bridge/sc_ext.cc:                if (pkt->req->hasStreamId()) {
src/systemc/tlm_bridge/sc_ext.cc:                    control_ex->setStreamId(pkt->req->streamId());
src/systemc/tlm_bridge/sc_ext.cc:                if (pkt->req->hasSubstreamId()) {
src/systemc/tlm_bridge/sc_ext.cc:                    control_ex->setSubstreamId(pkt->req->substreamId());
src/systemc/tlm_bridge/tlm_to_gem5.cc:        pkt->setAddr(trans.get_address());
src/systemc/tlm_bridge/tlm_to_gem5.cc:    pkt->dataStatic(trans.get_data_ptr());
src/systemc/tlm_bridge/tlm_to_gem5.cc:    if (!pkt->isError()) {
src/systemc/tlm_bridge/tlm_to_gem5.cc:    } else if (pkt->isRead() || pkt->isWrite()) {
src/systemc/tlm_bridge/tlm_to_gem5.cc:    pkt->pushSenderState(new Gem5SystemC::TlmSenderState(trans));
src/systemc/tlm_bridge/tlm_to_gem5.cc:    bool needsResponse = pkt->needsResponse();
src/systemc/tlm_bridge/tlm_to_gem5.cc:    pkt->pushSenderState(new Gem5SystemC::TlmSenderState(trans));
src/systemc/tlm_bridge/tlm_to_gem5.cc:    panic_if(pkt->needsResponse() && !pkt->isResponse(),
src/systemc/tlm_bridge/tlm_to_gem5.cc:    gem5::Packet::SenderState *senderState = pkt->popSenderState();
src/systemc/tlm_bridge/tlm_to_gem5.cc:        pkt->pushSenderState(new Gem5SystemC::TlmSenderState(trans));
src/systemc/tlm_bridge/tlm_to_gem5.cc:        gem5::Packet::SenderState *senderState = pkt->popSenderState();
src/systemc/tlm_bridge/tlm_to_gem5.cc:    sc_assert(pkt->isResponse());
src/systemc/tlm_bridge/tlm_to_gem5.cc:    auto delay = sc_core::sc_time::from_value(pkt->payloadDelay);
src/systemc/tlm_bridge/tlm_to_gem5.cc:    pkt->payloadDelay = 0;
src/systemc/tlm_bridge/tlm_to_gem5.cc:    pkt->headerDelay = 0;
src/systemc/tlm_bridge/tlm_to_gem5.cc:        dynamic_cast<Gem5SystemC::TlmSenderState*>(pkt->popSenderState());
src/systemc/tlm_bridge/gem5_to_tlm.cc:    pkt->makeResponse();
src/systemc/tlm_bridge/gem5_to_tlm.cc:        pkt->setBadCommand();
src/systemc/tlm_bridge/gem5_to_tlm.cc:        pkt->setBadAddress();
Binary file chiplets/gapbs.img matches
Binary file .git/objects/pack/pack-35fddc6be10b85862acc86e98966d0e9fc8cf1cf.pack matches
